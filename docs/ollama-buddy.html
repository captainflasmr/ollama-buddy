<!DOCTYPE html>
<html>
<!-- Created by GNU Texinfo 7.2, https://www.gnu.org/software/texinfo/ -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!-- Copyright Â© 2024-2026 James Dyer

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. -->
<title>Ollama Buddy User Manual</title>

<meta name="description" content="Ollama Buddy User Manual">
<meta name="keywords" content="Ollama Buddy User Manual">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">
<meta name="Generator" content="makeinfo">
<meta name="viewport" content="width=device-width,initial-scale=1">

<link href="#Top" rel="start" title="Top">
<link href="#Index" rel="index" title="Index">
<link href="#SEC_Contents" rel="contents" title="Table of Contents">
<link href="#Introduction" rel="next" title="Introduction">
<style type="text/css">
<!--
a.copiable-link {visibility: hidden; text-decoration: none; line-height: 0em}
div.example {margin-left: 3.2em}
kbd.kbd {font-style: oblique}
span:hover a.copiable-link {visibility: visible}
ul.mark-bullet {list-style-type: disc}
ul.mark-minus {list-style-type: "\2212"}
ul.toc-numbered-mark {list-style: none}
-->
</style>


</head>

<body lang="en">





<div class="top-level-extent" id="Top">
<div class="nav-panel">
<p>
Next: <a href="#Introduction" accesskey="n" rel="next">Introduction</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h1 class="top" id="Ollama-Buddy"><span>Ollama Buddy<a class="copiable-link" href="#Ollama-Buddy"> &para;</a></span></h1>

<p>Ollama Buddy is a comprehensive Emacs package that provides seamless integration with Ollama, 
allowing you to leverage powerful large language models (LLMs) directly within your Emacs workflow.
</p>


<div class="region-contents" id="SEC_Contents">
<h2 class="contents-heading">Table of Contents</h2>

<div class="contents">

<ul class="toc-numbered-mark">
  <li><a id="toc-Introduction" href="#Introduction">1 Introduction</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-What-is-Ollama-Buddy_003f" href="#What-is-Ollama-Buddy_003f">1.1 What is Ollama Buddy?</a></li>
    <li><a id="toc-Why-Use-Ollama-Buddy_003f" href="#Why-Use-Ollama-Buddy_003f">1.2 Why Use Ollama Buddy?</a></li>
    <li><a id="toc-Prerequisites" href="#Prerequisites">1.3 Prerequisites</a></li>
  </ul></li>
  <li><a id="toc-Installation" href="#Installation">2 Installation</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Installing-Ollama" href="#Installing-Ollama">2.1 Installing Ollama</a></li>
    <li><a id="toc-Package-Installation" href="#Package-Installation">2.2 Package Installation</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Using-package_002eel" href="#Using-package_002eel">2.2.1 Using package.el</a></li>
      <li><a id="toc-Using-use_002dpackage" href="#Using-use_002dpackage">2.2.2 Using use-package</a></li>
      <li><a id="toc-Manual-Installation" href="#Manual-Installation">2.2.3 Manual Installation</a></li>
    </ul></li>
    <li><a id="toc-Dependencies" href="#Dependencies">2.3 Dependencies</a></li>
    <li><a id="toc-Communication-Backends" href="#Communication-Backends">2.4 Communication Backends</a></li>
    <li><a id="toc-API-Key-Setup" href="#API-Key-Setup">2.5 API Key Setup</a></li>
  </ul></li>
  <li><a id="toc-Configuration" href="#Configuration">3 Configuration</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Basic-Configuration" href="#Basic-Configuration">3.1 Basic Configuration</a></li>
    <li><a id="toc-Display-Options" href="#Display-Options">3.2 Display Options</a></li>
    <li><a id="toc-File-Attachment-Configuration" href="#File-Attachment-Configuration">3.3 File Attachment Configuration</a></li>
    <li><a id="toc-Directory-Configuration" href="#Directory-Configuration">3.4 Directory Configuration</a></li>
    <li><a id="toc-History-and-Session-Configuration" href="#History-and-Session-Configuration">3.5 History and Session Configuration</a></li>
    <li><a id="toc-Context-Management-Configuration" href="#Context-Management-Configuration">3.6 Context Management Configuration</a></li>
    <li><a id="toc-External-API-Configuration" href="#External-API-Configuration">3.7 External API Configuration</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Provider-Prefixes" href="#Provider-Prefixes">3.7.1 Provider Prefixes</a></li>
    </ul></li>
    <li><a id="toc-Global-System-Prompt-Configuration" href="#Global-System-Prompt-Configuration">3.8 Global System Prompt Configuration</a></li>
    <li><a id="toc-Vision-Configuration" href="#Vision-Configuration">3.9 Vision Configuration</a></li>
    <li><a id="toc-Cloud-Model-Configuration" href="#Cloud-Model-Configuration">3.10 Cloud Model Configuration</a></li>
    <li><a id="toc-Awesome-ChatGPT-Prompts-Configuration" href="#Awesome-ChatGPT-Prompts-Configuration">3.11 Awesome ChatGPT Prompts Configuration</a></li>
  </ul></li>
  <li><a id="toc-Quick-Start" href="#Quick-Start">4 Quick Start</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Basic-Usage" href="#Basic-Usage">4.1 Basic Usage</a></li>
    <li><a id="toc-Common-Operations" href="#Common-Operations">4.2 Common Operations</a></li>
  </ul></li>
  <li><a id="toc-Core-Features" href="#Core-Features">5 Core Features</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Chat-Interface" href="#Chat-Interface-1">5.1 Chat Interface</a></li>
    <li><a id="toc-Pre_002dbuilt-Commands" href="#Pre_002dbuilt-Commands">5.2 Pre-built Commands</a></li>
    <li><a id="toc-Model-Management" href="#Model-Management">5.3 Model Management</a></li>
    <li><a id="toc-Parameter-Control" href="#Parameter-Control-1">5.4 Parameter Control</a></li>
    <li><a id="toc-Roles-and-Custom-Commands" href="#Roles-and-Custom-Commands">5.5 Roles and Custom Commands</a></li>
    <li><a id="toc-Prompt-Template-Collections" href="#Prompt-Template-Collections">5.6 Prompt Template Collections</a></li>
    <li><a id="toc-External-API-Integration" href="#External-API-Integration">5.7 External API Integration</a></li>
    <li><a id="toc-Vision-Support" href="#Vision-Support">5.8 Vision Support</a></li>
    <li><a id="toc-File-Attachments" href="#File-Attachments-1">5.9 File Attachments</a></li>
  </ul></li>
  <li><a id="toc-Chat-Interface-1" href="#Chat-Interface">6 Chat Interface</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Opening-the-Chat" href="#Opening-the-Chat">6.1 Opening the Chat</a></li>
    <li><a id="toc-Interface-Overview" href="#Interface-Overview">6.2 Interface Overview</a></li>
    <li><a id="toc-Sending-Prompts" href="#Sending-Prompts">6.3 Sending Prompts</a></li>
    <li><a id="toc-System-Prompts" href="#System-Prompts">6.4 System Prompts</a></li>
    <li><a id="toc-Markdown-to-Org-Conversion" href="#Markdown-to-Org-Conversion">6.5 Markdown to Org Conversion</a></li>
    <li><a id="toc-Reasoning-Visibility-Control" href="#Reasoning-Visibility-Control">6.6 Reasoning Visibility Control</a></li>
  </ul></li>
  <li><a id="toc-Working-with-Models" href="#Working-with-Models">7 Working with Models</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Available-Models" href="#Available-Models">7.1 Available Models</a></li>
    <li><a id="toc-Switching-Models" href="#Switching-Models">7.2 Switching Models</a></li>
    <li><a id="toc-Local-vs_002e-Cloud-Models" href="#Local-vs_002e-Cloud-Models">7.3 Local vs. Cloud Models</a></li>
    <li><a id="toc-Managing-Models" href="#Managing-Models">7.4 Managing Models</a></li>
    <li><a id="toc-Pulling-New-Models" href="#Pulling-New-Models">7.5 Pulling New Models</a></li>
    <li><a id="toc-Importing-GGUF-Files" href="#Importing-GGUF-Files">7.6 Importing GGUF Files</a></li>
    <li><a id="toc-Vision-Models" href="#Vision-Models">7.7 Vision Models</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Configuring-Vision-Support" href="#Configuring-Vision-Support">7.7.1 Configuring Vision Support</a></li>
      <li><a id="toc-Using-Vision-Models" href="#Using-Vision-Models">7.7.2 Using Vision Models</a></li>
    </ul></li>
    <li><a id="toc-Multishot-Mode" href="#Multishot-Mode">7.8 Multishot Mode</a></li>
  </ul></li>
  <li><a id="toc-Context-Management" href="#Context-Management">8 Context Management</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Understanding-Context-Windows" href="#Understanding-Context-Windows">8.1 Understanding Context Windows</a></li>
    <li><a id="toc-Context-Size-Detection" href="#Context-Size-Detection">8.2 Context Size Detection</a></li>
    <li><a id="toc-Enabling-Context-Monitoring" href="#Enabling-Context-Monitoring">8.3 Enabling Context Monitoring</a></li>
    <li><a id="toc-Context-with-File-Attachments" href="#Context-with-File-Attachments">8.4 Context with File Attachments</a></li>
    <li><a id="toc-Context-Management-Commands" href="#Context-Management-Commands">8.5 Context Management Commands</a></li>
    <li><a id="toc-Token-Estimation" href="#Token-Estimation">8.6 Token Estimation</a></li>
    <li><a id="toc-Managing-Context-in-Practice" href="#Managing-Context-in-Practice">8.7 Managing Context in Practice</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Workflow-Strategies" href="#Workflow-Strategies">8.7.1 Workflow Strategies</a>
      <ul class="toc-numbered-mark">
        <li><a id="toc-Paste_002dand_002dSend-Approach" href="#Paste_002dand_002dSend-Approach">8.7.1.1 Paste-and-Send Approach</a></li>
        <li><a id="toc-Preemptive-Checking" href="#Preemptive-Checking">8.7.1.2 Preemptive Checking</a></li>
        <li><a id="toc-History-Length-Management" href="#History-Length-Management">8.7.1.3 History Length Management</a></li>
      </ul></li>
      <li><a id="toc-Using-num_005fctx-Parameter" href="#Using-num_005fctx-Parameter">8.7.2 Using num_ctx Parameter</a></li>
    </ul></li>
    <li><a id="toc-Context-Display-Configuration" href="#Context-Display-Configuration">8.8 Context Display Configuration</a></li>
    <li><a id="toc-Fallback-Context-Sizes" href="#Fallback-Context-Sizes">8.9 Fallback Context Sizes</a></li>
    <li><a id="toc-Troubleshooting-Context-Issues" href="#Troubleshooting-Context-Issues">8.10 Troubleshooting Context Issues</a></li>
  </ul></li>
  <li><a id="toc-File-Attachments-1" href="#File-Attachments">9 File Attachments</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview" href="#Overview">9.1 Overview</a></li>
    <li><a id="toc-Supported-File-Types" href="#Supported-File-Types">9.2 Supported File Types</a></li>
    <li><a id="toc-Attaching-Files" href="#Attaching-Files">9.3 Attaching Files</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Basic-File-Attachment" href="#Basic-File-Attachment">9.3.1 Basic File Attachment</a></li>
      <li><a id="toc-Dired-Integration" href="#Dired-Integration">9.3.2 Dired Integration</a></li>
    </ul></li>
    <li><a id="toc-Managing-Attachments" href="#Managing-Attachments">9.4 Managing Attachments</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Viewing-Attachments" href="#Viewing-Attachments">9.4.1 Viewing Attachments</a></li>
      <li><a id="toc-Detaching-Files" href="#Detaching-Files">9.4.2 Detaching Files</a></li>
      <li><a id="toc-Clearing-All-Attachments" href="#Clearing-All-Attachments">9.4.3 Clearing All Attachments</a></li>
    </ul></li>
    <li><a id="toc-File-Size-Limits" href="#File-Size-Limits">9.5 File Size Limits</a></li>
    <li><a id="toc-How-Attachments-Work" href="#How-Attachments-Work">9.6 How Attachments Work</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Context-Integration" href="#Context-Integration">9.6.1 Context Integration</a></li>
      <li><a id="toc-Session-Persistence" href="#Session-Persistence">9.6.2 Session Persistence</a></li>
    </ul></li>
    <li><a id="toc-File-Attachment-Workflow-Examples" href="#File-Attachment-Workflow-Examples">9.7 File Attachment Workflow Examples</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Code-Review-Workflow" href="#Code-Review-Workflow">9.7.1 Code Review Workflow</a></li>
      <li><a id="toc-Multi_002dFile-Analysis" href="#Multi_002dFile-Analysis">9.7.2 Multi-File Analysis</a></li>
      <li><a id="toc-Configuration-Troubleshooting" href="#Configuration-Troubleshooting">9.7.3 Configuration Troubleshooting</a></li>
    </ul></li>
    <li><a id="toc-Context-Considerations" href="#Context-Considerations">9.8 Context Considerations</a></li>
    <li><a id="toc-Best-Practices" href="#Best-Practices">9.9 Best Practices</a></li>
    <li><a id="toc-Troubleshooting-Attachments" href="#Troubleshooting-Attachments">9.10 Troubleshooting Attachments</a></li>
  </ul></li>
  <li><a id="toc-Web-Search" href="#Web-Search">10 Web Search</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview-1" href="#Overview-1">10.1 Overview</a></li>
    <li><a id="toc-How-It-Works" href="#How-It-Works">10.2 How It Works</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-eww-Mode-_0028Default_002c-Recommended_0029" href="#eww-Mode-_0028Default_002c-Recommended_0029">10.2.1 eww Mode (Default, Recommended)</a></li>
      <li><a id="toc-API-Mode-_0028Experimental_0029" href="#API-Mode-_0028Experimental_0029">10.2.2 API Mode (Experimental)</a></li>
    </ul></li>
    <li><a id="toc-Configuration-1" href="#Configuration-2">10.3 Configuration</a></li>
    <li><a id="toc-Usage" href="#Usage">10.4 Usage</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Inline-Search-Syntax" href="#Inline-Search-Syntax">10.4.1 Inline Search Syntax</a></li>
      <li><a id="toc-Manual-Search-Commands" href="#Manual-Search-Commands">10.4.2 Manual Search Commands</a></li>
      <li><a id="toc-Transient-Menu" href="#Transient-Menu">10.4.3 Transient Menu</a></li>
    </ul></li>
    <li><a id="toc-Viewing-Web-Search-Results" href="#Viewing-Web-Search-Results">10.5 Viewing Web Search Results</a></li>
    <li><a id="toc-Managing-Web-Searches" href="#Managing-Web-Searches">10.6 Managing Web Searches</a></li>
    <li><a id="toc-Provider-Support" href="#Provider-Support">10.7 Provider Support</a></li>
    <li><a id="toc-Troubleshooting" href="#Troubleshooting-1">10.8 Troubleshooting</a></li>
  </ul></li>
  <li><a id="toc-Parameter-Control-1" href="#Parameter-Control">11 Parameter Control</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Understanding-Parameters" href="#Understanding-Parameters">11.1 Understanding Parameters</a></li>
    <li><a id="toc-Viewing-Current-Parameters" href="#Viewing-Current-Parameters">11.2 Viewing Current Parameters</a></li>
    <li><a id="toc-Editing-Parameters" href="#Editing-Parameters">11.3 Editing Parameters</a></li>
    <li><a id="toc-Parameter-Profiles" href="#Parameter-Profiles">11.4 Parameter Profiles</a></li>
    <li><a id="toc-Command_002dSpecific-Parameters" href="#Command_002dSpecific-Parameters">11.5 Command-Specific Parameters</a></li>
    <li><a id="toc-Reset-Parameters" href="#Reset-Parameters">11.6 Reset Parameters</a></li>
    <li><a id="toc-Displaying-Parameters-in-Header" href="#Displaying-Parameters-in-Header">11.7 Displaying Parameters in Header</a></li>
  </ul></li>
  <li><a id="toc-Session-Management" href="#Session-Management">12 Session Management</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Understanding-Sessions" href="#Understanding-Sessions">12.1 Understanding Sessions</a></li>
    <li><a id="toc-Creating-a-New-Session" href="#Creating-a-New-Session">12.2 Creating a New Session</a></li>
    <li><a id="toc-Saving-a-Session" href="#Saving-a-Session">12.3 Saving a Session</a></li>
    <li><a id="toc-Loading-a-Session" href="#Loading-a-Session">12.4 Loading a Session</a></li>
    <li><a id="toc-Managing-Sessions" href="#Managing-Sessions">12.5 Managing Sessions</a></li>
    <li><a id="toc-Conversation-History" href="#Conversation-History">12.6 Conversation History</a></li>
  </ul></li>
  <li><a id="toc-User-System-Prompts" href="#User-System-Prompts">13 User System Prompts</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview-2" href="#Overview-2">13.1 Overview</a></li>
    <li><a id="toc-Accessing-the-System-Prompts-Menu" href="#Accessing-the-System-Prompts-Menu">13.2 Accessing the System Prompts Menu</a></li>
    <li><a id="toc-Saving-System-Prompts" href="#Saving-System-Prompts">13.3 Saving System Prompts</a></li>
    <li><a id="toc-Loading-Saved-Prompts" href="#Loading-Saved-Prompts">13.4 Loading Saved Prompts</a></li>
    <li><a id="toc-Managing-Your-Prompt-Library" href="#Managing-Your-Prompt-Library">13.5 Managing Your Prompt Library</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Viewing-All-Prompts" href="#Viewing-All-Prompts">13.5.1 Viewing All Prompts</a></li>
      <li><a id="toc-Editing-Prompts" href="#Editing-Prompts">13.5.2 Editing Prompts</a></li>
      <li><a id="toc-Creating-New-Prompts" href="#Creating-New-Prompts">13.5.3 Creating New Prompts</a></li>
      <li><a id="toc-Deleting-Prompts" href="#Deleting-Prompts">13.5.4 Deleting Prompts</a></li>
    </ul></li>
    <li><a id="toc-Categories-and-Organization" href="#Categories-and-Organization">13.6 Categories and Organization</a></li>
    <li><a id="toc-Prompt-Storage-Format" href="#Prompt-Storage-Format">13.7 Prompt Storage Format</a></li>
    <li><a id="toc-Best-Practices-for-System-Prompts" href="#Best-Practices-for-System-Prompts">13.8 Best Practices for System Prompts</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Components-of-Effective-Prompts" href="#Components-of-Effective-Prompts">13.8.1 Components of Effective Prompts</a></li>
      <li><a id="toc-Example-Patterns" href="#Example-Patterns">13.8.2 Example Patterns</a></li>
    </ul></li>
    <li><a id="toc-Example-System-Prompts" href="#Example-System-Prompts">13.9 Example System Prompts</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Technical-Writing-Assistant" href="#Technical-Writing-Assistant">13.9.1 Technical Writing Assistant</a></li>
      <li><a id="toc-Code-Reviewer" href="#Code-Reviewer">13.9.2 Code Reviewer</a></li>
    </ul></li>
    <li><a id="toc-Workflow-Examples" href="#Workflow-Examples">13.10 Workflow Examples</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Python-Code-Assistance" href="#Python-Code-Assistance">13.10.1 Python Code Assistance</a></li>
      <li><a id="toc-Technical-Writing-Help" href="#Technical-Writing-Help">13.10.2 Technical Writing Help</a></li>
    </ul></li>
    <li><a id="toc-Integration-with-Roles" href="#Integration-with-Roles">13.11 Integration with Roles</a></li>
  </ul></li>
  <li><a id="toc-Roles-and-Commands" href="#Roles-and-Commands">14 Roles and Commands</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Understanding-Roles" href="#Understanding-Roles">14.1 Understanding Roles</a></li>
    <li><a id="toc-Role-File-Naming-Convention" href="#Role-File-Naming-Convention">14.2 Role File Naming Convention</a></li>
    <li><a id="toc-Built_002din-Commands" href="#Built_002din-Commands">14.3 Built-in Commands</a></li>
    <li><a id="toc-Creating-Custom-Roles" href="#Creating-Custom-Roles">14.4 Creating Custom Roles</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Interactive-Role-Creator" href="#Interactive-Role-Creator">14.4.1 Interactive Role Creator</a></li>
      <li><a id="toc-Manual-Role-Creation" href="#Manual-Role-Creation">14.4.2 Manual Role Creation</a></li>
    </ul></li>
    <li><a id="toc-Switching-Roles" href="#Switching-Roles">14.5 Switching Roles</a></li>
    <li><a id="toc-Managing-Role-Files" href="#Managing-Role-Files">14.6 Managing Role Files</a></li>
    <li><a id="toc-Advanced-Role-Customization" href="#Advanced-Role-Customization">14.7 Advanced Role Customization</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Command_002dSpecific-Models" href="#Command_002dSpecific-Models">14.7.1 Command-Specific Models</a></li>
      <li><a id="toc-Command_002dSpecific-Parameters-1" href="#Command_002dSpecific-Parameters-1">14.7.2 Command-Specific Parameters</a></li>
      <li><a id="toc-Creating-New-Commands" href="#Creating-New-Commands">14.7.3 Creating New Commands</a></li>
    </ul></li>
    <li><a id="toc-Role-Examples" href="#Role-Examples">14.8 Role Examples</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Programming-Role" href="#Programming-Role">14.8.1 Programming Role</a></li>
      <li><a id="toc-Writing-Role" href="#Writing-Role">14.8.2 Writing Role</a></li>
    </ul></li>
    <li><a id="toc-Tips-for-Effective-Role-Usage" href="#Tips-for-Effective-Role-Usage">14.9 Tips for Effective Role Usage</a></li>
  </ul></li>
  <li><a id="toc-Fabric-Pattern-Integration" href="#Fabric-Pattern-Integration">15 Fabric Pattern Integration</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-What-are-Fabric-Patterns_003f" href="#What-are-Fabric-Patterns_003f">15.1 What are Fabric Patterns?</a></li>
    <li><a id="toc-Setting-Up-Fabric-Integration" href="#Setting-Up-Fabric-Integration">15.2 Setting Up Fabric Integration</a></li>
    <li><a id="toc-Using-Fabric-Patterns" href="#Using-Fabric-Patterns">15.3 Using Fabric Patterns</a></li>
    <li><a id="toc-Browsing-Available-Patterns" href="#Browsing-Available-Patterns">15.4 Browsing Available Patterns</a></li>
    <li><a id="toc-Viewing-Pattern-Details" href="#Viewing-Pattern-Details">15.5 Viewing Pattern Details</a></li>
    <li><a id="toc-Updating-Patterns" href="#Updating-Patterns">15.6 Updating Patterns</a></li>
    <li><a id="toc-Using-Patterns-by-Category" href="#Using-Patterns-by-Category">15.7 Using Patterns by Category</a></li>
  </ul></li>
  <li><a id="toc-Awesome-ChatGPT-Prompts" href="#Awesome-ChatGPT-Prompts">16 Awesome ChatGPT Prompts</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-What-is-Awesome-ChatGPT-Prompts_003f" href="#What-is-Awesome-ChatGPT-Prompts_003f">16.1 What is Awesome ChatGPT Prompts?</a></li>
    <li><a id="toc-Setting-Up-Awesome-ChatGPT-Prompts" href="#Setting-Up-Awesome-ChatGPT-Prompts">16.2 Setting Up Awesome ChatGPT Prompts</a></li>
    <li><a id="toc-Using-Awesome-ChatGPT-Prompts" href="#Using-Awesome-ChatGPT-Prompts">16.3 Using Awesome ChatGPT Prompts</a></li>
    <li><a id="toc-Browsing-Available-Prompts" href="#Browsing-Available-Prompts">16.4 Browsing Available Prompts</a></li>
    <li><a id="toc-Categorized-Browsing" href="#Categorized-Browsing">16.5 Categorized Browsing</a></li>
    <li><a id="toc-Viewing-Prompt-Details" href="#Viewing-Prompt-Details">16.6 Viewing Prompt Details</a></li>
    <li><a id="toc-Updating-Prompts" href="#Updating-Prompts">16.7 Updating Prompts</a></li>
    <li><a id="toc-Setting-Without-Sending" href="#Setting-Without-Sending">16.8 Setting Without Sending</a></li>
    <li><a id="toc-Example-Usage" href="#Example-Usage">16.9 Example Usage</a></li>
  </ul></li>
  <li><a id="toc-Remote-Providers" href="#Remote-Providers">17 Remote Providers</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview-3" href="#Overview-3">17.1 Overview</a></li>
    <li><a id="toc-Status-Line-Indicators" href="#Status-Line-Indicators">17.2 Status Line Indicators</a></li>
    <li><a id="toc-Setting-Up-API-Access" href="#Setting-Up-API-Access">17.3 Setting Up API Access</a></li>
    <li><a id="toc-Selecting-Remote-Models" href="#Selecting-Remote-Models">17.4 Selecting Remote Models</a></li>
    <li><a id="toc-Provider-Configuration" href="#Provider-Configuration">17.5 Provider Configuration</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-OpenAI-Configuration" href="#OpenAI-Configuration">17.5.1 OpenAI Configuration</a></li>
      <li><a id="toc-Claude-Configuration" href="#Claude-Configuration">17.5.2 Claude Configuration</a></li>
      <li><a id="toc-Gemini-Configuration" href="#Gemini-Configuration">17.5.3 Gemini Configuration</a></li>
      <li><a id="toc-Grok-Configuration" href="#Grok-Configuration">17.5.4 Grok Configuration</a></li>
      <li><a id="toc-GitHub-Copilot-Configuration" href="#GitHub-Copilot-Configuration">17.5.5 GitHub Copilot Configuration</a></li>
      <li><a id="toc-Codestral-Configuration" href="#Codestral-Configuration">17.5.6 Codestral Configuration</a></li>
    </ul></li>
    <li><a id="toc-History-Management" href="#History-Management">17.6 History Management</a></li>
  </ul></li>
  <li><a id="toc-Ollama-Cloud-Models" href="#Ollama-Cloud-Models">18 Ollama Cloud Models</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview-4" href="#Overview-4">18.1 Overview</a></li>
    <li><a id="toc-Available-Cloud-Models" href="#Available-Cloud-Models">18.2 Available Cloud Models</a></li>
    <li><a id="toc-Selecting-Cloud-Models" href="#Selecting-Cloud-Models">18.3 Selecting Cloud Models</a></li>
    <li><a id="toc-Cloud-Authentication" href="#Cloud-Authentication">18.4 Cloud Authentication</a></li>
    <li><a id="toc-Cloud-Model-Indicator" href="#Cloud-Model-Indicator">18.5 Cloud Model Indicator</a></li>
    <li><a id="toc-Configuration-2" href="#Configuration-3">18.6 Configuration</a></li>
  </ul></li>
  <li><a id="toc-Global-System-Prompt" href="#Global-System-Prompt">19 Global System Prompt</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Overview-5" href="#Overview-5">19.1 Overview</a></li>
    <li><a id="toc-Configuration-3" href="#Configuration-4">19.2 Configuration</a></li>
    <li><a id="toc-Toggling-Global-Prompt" href="#Toggling-Global-Prompt">19.3 Toggling Global Prompt</a></li>
    <li><a id="toc-How-It-Works-1" href="#How-It-Works-1">19.4 How It Works</a></li>
  </ul></li>
  <li><a id="toc-Advanced-Usage" href="#Advanced-Usage">20 Advanced Usage</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Managing-Token-Usage" href="#Managing-Token-Usage">20.1 Managing Token Usage</a></li>
    <li><a id="toc-Customizing-the-Interface" href="#Customizing-the-Interface">20.2 Customizing the Interface</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Streaming-Options" href="#Streaming-Options">20.2.1 Streaming Options</a></li>
      <li><a id="toc-Debug-Mode" href="#Debug-Mode">20.2.2 Debug Mode</a></li>
    </ul></li>
    <li><a id="toc-Editing-Conversation-History" href="#Editing-Conversation-History">20.3 Editing Conversation History</a></li>
    <li><a id="toc-Advanced-System-Prompt-Management" href="#Advanced-System-Prompt-Management">20.4 Advanced System Prompt Management</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Setting-a-system-prompt-without-sending" href="#Setting-a-system-prompt-without-sending">20.4.1 Setting a system prompt without sending</a></li>
      <li><a id="toc-Using-a-system-prompt-from-Fabric" href="#Using-a-system-prompt-from-Fabric">20.4.2 Using a system prompt from Fabric</a></li>
    </ul></li>
    <li><a id="toc-Using-Direct-API-Access" href="#Using-Direct-API-Access">20.5 Using Direct API Access</a></li>
  </ul></li>
  <li><a id="toc-API-Reference" href="#API-Reference">21 API Reference</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Interactive-Functions" href="#Interactive-Functions">21.1 Interactive Functions</a></li>
    <li><a id="toc-Core-Functions" href="#Core-Functions">21.2 Core Functions</a></li>
    <li><a id="toc-Customization-Functions" href="#Customization-Functions">21.3 Customization Functions</a></li>
  </ul></li>
  <li><a id="toc-Frequently-Asked-Questions" href="#FAQ">22 Frequently Asked Questions</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-General-Questions" href="#General-Questions">22.1 General Questions</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f" href="#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f">22.1.1 What is the difference between Ollama Buddy and other AI assistants?</a></li>
      <li><a id="toc-Does-Ollama-Buddy-require-an-internet-connection_003f" href="#Does-Ollama-Buddy-require-an-internet-connection_003f">22.1.2 Does Ollama Buddy require an internet connection?</a></li>
      <li><a id="toc-How-do-I-use-Ollama-cloud-models_003f" href="#How-do-I-use-Ollama-cloud-models_003f">22.1.3 How do I use Ollama cloud models?</a></li>
      <li><a id="toc-Which-models-work-best-with-Ollama-Buddy_003f" href="#Which-models-work-best-with-Ollama-Buddy_003f">22.1.4 Which models work best with Ollama Buddy?</a></li>
      <li><a id="toc-How-much-RAM-do-I-need_003f" href="#How-much-RAM-do-I-need_003f">22.1.5 How much RAM do I need?</a></li>
    </ul></li>
    <li><a id="toc-Usage-Questions" href="#Usage-Questions">22.2 Usage Questions</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-How-do-I-cancel-a-request-that_0027s-taking-too-long_003f" href="#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f">22.2.1 How do I cancel a request that&rsquo;s taking too long?</a></li>
      <li><a id="toc-How-can-I-save-my-conversations_003f" href="#How-can-I-save-my-conversations_003f">22.2.2 How can I save my conversations?</a></li>
      <li><a id="toc-Can-I-use-multiple-models-in-the-same-conversation_003f" href="#Can-I-use-multiple-models-in-the-same-conversation_003f">22.2.3 Can I use multiple models in the same conversation?</a></li>
      <li><a id="toc-How-do-I-clear-the-conversation-history_003f" href="#How-do-I-clear-the-conversation-history_003f">22.2.4 How do I clear the conversation history?</a></li>
      <li><a id="toc-How-can-I-create-a-custom-command_003f" href="#How-can-I-create-a-custom-command_003f">22.2.5 How can I create a custom command?</a></li>
      <li><a id="toc-How-can-I-manage-context-windows_003f" href="#How-can-I-manage-context-windows_003f">22.2.6 How can I manage context windows?</a></li>
      <li><a id="toc-What-happens-when-I-exceed-the-context-limit_003f" href="#What-happens-when-I-exceed-the-context-limit_003f">22.2.7 What happens when I exceed the context limit?</a></li>
      <li><a id="toc-How-do-I-create-effective-system-prompts_003f" href="#How-do-I-create-effective-system-prompts_003f">22.2.8 How do I create effective system prompts?</a></li>
      <li><a id="toc-Where-are-my-system-prompts-stored_003f" href="#Where-are-my-system-prompts-stored_003f">22.2.9 Where are my system prompts stored?</a></li>
    </ul></li>
    <li><a id="toc-Troubleshooting-1" href="#Troubleshooting-2">22.3 Troubleshooting</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Ollama-Buddy-shows-_0022OFFLINE_0022-status" href="#Ollama-Buddy-shows-_0022OFFLINE_0022-status">22.3.1 Ollama Buddy shows &quot;OFFLINE&quot; status</a></li>
      <li><a id="toc-Responses-are-slow-or-the-model-seems-to-hang" href="#Responses-are-slow-or-the-model-seems-to-hang">22.3.2 Responses are slow or the model seems to hang</a></li>
      <li><a id="toc-Getting-_0022error-parsing-model_0022-when-pulling-a-model" href="#Getting-_0022error-parsing-model_0022-when-pulling-a-model">22.3.3 Getting &quot;error parsing model&quot; when pulling a model</a></li>
      <li><a id="toc-Model-responses-are-low-quality-or-truncated" href="#Model-responses-are-low-quality-or-truncated">22.3.4 Model responses are low quality or truncated</a></li>
      <li><a id="toc-How-do-system-prompts-differ-from-regular-prompts_003f" href="#How-do-system-prompts-differ-from-regular-prompts_003f">22.3.5 How do system prompts differ from regular prompts?</a></li>
      <li><a id="toc-What-is-the-global-system-prompt_003f" href="#What-is-the-global-system-prompt_003f">22.3.6 What is the global system prompt?</a></li>
      <li><a id="toc-How-do-I-use-image-analysis_002fvision-features_003f" href="#How-do-I-use-image-analysis_002fvision-features_003f">22.3.7 How do I use image analysis/vision features?</a></li>
      <li><a id="toc-Can-I-share-system-prompts-between-different-installations_003f" href="#Can-I-share-system-prompts-between-different-installations_003f">22.3.8 Can I share system prompts between different installations?</a></li>
    </ul></li>
  </ul></li>
  <li><a id="toc-Troubleshooting-2" href="#Troubleshooting">23 Troubleshooting</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Common-Issues" href="#Common-Issues">23.1 Common Issues</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Connection-Problems" href="#Connection-Problems">23.1.1 Connection Problems</a></li>
      <li><a id="toc-Model-Problems" href="#Model-Problems">23.1.2 Model Problems</a></li>
      <li><a id="toc-Interface-Issues" href="#Interface-Issues">23.1.3 Interface Issues</a></li>
    </ul></li>
    <li><a id="toc-Debugging" href="#Debugging">23.2 Debugging</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Enable-Debug-Mode" href="#Enable-Debug-Mode">23.2.1 Enable Debug Mode</a></li>
      <li><a id="toc-Check-Logs" href="#Check-Logs">23.2.2 Check Logs</a></li>
      <li><a id="toc-Report-Issues" href="#Report-Issues">23.2.3 Report Issues</a></li>
    </ul></li>
  </ul></li>
  <li><a id="toc-Contributing" href="#Contributing">24 Contributing</a>
  <ul class="toc-numbered-mark">
    <li><a id="toc-Getting-Started" href="#Getting-Started">24.1 Getting Started</a></li>
    <li><a id="toc-Development-Setup" href="#Development-Setup">24.2 Development Setup</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Required-Tools" href="#Required-Tools">24.2.1 Required Tools</a></li>
      <li><a id="toc-Recommended-Packages" href="#Recommended-Packages">24.2.2 Recommended Packages</a></li>
    </ul></li>
    <li><a id="toc-Coding-Guidelines" href="#Coding-Guidelines">24.3 Coding Guidelines</a></li>
    <li><a id="toc-Testing" href="#Testing">24.4 Testing</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-Run-Existing-Tests" href="#Run-Existing-Tests">24.4.1 Run Existing Tests</a></li>
      <li><a id="toc-Adding-New-Tests" href="#Adding-New-Tests">24.4.2 Adding New Tests</a></li>
    </ul></li>
    <li><a id="toc-Feature-Requests-and-Bug-Reports" href="#Feature-Requests-and-Bug-Reports">24.5 Feature Requests and Bug Reports</a>
    <ul class="toc-numbered-mark">
      <li><a id="toc-User-System-Prompts-Issues" href="#User-System-Prompts-Issues">24.5.1 User System Prompts Issues</a></li>
    </ul></li>
  </ul></li>
  <li><a id="toc-Index" href="#Index" rel="index">Index</a></li>
</ul>
</div>
</div>
<hr>
<div class="chapter-level-extent" id="Introduction">
<div class="nav-panel">
<p>
Next: <a href="#Installation" accesskey="n" rel="next">Installation</a>, Previous: <a href="#Top" accesskey="p" rel="prev">Ollama Buddy</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Introduction-1"><span>1 Introduction<a class="copiable-link" href="#Introduction-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#What-is-Ollama-Buddy_003f" accesskey="1">What is Ollama Buddy?</a></li>
<li><a href="#Why-Use-Ollama-Buddy_003f" accesskey="2">Why Use Ollama Buddy?</a></li>
<li><a href="#Prerequisites" accesskey="3">Prerequisites</a></li>
</ul>
<div class="section-level-extent" id="What-is-Ollama-Buddy_003f">
<h3 class="section"><span>1.1 What is Ollama Buddy?<a class="copiable-link" href="#What-is-Ollama-Buddy_003f"> &para;</a></span></h3>

<p>Ollama Buddy is an Emacs package that provides a friendly AI assistant interface to Ollama, 
a tool for running large language models (LLMs) locally on your computer. It allows you to 
interact with AI models directly from within Emacs for various tasks such as:
</p>
<ul class="itemize mark-bullet">
<li>Code refactoring and explanation
</li><li>Writing assistance and proofreading
</li><li>Generating Git commit messages
</li><li>Dictionary lookups and language assistance
</li><li>Custom AI-powered workflows via roles
</li><li>Using pre-built prompt templates from Fabric
</li><li>Utilizing Awesome ChatGPT Prompts
</li><li>Integrating with commercial APIs (OpenAI, Claude, Gemini, Grok, Copilot, Codestral)
</li><li>Using Ollama cloud models
</li><li>Including files as context in conversations
</li><li>Real-time web search to provide current information to LLMs
</li></ul>

<p>Instead of context-switching to web interfaces or terminal applications, Ollama Buddy brings 
the power of local LLMs right into your Emacs workflow.
</p>
</div>
<div class="section-level-extent" id="Why-Use-Ollama-Buddy_003f">
<h3 class="section"><span>1.2 Why Use Ollama Buddy?<a class="copiable-link" href="#Why-Use-Ollama-Buddy_003f"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li><strong class="strong">Privacy</strong>: All interactions happen locally with Ollama - no data sent to external services unless you use commercial APIs
</li><li><strong class="strong">Integration</strong>: Seamlessly fits into your existing Emacs workflow
</li><li><strong class="strong">Flexibility</strong>: Supports multiple models, parameter tuning, and custom commands
</li><li><strong class="strong">Efficiency</strong>: Quick access to AI assistance without leaving your editor
</li><li><strong class="strong">Extensibility</strong>: Create custom roles and commands for your specific needs
</li><li><strong class="strong">File Support</strong>: Include text files, code, and documentation directly in conversations
</li></ul>

</div>
<div class="section-level-extent" id="Prerequisites">
<h3 class="section"><span>1.3 Prerequisites<a class="copiable-link" href="#Prerequisites"> &para;</a></span></h3>

<p>Before using Ollama Buddy, you need:
</p>
<ul class="itemize mark-bullet">
<li>Emacs 28.1 or later
</li><li>Ollama installed and running on your system (see <a class="url" href="https://ollama.ai">https://ollama.ai</a>)
</li><li>At least one language model pulled into Ollama
</li><li>(Optional) API keys for OpenAI or Claude if you want to use those services
</li></ul>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Installation">
<div class="nav-panel">
<p>
Next: <a href="#Configuration" accesskey="n" rel="next">Configuration</a>, Previous: <a href="#Introduction" accesskey="p" rel="prev">Introduction</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Installation-1"><span>2 Installation<a class="copiable-link" href="#Installation-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Installing-Ollama" accesskey="1">Installing Ollama</a></li>
<li><a href="#Package-Installation" accesskey="2">Package Installation</a></li>
<li><a href="#Dependencies" accesskey="3">Dependencies</a></li>
<li><a href="#Communication-Backends" accesskey="4">Communication Backends</a></li>
<li><a href="#API-Key-Setup" accesskey="5">API Key Setup</a></li>
</ul>
<div class="section-level-extent" id="Installing-Ollama">
<h3 class="section"><span>2.1 Installing Ollama<a class="copiable-link" href="#Installing-Ollama"> &para;</a></span></h3>

<p>Before installing Ollama Buddy, you need to install Ollama itself:
</p>
<ol class="enumerate">
<li> Visit <a class="url" href="https://ollama.ai">https://ollama.ai</a> and download the installer for your platform
</li><li> Install and run Ollama according to the instructions
</li><li> Pull at least one model using <code class="code">ollama pull llama3:latest</code> (or another model of your choice)
</li></ol>

</div>
<div class="section-level-extent" id="Package-Installation">
<h3 class="section"><span>2.2 Package Installation<a class="copiable-link" href="#Package-Installation"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Using-package_002eel" accesskey="1">Using package.el</a></li>
<li><a href="#Using-use_002dpackage" accesskey="2">Using use-package</a></li>
<li><a href="#Manual-Installation" accesskey="3">Manual Installation</a></li>
</ul>
<div class="subsection-level-extent" id="Using-package_002eel">
<h4 class="subsection"><span>2.2.1 Using package.el<a class="copiable-link" href="#Using-package_002eel"> &para;</a></span></h4>

<p>The recommended way to install Ollama Buddy is through MELPA:
</p>
<div class="example">
<pre class="example-preformatted">M-x package-install RET ollama-buddy RET
</pre></div>

</div>
<div class="subsection-level-extent" id="Using-use_002dpackage">
<h4 class="subsection"><span>2.2.2 Using use-package<a class="copiable-link" href="#Using-use_002dpackage"> &para;</a></span></h4>

<p>If you use <code class="code">use-package</code>, add the following to your Emacs configuration:
</p>
<div class="example">
<pre class="example-preformatted">(use-package ollama-buddy
  :ensure t
  :bind (&quot;C-c o&quot; . ollama-buddy-menu))
</pre></div>

<p>With a default model:
</p>
<div class="example">
<pre class="example-preformatted">(use-package ollama-buddy
  :ensure t
  :bind (&quot;C-c o&quot; . ollama-buddy-menu)
  :custom (ollama-buddy-default-model &quot;llama3:latest&quot;))
</pre></div>

</div>
<div class="subsection-level-extent" id="Manual-Installation">
<h4 class="subsection"><span>2.2.3 Manual Installation<a class="copiable-link" href="#Manual-Installation"> &para;</a></span></h4>

<p>To install manually:
</p>
<ol class="enumerate">
<li> Clone the repository:
<div class="example">
<pre class="example-preformatted">git clone https://github.com/captainflasmr/ollama-buddy.git
</pre></div>

</li><li> Add to your configuration:
<div class="example">
<pre class="example-preformatted">(add-to-list 'load-path &quot;/path/to/ollama-buddy&quot;)
(require 'ollama-buddy)
(global-set-key (kbd &quot;C-c o&quot;) #'ollama-buddy-menu)
</pre></div>
</li></ol>

</div>
</div>
<div class="section-level-extent" id="Dependencies">
<h3 class="section"><span>2.3 Dependencies<a class="copiable-link" href="#Dependencies"> &para;</a></span></h3>

<p>Ollama Buddy requires the following:
</p>
<ul class="itemize mark-bullet">
<li>Emacs 28.1 or later
</li><li>(Optional) transient 0.4.0+ for advanced menu system
</li></ul>

<p>Built-in packages used: json, cl-lib, url, subr-x, dired, org, savehist.
</p>
</div>
<div class="section-level-extent" id="Communication-Backends">
<h3 class="section"><span>2.4 Communication Backends<a class="copiable-link" href="#Communication-Backends"> &para;</a></span></h3>

<p>Ollama Buddy supports two communication backends:
</p>
<dl class="table">
<dt><code class="code">network-process (default)</code></dt>
<dd><p>Uses Emacs built-in network process.
</p>
</dd>
<dt><code class="code">curl</code></dt>
<dd><p>Uses external curl command for requests. Useful as a fallback when network process has issues.
</p></dd>
</dl>

<p>To switch backends:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-switch-communication-backend
</pre></div>
<p>or press <code class="code">C-c e</code> in the chat buffer.
</p>
<p>To configure curl backend:
</p><div class="example">
<pre class="example-preformatted">(require 'ollama-buddy-curl)
(setq ollama-buddy-communication-backend 'curl)
(setq ollama-buddy-curl-executable &quot;curl&quot;)
(setq ollama-buddy-curl-timeout 300)
</pre></div>

</div>
<div class="section-level-extent" id="API-Key-Setup">
<h3 class="section"><span>2.5 API Key Setup<a class="copiable-link" href="#API-Key-Setup"> &para;</a></span></h3>

<p>If you want to use OpenAI or Claude integration, you&rsquo;ll need to set up API keys securely:
</p>
<ol class="enumerate">
<li> Use Emacs built-in auth-source for secure storage
</li><li> Add to your auth sources (e.g., ~/.authinfo.gpg):
<div class="example">
<pre class="example-preformatted">machine api.openai.com login apikey password YOUR_OPENAI_API_KEY_HERE
machine api.anthropic.com login apikey password YOUR_CLAUDE_API_KEY_HERE
</pre></div>
</li><li> Alternatively, set the variables directly (less secure):
<div class="example">
<pre class="example-preformatted">(setq ollama-buddy-openai-api-key &quot;your-openai-key&quot;)
(setq ollama-buddy-claude-api-key &quot;your-claude-key&quot;)
</pre></div>
</li></ol>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Configuration">
<div class="nav-panel">
<p>
Next: <a href="#Quick-Start" accesskey="n" rel="next">Quick Start</a>, Previous: <a href="#Installation" accesskey="p" rel="prev">Installation</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Configuration-1"><span>3 Configuration<a class="copiable-link" href="#Configuration-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Basic-Configuration" accesskey="1">Basic Configuration</a></li>
<li><a href="#Display-Options" accesskey="2">Display Options</a></li>
<li><a href="#File-Attachment-Configuration" accesskey="3">File Attachment Configuration</a></li>
<li><a href="#Directory-Configuration" accesskey="4">Directory Configuration</a></li>
<li><a href="#History-and-Session-Configuration" accesskey="5">History and Session Configuration</a></li>
<li><a href="#Context-Management-Configuration" accesskey="6">Context Management Configuration</a></li>
<li><a href="#External-API-Configuration" accesskey="7">External API Configuration</a></li>
<li><a href="#Global-System-Prompt-Configuration" accesskey="8">Global System Prompt Configuration</a></li>
<li><a href="#Vision-Configuration" accesskey="9">Vision Configuration</a></li>
<li><a href="#Cloud-Model-Configuration">Cloud Model Configuration</a></li>
<li><a href="#Awesome-ChatGPT-Prompts-Configuration">Awesome ChatGPT Prompts Configuration</a></li>
</ul>
<div class="section-level-extent" id="Basic-Configuration">
<h3 class="section"><span>3.1 Basic Configuration<a class="copiable-link" href="#Basic-Configuration"> &para;</a></span></h3>

<p>Here are the essential configuration options:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-default-model</code></dt>
<dd><p>Set your preferred default model.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-default-model &quot;llama3:latest&quot;)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-host</code></dt>
<dd><p>Host where Ollama server is running (default: &quot;localhost&quot;).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-host &quot;localhost&quot;)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-port</code></dt>
<dd><p>Port where Ollama server is running (default: 11434).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-port 11434)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Display-Options">
<h3 class="section"><span>3.2 Display Options<a class="copiable-link" href="#Display-Options"> &para;</a></span></h3>

<p>Customize the appearance and behavior of Ollama Buddy:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-convert-markdown-to-org</code></dt>
<dd><p>Whether to automatically convert markdown to org-mode format in responses (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-convert-markdown-to-org t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-display-token-stats</code></dt>
<dd><p>Whether to display token usage statistics after responses (default: nil).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-display-token-stats t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-streaming-enabled</code></dt>
<dd><p>Whether to use streaming mode for responses (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-streaming-enabled t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-auto-scroll</code></dt>
<dd><p>Whether to auto-scroll the chat buffer during streaming output (default: nil).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-auto-scroll t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-pulse-response</code></dt>
<dd><p>Whether to pulse/flash the response text when streaming completes (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-pulse-response t)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="File-Attachment-Configuration">
<h3 class="section"><span>3.3 File Attachment Configuration<a class="copiable-link" href="#File-Attachment-Configuration"> &para;</a></span></h3>

<p>Configure file attachment behavior:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-max-file-size</code></dt>
<dd><p>Maximum size for attached files in bytes (default: 10MB).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-max-file-size (* 10 1024 1024))  ; 10MB
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-supported-file-types</code></dt>
<dd><p>List of regex patterns for supported file types (default includes text, code, and configuration files).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-supported-file-types
      '(&quot;\\.txt$&quot; &quot;\\.md$&quot; &quot;\\.org$&quot; &quot;\\.py$&quot; &quot;\\.js$&quot; &quot;\\.el$&quot;))
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Directory-Configuration">
<h3 class="section"><span>3.4 Directory Configuration<a class="copiable-link" href="#Directory-Configuration"> &para;</a></span></h3>

<p>Customize where Ollama Buddy stores its files:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-sessions-directory</code></dt>
<dd><p>Directory for storing session files.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-sessions-directory 
      (expand-file-name &quot;ollama-buddy-sessions&quot; user-emacs-directory))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-roles-directory</code></dt>
<dd><p>Directory for storing role preset files.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-roles-directory
      (expand-file-name &quot;ollama-buddy-presets&quot; user-emacs-directory))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-modelfile-directory</code></dt>
<dd><p>Directory for storing temporary Modelfiles.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-modelfile-directory
      (expand-file-name &quot;ollama-buddy-modelfiles&quot; user-emacs-directory))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-awesome-local-dir</code></dt>
<dd><p>Directory for storing Awesome ChatGPT Prompts.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-awesome-local-dir
      (expand-file-name &quot;awesome-chatgpt-prompts&quot; user-emacs-directory))
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="History-and-Session-Configuration">
<h3 class="section"><span>3.5 History and Session Configuration<a class="copiable-link" href="#History-and-Session-Configuration"> &para;</a></span></h3>

<p>Configure how conversation history is managed:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-history-enabled</code></dt>
<dd><p>Whether to use conversation history in Ollama requests (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-history-enabled t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-max-history-length</code></dt>
<dd><p>Maximum number of message pairs to keep in conversation history (default: 10).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-max-history-length 10)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-show-history-indicator</code></dt>
<dd><p>Whether to show the history indicator in the header line (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-show-history-indicator t)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Context-Management-Configuration">
<h3 class="section"><span>3.6 Context Management Configuration<a class="copiable-link" href="#Context-Management-Configuration"> &para;</a></span></h3>

<p>Configure how Ollama Buddy handles context management:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-show-context-percentage</code></dt>
<dd><p>Whether to show context percentage in the status bar (default: nil).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-show-context-percentage t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-fallback-context-sizes</code></dt>
<dd><p>Mapping of model names to their default context sizes.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-fallback-context-sizes
  '((&quot;llama3:8b&quot; . 4096)
    (&quot;codellama:7b&quot; . 8192)))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-max-history-length</code></dt>
<dd><p>Maximum number of message pairs to keep (affects context usage).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-max-history-length 10)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="External-API-Configuration">
<h3 class="section"><span>3.7 External API Configuration<a class="copiable-link" href="#External-API-Configuration"> &para;</a></span></h3>

<p>For remote provider integration, configure API keys using auth-source (recommended):
</p>
<div class="example">
<pre class="example-preformatted">;; In ~/.authinfo.gpg
machine ollama-buddy-openai login apikey password YOUR_KEY
machine ollama-buddy-claude login apikey password YOUR_KEY
machine ollama-buddy-gemini login apikey password YOUR_KEY
machine ollama-buddy-grok login apikey password YOUR_KEY
machine ollama-buddy-codestral login apikey password YOUR_KEY
</pre></div>

<p>Then in your Emacs config:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password
       :host &quot;ollama-buddy-openai&quot; :user &quot;apikey&quot;))
</pre></div>

<ul class="mini-toc">
<li><a href="#Provider-Prefixes" accesskey="1">Provider Prefixes</a></li>
</ul>
<div class="subsection-level-extent" id="Provider-Prefixes">
<h4 class="subsection"><span>3.7.1 Provider Prefixes<a class="copiable-link" href="#Provider-Prefixes"> &para;</a></span></h4>

<p>Each provider uses a prefix to identify its models:
</p><ul class="itemize mark-bullet">
<li><code class="code">a:</code> - OpenAI (e.g., <code class="code">a:gpt-4</code>)
</li><li><code class="code">c:</code> - Claude (e.g., <code class="code">c:claude-3-opus</code>)
</li><li><code class="code">g:</code> - Gemini (e.g., <code class="code">g:gemini-pro</code>)
</li><li><code class="code">k:</code> - Grok (e.g., <code class="code">k:grok-1</code>)
</li><li><code class="code">p:</code> - GitHub Copilot (e.g., <code class="code">p:gpt-4o</code>)
</li><li><code class="code">s:</code> - Codestral (e.g., <code class="code">s:codestral-latest</code>)
</li><li><code class="code">o:</code> - Ollama local (when remote providers are loaded)
</li></ul>

</div>
</div>
<div class="section-level-extent" id="Global-System-Prompt-Configuration">
<h3 class="section"><span>3.8 Global System Prompt Configuration<a class="copiable-link" href="#Global-System-Prompt-Configuration"> &para;</a></span></h3>

<p>Configure a persistent system prompt for all requests:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-global-system-prompt</code></dt>
<dd><p>The global prompt prepended to all requests.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-global-system-prompt
  &quot;Format responses in plain prose. Avoid markdown tables.&quot;)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-global-system-prompt-enabled</code></dt>
<dd><p>Whether to enable the global prompt (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-global-system-prompt-enabled t)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Vision-Configuration">
<h3 class="section"><span>3.9 Vision Configuration<a class="copiable-link" href="#Vision-Configuration"> &para;</a></span></h3>

<p>Configure vision support for image analysis:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-vision-enabled</code></dt>
<dd><p>Whether to enable vision support (default: t).
</p>
</dd>
<dt><code class="code">ollama-buddy-vision-models</code></dt>
<dd><p>Models known to support vision capabilities.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-vision-models
      '(&quot;gemma3:4b&quot; &quot;llama3.2:3b&quot; &quot;llama3.2:8b&quot;))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-image-formats</code></dt>
<dd><p>Supported image file formats.
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Cloud-Model-Configuration">
<h3 class="section"><span>3.10 Cloud Model Configuration<a class="copiable-link" href="#Cloud-Model-Configuration"> &para;</a></span></h3>

<p>Configure Ollama cloud models:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-cloud-models</code></dt>
<dd><p>List of available cloud models.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-cloud-models
  '(&quot;qwen3-coder:480b-cloud&quot;
    &quot;deepseek-v3.1:671b-cloud&quot;))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-ollama-executable</code></dt>
<dd><p>Path to the ollama CLI (default: &quot;ollama&quot;).
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Awesome-ChatGPT-Prompts-Configuration">
<h3 class="section"><span>3.11 Awesome ChatGPT Prompts Configuration<a class="copiable-link" href="#Awesome-ChatGPT-Prompts-Configuration"> &para;</a></span></h3>

<p>Configure the Awesome ChatGPT Prompts integration:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-awesome-repo-url</code></dt>
<dd><p>URL of the Awesome ChatGPT Prompts GitHub repository.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-awesome-repo-url &quot;https://github.com/f/awesome-chatgpt-prompts.git&quot;)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-awesome-update-on-startup</code></dt>
<dd><p>Whether to automatically update prompts when Emacs starts.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-awesome-update-on-startup nil)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-awesome-categorize-prompts</code></dt>
<dd><p>Whether to categorize prompts based on common keywords.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-awesome-categorize-prompts t)
</pre></div>
</dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Quick-Start">
<div class="nav-panel">
<p>
Next: <a href="#Core-Features" accesskey="n" rel="next">Core Features</a>, Previous: <a href="#Configuration" accesskey="p" rel="prev">Configuration</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Quick-Start-1"><span>4 Quick Start<a class="copiable-link" href="#Quick-Start-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Basic-Usage" accesskey="1">Basic Usage</a></li>
<li><a href="#Common-Operations" accesskey="2">Common Operations</a></li>
</ul>
<div class="section-level-extent" id="Basic-Usage">
<h3 class="section"><span>4.1 Basic Usage<a class="copiable-link" href="#Basic-Usage"> &para;</a></span></h3>

<ol class="enumerate">
<li> Launch Ollama Buddy:
<div class="example">
<pre class="example-preformatted">M-x ollama-buddy-menu
</pre></div>
<p>or use your configured keybinding (e.g., <code class="code">C-c o</code>).
</p>
</li><li> The menu will show available options. Press the corresponding key for the action you want.

</li><li> To open the chat interface, press <code class="code">o</code> or select &quot;Open Chat&quot;.

</li><li> In the chat buffer, type your prompt and press <code class="code">C-c C-c</code> to send it.

</li><li> The AI will respond in the chat buffer.
</li></ol>

</div>
<div class="section-level-extent" id="Common-Operations">
<h3 class="section"><span>4.2 Common Operations<a class="copiable-link" href="#Common-Operations"> &para;</a></span></h3>

<dl class="table">
<dt>Sending text from a file</dt>
<dd><p>Select text in any buffer, then press <code class="code">C-c o</code> and choose &quot;Send Region&quot; (or press <code class="code">l</code>).
</p>
</dd>
<dt>Refactoring code</dt>
<dd><p>Select code, press <code class="code">C-c o</code>, then choose &quot;Refactor Code&quot; (or press <code class="code">r</code>).
</p>
</dd>
<dt>Generating a commit message</dt>
<dd><p>Select your changes, press <code class="code">C-c o</code>, then choose &quot;Git Commit Message&quot; (or press <code class="code">g</code>).
</p>
</dd>
<dt>Changing models</dt>
<dd><p>Press <code class="code">C-c m</code> to switch between available models, or <code class="code">C-u C-c m</code> to select from cloud models.
</p>
</dd>
<dt>Using the transient menu</dt>
<dd><p>Press <code class="code">C-c O</code> for the main transient menu with all features organized by category.
</p>
</dd>
<dt>Attaching files</dt>
<dd><p>Press <code class="code">C-c C-a</code> to attach a file, or use the transient menu &quot;Attachments&quot; section.
</p>
</dd>
<dt>Toggling reasoning visibility</dt>
<dd><p>Press <code class="code">C-c V</code> to hide or show reasoning/thinking sections in responses.
</p>
</dd>
<dt>Using Awesome ChatGPT Prompts</dt>
<dd><p>Select text, press <code class="code">C-c o</code>, then <code class="code">a</code> for the Awesome prompts menu, then <code class="code">s</code> to send with a prompt.
</p>
</dd>
<dt>Using Fabric patterns</dt>
<dd><p>Select text, press <code class="code">C-c o</code>, then <code class="code">f</code> for the Fabric menu, then <code class="code">s</code> to send with a pattern.
</p>
</dd>
<dt>Getting help</dt>
<dd><p>In the chat buffer, press <code class="code">C-c h</code> to display the help screen with available commands and models.
</p></dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Core-Features">
<div class="nav-panel">
<p>
Next: <a href="#Chat-Interface" accesskey="n" rel="next">Chat Interface</a>, Previous: <a href="#Quick-Start" accesskey="p" rel="prev">Quick Start</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Core-Features-1"><span>5 Core Features<a class="copiable-link" href="#Core-Features-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Chat-Interface-1" accesskey="1">Chat Interface</a></li>
<li><a href="#Pre_002dbuilt-Commands" accesskey="2">Pre-built Commands</a></li>
<li><a href="#Model-Management" accesskey="3">Model Management</a></li>
<li><a href="#Parameter-Control-1" accesskey="4">Parameter Control</a></li>
<li><a href="#Roles-and-Custom-Commands" accesskey="5">Roles and Custom Commands</a></li>
<li><a href="#Prompt-Template-Collections" accesskey="6">Prompt Template Collections</a></li>
<li><a href="#External-API-Integration" accesskey="7">External API Integration</a></li>
<li><a href="#Vision-Support" accesskey="8">Vision Support</a></li>
<li><a href="#File-Attachments-1" accesskey="9">File Attachments</a></li>
</ul>
<div class="section-level-extent" id="Chat-Interface-1">
<h3 class="section"><span>5.1 Chat Interface<a class="copiable-link" href="#Chat-Interface-1"> &para;</a></span></h3>

<p>The chat interface is the main way to interact with Ollama Buddy:
</p>
<ul class="itemize mark-bullet">
<li>Persistent conversation with history
</li><li>Markdown to Org-mode conversion
</li><li>Model-specific colors
</li><li>System prompt support
</li><li>Parameter customization
</li><li>Reasoning/thinking section visibility control
</li><li>Context window management and monitoring
</li><li>Real-time context usage display
</li><li>Context size validation before sending prompts
</li><li>Customizable context thresholds and warnings
</li><li>File attachment support
</li></ul>

</div>
<div class="section-level-extent" id="Pre_002dbuilt-Commands">
<h3 class="section"><span>5.2 Pre-built Commands<a class="copiable-link" href="#Pre_002dbuilt-Commands"> &para;</a></span></h3>

<p>Ollama Buddy comes with several pre-built commands:
</p>
<dl class="table">
<dt>Code Refactoring</dt>
<dd><p>Improves code while maintaining functionality
</p>
</dd>
<dt>Code Description</dt>
<dd><p>Explains what code does and how it works
</p>
</dd>
<dt>Git Commit Messages</dt>
<dd><p>Generates meaningful commit messages from code changes
</p>
</dd>
<dt>Dictionary Lookups</dt>
<dd><p>Provides comprehensive word definitions
</p>
</dd>
<dt>Synonym Finder</dt>
<dd><p>Suggests alternative words with context
</p>
</dd>
<dt>Proofreading</dt>
<dd><p>Corrects grammar, style, and spelling
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Model-Management">
<h3 class="section"><span>5.3 Model Management<a class="copiable-link" href="#Model-Management"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Switch between any model available in Ollama
</li><li>Use ChatGPT and Claude models with API keys
</li><li>Pull new models directly from the interface
</li><li>View model information and statistics
</li><li>Delete models you no longer need
</li><li>Import GGUF files to create new models
</li></ul>

</div>
<div class="section-level-extent" id="Parameter-Control-1">
<h3 class="section"><span>5.4 Parameter Control<a class="copiable-link" href="#Parameter-Control-1"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Fine-tune model behavior with customizable parameters
</li><li>Save and use parameter profiles for different use cases
</li><li>Command-specific parameter settings
</li><li>Real-time parameter adjustment
</li></ul>

</div>
<div class="section-level-extent" id="Roles-and-Custom-Commands">
<h3 class="section"><span>5.5 Roles and Custom Commands<a class="copiable-link" href="#Roles-and-Custom-Commands"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Create custom command sets for specific workflows
</li><li>Design specialized AI assistants with custom system prompts
</li><li>Save and switch between different roles
</li><li>Share role configurations across your team
</li></ul>

</div>
<div class="section-level-extent" id="Prompt-Template-Collections">
<h3 class="section"><span>5.6 Prompt Template Collections<a class="copiable-link" href="#Prompt-Template-Collections"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Use pre-built prompt patterns from Fabric project
</li><li>Utilize the Awesome ChatGPT Prompts collection
</li><li>Apply specialized prompts to your content with one command
</li><li>Browse prompts by category
</li></ul>

</div>
<div class="section-level-extent" id="External-API-Integration">
<h3 class="section"><span>5.7 External API Integration<a class="copiable-link" href="#External-API-Integration"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Connect to OpenAI&rsquo;s ChatGPT API (prefix: <code class="code">a:</code>)
</li><li>Connect to Anthropic&rsquo;s Claude API (prefix: <code class="code">c:</code>)
</li><li>Connect to Google Gemini API (prefix: <code class="code">g:</code>)
</li><li>Connect to X Grok API (prefix: <code class="code">k:</code>)
</li><li>Connect to GitHub Copilot Chat API (prefix: <code class="code">p:</code>)
</li><li>Connect to Mistral Codestral API (prefix: <code class="code">s:</code>)
</li><li>Use Ollama cloud models (indicated by <code class="code">â</code>)
</li><li>Seamlessly switch between local and cloud models
</li><li>Secure API key management via auth-source
</li></ul>

</div>
<div class="section-level-extent" id="Vision-Support">
<h3 class="section"><span>5.8 Vision Support<a class="copiable-link" href="#Vision-Support"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Analyze images with vision-capable models
</li><li>Automatic image detection in prompts
</li><li>Support for PNG, JPG, JPEG, WebP, and GIF formats
</li><li>Configurable list of vision-capable models
</li></ul>

</div>
<div class="section-level-extent" id="File-Attachments-1">
<h3 class="section"><span>5.9 File Attachments<a class="copiable-link" href="#File-Attachments-1"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Attach text files, code, and documentation to conversations
</li><li>Automatic context inclusion with proper token counting
</li><li>Session persistence for attachments
</li><li>Support for various file types
</li><li>Dired integration for bulk file attachment
</li></ul>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Chat-Interface">
<div class="nav-panel">
<p>
Next: <a href="#Working-with-Models" accesskey="n" rel="next">Working with Models</a>, Previous: <a href="#Core-Features" accesskey="p" rel="prev">Core Features</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Chat-Interface-2"><span>6 Chat Interface<a class="copiable-link" href="#Chat-Interface-2"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Opening-the-Chat" accesskey="1">Opening the Chat</a></li>
<li><a href="#Interface-Overview" accesskey="2">Interface Overview</a></li>
<li><a href="#Sending-Prompts" accesskey="3">Sending Prompts</a></li>
<li><a href="#System-Prompts" accesskey="4">System Prompts</a></li>
<li><a href="#Markdown-to-Org-Conversion" accesskey="5">Markdown to Org Conversion</a></li>
<li><a href="#Reasoning-Visibility-Control" accesskey="6">Reasoning Visibility Control</a></li>
</ul>
<div class="section-level-extent" id="Opening-the-Chat">
<h3 class="section"><span>6.1 Opening the Chat<a class="copiable-link" href="#Opening-the-Chat"> &para;</a></span></h3>

<p>To open the chat interface:
</p>
<ol class="enumerate">
<li> Use <code class="code">M-x ollama-buddy-menu</code> or your configured keybinding
</li><li> Press <code class="code">o</code> to select &quot;Open Chat&quot;
</li><li> A new buffer will open with the Ollama Buddy chat interface
</li></ol>

</div>
<div class="section-level-extent" id="Interface-Overview">
<h3 class="section"><span>6.2 Interface Overview<a class="copiable-link" href="#Interface-Overview"> &para;</a></span></h3>

<p>The chat interface consists of:
</p>
<ul class="itemize mark-bullet">
<li>A welcome message with available models and providers
</li><li>Conversation history (previous prompts and responses)
</li><li>A prompt area for entering your queries
</li><li>A header line with status information including:
  <ul class="itemize mark-minus">
<li>Cloud model indicator (<code class="code">â</code>)
  </li><li>Loaded provider indicators (<code class="code">acgks</code>)
  </li><li>Context usage bar (when enabled)
  </li><li>History count (<code class="code">H5/10</code>)
  </li><li>Current model name
  </li><li>System prompt indicator
  </li><li>Modified parameters
  </li></ul>
</li><li>Context warnings and validation
</li><li>Attachment indicators when files are attached
</li></ul>

</div>
<div class="section-level-extent" id="Sending-Prompts">
<h3 class="section"><span>6.3 Sending Prompts<a class="copiable-link" href="#Sending-Prompts"> &para;</a></span></h3>

<p>To send a prompt to the AI:
</p>
<ol class="enumerate">
<li> Type your message in the prompt area (after &quot;&gt;&gt; PROMPT:&quot;)
</li><li> Press <code class="code">C-c C-c</code> to send
</li><li> Wait for the AI to generate a response
</li></ol>

<p>You can also:
</p><ul class="itemize mark-bullet">
<li>Use <code class="code">M-p</code> and <code class="code">M-n</code> to navigate through prompt history
</li><li>Press <code class="code">C-c k</code> to cancel a request if it&rsquo;s taking too long
</li></ul>

</div>
<div class="section-level-extent" id="System-Prompts">
<h3 class="section"><span>6.4 System Prompts<a class="copiable-link" href="#System-Prompts"> &para;</a></span></h3>

<p>System prompts allow you to define the AI&rsquo;s behavior:
</p>
<dl class="table">
<dt>Setting a system prompt</dt>
<dd><p>Type your system prompt, then press <code class="code">C-c s</code>
</p>
</dd>
<dt>Viewing the current system prompt</dt>
<dd><p>Press <code class="code">C-c C-s</code>
</p>
</dd>
<dt>Resetting the system prompt</dt>
<dd><p>Press <code class="code">C-c r</code>
</p>
</dd>
<dt>Using a pre-built prompt</dt>
<dd><p>Use Fabric patterns (<code class="code">C-c f p</code>) or Awesome ChatGPT prompts (<code class="code">C-c w p</code>)
</p></dd>
</dl>

<p>Example system prompt:
</p><div class="example">
<pre class="example-preformatted">You are a programming expert who specializes in Python. 
Provide concise, efficient solutions with explanations.
</pre></div>

</div>
<div class="section-level-extent" id="Markdown-to-Org-Conversion">
<h3 class="section"><span>6.5 Markdown to Org Conversion<a class="copiable-link" href="#Markdown-to-Org-Conversion"> &para;</a></span></h3>

<p>By default, Ollama Buddy converts markdown in responses to Org-mode syntax:
</p>
<ul class="itemize mark-bullet">
<li>Code blocks are converted to Org-mode source blocks
</li><li>Headers are converted to Org-mode headings
</li><li>Lists are properly formatted
</li><li>Links are converted to Org-mode format
</li></ul>

<p>To toggle this feature:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-markdown-conversion
</pre></div>
<p>or press <code class="code">C-c C-o</code> in the chat buffer.
</p>
</div>
<div class="section-level-extent" id="Reasoning-Visibility-Control">
<h3 class="section"><span>6.6 Reasoning Visibility Control<a class="copiable-link" href="#Reasoning-Visibility-Control"> &para;</a></span></h3>

<p>Ollama Buddy can hide reasoning/thinking sections in responses, making the output cleaner:
</p>
<ul class="itemize mark-bullet">
<li>Toggle visibility with <code class="code">C-c V</code> or <code class="code">M-x ollama-buddy-toggle-reasoning-visibility</code>
</li><li>Configure markers with the <code class="code">ollama-buddy-reasoning-markers</code> variable
</li><li>When hidden, a status message shows the current reasoning section (e.g., &quot;Think...&quot;)
</li><li>Header line indicates when reasoning is hidden with &quot;REASONING HIDDEN&quot; text
</li></ul>

<p>This feature helps focus on final answers while preserving the option to view the full reasoning process.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="Working-with-Models">
<div class="nav-panel">
<p>
Next: <a href="#Context-Management" accesskey="n" rel="next">Context Management</a>, Previous: <a href="#Chat-Interface" accesskey="p" rel="prev">Chat Interface</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Working-with-Models-1"><span>7 Working with Models<a class="copiable-link" href="#Working-with-Models-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Available-Models" accesskey="1">Available Models</a></li>
<li><a href="#Switching-Models" accesskey="2">Switching Models</a></li>
<li><a href="#Local-vs_002e-Cloud-Models" accesskey="3">Local vs. Cloud Models</a></li>
<li><a href="#Managing-Models" accesskey="4">Managing Models</a></li>
<li><a href="#Pulling-New-Models" accesskey="5">Pulling New Models</a></li>
<li><a href="#Importing-GGUF-Files" accesskey="6">Importing GGUF Files</a></li>
<li><a href="#Vision-Models" accesskey="7">Vision Models</a></li>
<li><a href="#Multishot-Mode" accesskey="8">Multishot Mode</a></li>
</ul>
<div class="section-level-extent" id="Available-Models">
<h3 class="section"><span>7.1 Available Models<a class="copiable-link" href="#Available-Models"> &para;</a></span></h3>

<p>Ollama Buddy displays available models in the chat interface. Each model is assigned a letter for quick selection.
</p>
<p>To view detailed model information:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-show-model-status
</pre></div>
<p>or press <code class="code">C-c v</code> in the chat buffer.
</p>
</div>
<div class="section-level-extent" id="Switching-Models">
<h3 class="section"><span>7.2 Switching Models<a class="copiable-link" href="#Switching-Models"> &para;</a></span></h3>

<p>To change the current model:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c m</code> in the chat buffer
</li><li> Select a model from the completion list
</li><li> The new model will be used for future requests
</li></ol>

<p>You can also switch models from the main menu with <code class="code">m</code>.
</p>
</div>
<div class="section-level-extent" id="Local-vs_002e-Cloud-Models">
<h3 class="section"><span>7.3 Local vs. Cloud Models<a class="copiable-link" href="#Local-vs_002e-Cloud-Models"> &para;</a></span></h3>

<p>Ollama Buddy supports both local Ollama models and cloud-based models:
</p>
<ul class="itemize mark-bullet">
<li>Local models (via Ollama): llama3, codellama, mistral, etc.
</li><li>OpenAI models: gpt-3.5-turbo, gpt-4, etc.
</li><li>Claude models: claude-3-opus, claude-3-sonnet, etc.
</li></ul>

<p>To use cloud models, you need to configure API keys as described in the Installation chapter.
</p>
</div>
<div class="section-level-extent" id="Managing-Models">
<h3 class="section"><span>7.4 Managing Models<a class="copiable-link" href="#Managing-Models"> &para;</a></span></h3>

<p>Ollama Buddy provides a comprehensive model management interface. To access it:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-manage-models
</pre></div>
<p>or press <code class="code">C-c W</code> in the chat buffer.
</p>
<p>From this interface, you can:
</p><ul class="itemize mark-bullet">
<li>See which models are currently running
</li><li>Pull new models from Ollama Hub
</li><li>Delete models you no longer need
</li><li>View detailed model information
</li><li>Select models for use
</li></ul>

</div>
<div class="section-level-extent" id="Pulling-New-Models">
<h3 class="section"><span>7.5 Pulling New Models<a class="copiable-link" href="#Pulling-New-Models"> &para;</a></span></h3>

<p>To pull a new model:
</p>
<ol class="enumerate">
<li> Open the model management interface with <code class="code">C-c W</code>
</li><li> Click &quot;[Pull Any Model]&quot; or press the appropriate key
</li><li> Enter the model name (e.g., &quot;phi:latest&quot;, &quot;codellama:7b&quot;)
</li><li> Wait for the model to download
</li></ol>

</div>
<div class="section-level-extent" id="Importing-GGUF-Files">
<h3 class="section"><span>7.6 Importing GGUF Files<a class="copiable-link" href="#Importing-GGUF-Files"> &para;</a></span></h3>

<p>You can import custom GGUF model files:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c W</code> to open the model management interface
</li><li> Click &quot;[Import GGUF File]&quot; or press the appropriate key
</li><li> Select the GGUF file from your file system
</li><li> Enter a name for the model
</li><li> Optionally provide model parameters
</li><li> Wait for Ollama to create the model
</li></ol>

</div>
<div class="section-level-extent" id="Vision-Models">
<h3 class="section"><span>7.7 Vision Models<a class="copiable-link" href="#Vision-Models"> &para;</a></span></h3>

<p>Some models support vision capabilities for image analysis.
</p>
<ul class="mini-toc">
<li><a href="#Configuring-Vision-Support" accesskey="1">Configuring Vision Support</a></li>
<li><a href="#Using-Vision-Models" accesskey="2">Using Vision Models</a></li>
</ul>
<div class="subsection-level-extent" id="Configuring-Vision-Support">
<h4 class="subsection"><span>7.7.1 Configuring Vision Support<a class="copiable-link" href="#Configuring-Vision-Support"> &para;</a></span></h4>

<dl class="table">
<dt><code class="code">ollama-buddy-vision-enabled</code></dt>
<dd><p>Whether to enable vision support (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-vision-enabled t)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-vision-models</code></dt>
<dd><p>List of models known to support vision:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-vision-models
      '(&quot;gemma3:4b&quot; &quot;llama3.2:3b&quot; &quot;llama3.2:8b&quot;))
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-image-formats</code></dt>
<dd><p>Supported image file formats:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-image-formats
      '(&quot;\\.png$&quot; &quot;\\.jpg$&quot; &quot;\\.jpeg$&quot; &quot;\\.webp$&quot; &quot;\\.gif$&quot;))
</pre></div>
</dd>
</dl>

</div>
<div class="subsection-level-extent" id="Using-Vision-Models">
<h4 class="subsection"><span>7.7.2 Using Vision Models<a class="copiable-link" href="#Using-Vision-Models"> &para;</a></span></h4>

<p>To analyze an image:
</p><ol class="enumerate">
<li> Select a vision-capable model
</li><li> Include an image path in your prompt, or use the &quot;Analyze image&quot; command
</li><li> The image will be automatically encoded and sent to the model
</li></ol>

<p>Image paths can be:
</p><ul class="itemize mark-bullet">
<li>Quoted paths: <code class="code">&quot;path/to/image.png&quot;</code>
</li><li>Unquoted paths: <code class="code">/home/user/image.jpg</code>
</li></ul>

</div>
</div>
<div class="section-level-extent" id="Multishot-Mode">
<h3 class="section"><span>7.8 Multishot Mode<a class="copiable-link" href="#Multishot-Mode"> &para;</a></span></h3>

<p>Multishot mode allows you to send the same prompt to multiple models simultaneously:
</p>
<ol class="enumerate">
<li> Type your prompt in the chat buffer
</li><li> Press <code class="code">C-c M</code>
</li><li> Enter the sequence of model letters you want to use (e.g., &quot;a,b,c&quot; to use models a, b, and c)
</li><li> Note that each item should be separated with a comma
</li><li> Watch as Ollama Buddy processes your request with each model in sequence
</li></ol>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Context-Management">
<div class="nav-panel">
<p>
Next: <a href="#File-Attachments" accesskey="n" rel="next">File Attachments</a>, Previous: <a href="#Working-with-Models" accesskey="p" rel="prev">Working with Models</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Context-Management-1"><span>8 Context Management<a class="copiable-link" href="#Context-Management-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Understanding-Context-Windows" accesskey="1">Understanding Context Windows</a></li>
<li><a href="#Context-Size-Detection" accesskey="2">Context Size Detection</a></li>
<li><a href="#Enabling-Context-Monitoring" accesskey="3">Enabling Context Monitoring</a></li>
<li><a href="#Context-with-File-Attachments" accesskey="4">Context with File Attachments</a></li>
<li><a href="#Context-Management-Commands" accesskey="5">Context Management Commands</a></li>
<li><a href="#Token-Estimation" accesskey="6">Token Estimation</a></li>
<li><a href="#Managing-Context-in-Practice" accesskey="7">Managing Context in Practice</a></li>
<li><a href="#Context-Display-Configuration" accesskey="8">Context Display Configuration</a></li>
<li><a href="#Fallback-Context-Sizes" accesskey="9">Fallback Context Sizes</a></li>
<li><a href="#Troubleshooting-Context-Issues">Troubleshooting Context Issues</a></li>
</ul>
<div class="section-level-extent" id="Understanding-Context-Windows">
<h3 class="section"><span>8.1 Understanding Context Windows<a class="copiable-link" href="#Understanding-Context-Windows"> &para;</a></span></h3>

<p>Context windows define how much text (measured in tokens) a model can process at once. This includes your current prompt, conversation history, any system prompts, and attached files. Understanding and managing context is crucial for:
</p>
<ul class="itemize mark-bullet">
<li>Preventing errors when context limits are exceeded
</li><li>Optimizing model performance for different tasks
</li><li>Managing longer conversations efficiently
</li><li>Including files without exceeding context limits
</li></ul>

</div>
<div class="section-level-extent" id="Context-Size-Detection">
<h3 class="section"><span>8.2 Context Size Detection<a class="copiable-link" href="#Context-Size-Detection"> &para;</a></span></h3>

<p>Ollama Buddy uses multiple methods to determine a model&rsquo;s context size:
</p>
<ol class="enumerate">
<li> Built-in mappings for popular models (llama3, mistral, codellama, etc.)
</li><li> Custom context sizes set via the <code class="code">num_ctx</code> parameter
</li><li> Manual configuration through interactive commands
</li><li> Fallback to reasonable defaults (4096 tokens) for unknown models
</li></ol>

</div>
<div class="section-level-extent" id="Enabling-Context-Monitoring">
<h3 class="section"><span>8.3 Enabling Context Monitoring<a class="copiable-link" href="#Enabling-Context-Monitoring"> &para;</a></span></h3>

<p>Context monitoring is disabled by default. To enable it:
</p>
<div class="example">
<pre class="example-preformatted">(setq ollama-buddy-show-context-percentage t)
</pre></div>

<p>With context monitoring enabled:
</p><ul class="itemize mark-bullet">
<li>The status bar shows current/max context usage (e.g., &quot;2048/8192&quot;)
</li><li>Text formatting indicates usage levels:
  <ul class="itemize mark-minus">
<li>Normal font: Under 85% usage
  </li><li>Bold and underlined: 85-100% usage
  </li><li>Inverted: At or exceeding 100% usage
  </li></ul>
</li><li>Warnings appear before sending prompts that exceed limits
</li></ul>

</div>
<div class="section-level-extent" id="Context-with-File-Attachments">
<h3 class="section"><span>8.4 Context with File Attachments<a class="copiable-link" href="#Context-with-File-Attachments"> &para;</a></span></h3>

<p>File attachments are included in context calculations:
</p>
<ul class="itemize mark-bullet">
<li>Each attached file contributes to the total token count
</li><li>The context breakdown shows attachment tokens separately
</li><li>File content is included in the request context
</li><li>Large files can significantly impact context usage
</li></ul>

</div>
<div class="section-level-extent" id="Context-Management-Commands">
<h3 class="section"><span>8.5 Context Management Commands<a class="copiable-link" href="#Context-Management-Commands"> &para;</a></span></h3>

<dl class="table">
<dt>Show Context Information (<kbd class="kbd">C-c C</kbd>)</dt>
<dd><p>Displays a breakdown of current context usage, including:
</p><ul class="itemize mark-bullet">
<li>Conversation history token count
</li><li>System prompt token count
</li><li>Attachment token count
</li><li>Current prompt token count
</li><li>Total usage percentage
</li></ul>

</dd>
<dt>Set Model Context Size (<kbd class="kbd">C-c $</kbd>)</dt>
<dd><p>Manually configure the context size for a specific model.
</p>
</dd>
<dt>Toggle Context Display (<kbd class="kbd">C-c %</kbd>)</dt>
<dd><p>Show or hide the context percentage in the status bar.
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Token-Estimation">
<h3 class="section"><span>8.6 Token Estimation<a class="copiable-link" href="#Token-Estimation"> &para;</a></span></h3>

<p>Ollama Buddy estimates token counts using a heuristic approach:
</p><ul class="itemize mark-bullet">
<li>Each word is multiplied by 1.3 (following common approximations)
</li><li>This provides a reasonable estimate for most use cases
</li><li>Actual token counts may vary slightly between models
</li></ul>

</div>
<div class="section-level-extent" id="Managing-Context-in-Practice">
<h3 class="section"><span>8.7 Managing Context in Practice<a class="copiable-link" href="#Managing-Context-in-Practice"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Workflow-Strategies" accesskey="1">Workflow Strategies</a></li>
<li><a href="#Using-num_005fctx-Parameter" accesskey="2">Using num_ctx Parameter</a></li>
</ul>
<div class="subsection-level-extent" id="Workflow-Strategies">
<h4 class="subsection"><span>8.7.1 Workflow Strategies<a class="copiable-link" href="#Workflow-Strategies"> &para;</a></span></h4>

<ul class="mini-toc">
<li><a href="#Paste_002dand_002dSend-Approach" accesskey="1">Paste-and-Send Approach</a></li>
<li><a href="#Preemptive-Checking" accesskey="2">Preemptive Checking</a></li>
<li><a href="#History-Length-Management" accesskey="3">History Length Management</a></li>
</ul>
<div class="subsubsection-level-extent" id="Paste_002dand_002dSend-Approach">
<h4 class="subsubsection"><span>8.7.1.1 Paste-and-Send Approach<a class="copiable-link" href="#Paste_002dand_002dSend-Approach"> &para;</a></span></h4>
<ol class="enumerate">
<li> Paste your content into the chat buffer
</li><li> Press the send keybinding
</li><li> If context is exceeded, you&rsquo;ll get a warning dialog
</li><li> Choose whether to proceed or modify your content
</li></ol>

</div>
<div class="subsubsection-level-extent" id="Preemptive-Checking">
<h4 class="subsubsection"><span>8.7.1.2 Preemptive Checking<a class="copiable-link" href="#Preemptive-Checking"> &para;</a></span></h4>
<ol class="enumerate">
<li> Paste your content
</li><li> Use <kbd class="kbd">C-c C</kbd> to check context usage
</li><li> If too high:
  <ul class="itemize mark-bullet">
<li>Trim your current prompt
  </li><li>Edit conversation history (<kbd class="kbd">C-c J</kbd>)
  </li><li>Switch to a larger context model
  </li><li>Adjust system prompt length
  </li><li>Remove or reduce file attachments
  </li></ul>
</li></ol>

</div>
<div class="subsubsection-level-extent" id="History-Length-Management">
<h4 class="subsubsection"><span>8.7.1.3 History Length Management<a class="copiable-link" href="#History-Length-Management"> &para;</a></span></h4>
<p>Control context by limiting conversation history:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-max-history-length 5)
</pre></div>

<p>This keeps only the last 5 message pairs, reducing context usage.
</p>
</div>
</div>
<div class="subsection-level-extent" id="Using-num_005fctx-Parameter">
<h4 class="subsection"><span>8.7.2 Using num_ctx Parameter<a class="copiable-link" href="#Using-num_005fctx-Parameter"> &para;</a></span></h4>

<p>The <code class="code">num_ctx</code> parameter allows you to set a specific context size:
</p><ol class="enumerate">
<li> Access the parameter menu with <kbd class="kbd">C-c P</kbd>
</li><li> Select <code class="code">num_ctx</code>
</li><li> Enter your desired context size
</li><li> Ollama Buddy will respect this limit
</li></ol>

</div>
</div>
<div class="section-level-extent" id="Context-Display-Configuration">
<h3 class="section"><span>8.8 Context Display Configuration<a class="copiable-link" href="#Context-Display-Configuration"> &para;</a></span></h3>

<p>Customize how context information is displayed:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-show-context-percentage</code></dt>
<dd><p>Whether to show context percentage in the status bar (default: nil).
</p>
</dd>
<dt><code class="code">ollama-buddy-context-warning-threshold</code></dt>
<dd><p>Percentage at which to warn about high context usage (default: 90).
</p>
</dd>
<dt><code class="code">ollama-buddy-context-error-threshold</code></dt>
<dd><p>Percentage at which to block sending (default: 100).
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Fallback-Context-Sizes">
<h3 class="section"><span>8.9 Fallback Context Sizes<a class="copiable-link" href="#Fallback-Context-Sizes"> &para;</a></span></h3>

<p>Ollama Buddy includes predefined context sizes for popular models. You can customize these via:
</p>
<div class="example">
<pre class="example-preformatted">(setq ollama-buddy-fallback-context-sizes
  '((&quot;llama3:8b&quot; . 4096)
    (&quot;codellama:7b&quot; . 8192)
    (&quot;mistral:7b&quot; . 8192)))
</pre></div>

</div>
<div class="section-level-extent" id="Troubleshooting-Context-Issues">
<h3 class="section"><span>8.10 Troubleshooting Context Issues<a class="copiable-link" href="#Troubleshooting-Context-Issues"> &para;</a></span></h3>

<dl class="table">
<dt>Context warnings appear unexpectedly</dt>
<dd><ul class="itemize mark-bullet">
<li>Check if you have a long system prompt
</li><li>Review conversation history length
</li><li>Verify the model&rsquo;s actual context size
</li><li>Check if files are attached and their sizes
</li></ul>

</dd>
<dt>Model responses are truncated</dt>
<dd><ul class="itemize mark-bullet">
<li>Increase the <code class="code">num_ctx</code> parameter
</li><li>Reduce history length with <kbd class="kbd">C-c Y</kbd>
</li><li>Clear some conversation history
</li><li>Remove large file attachments
</li></ul>

</dd>
<dt>Context calculations seem inaccurate</dt>
<dd><ul class="itemize mark-bullet">
<li>Remember that token estimation is approximate
</li><li>Different models may tokenize text differently
</li><li>Use <kbd class="kbd">C-c C</kbd> to see detailed breakdowns
</li></ul>
</dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="File-Attachments">
<div class="nav-panel">
<p>
Next: <a href="#Web-Search" accesskey="n" rel="next">Web Search</a>, Previous: <a href="#Context-Management" accesskey="p" rel="prev">Context Management</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="File-Attachments-2"><span>9 File Attachments<a class="copiable-link" href="#File-Attachments-2"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview" accesskey="1">Overview</a></li>
<li><a href="#Supported-File-Types" accesskey="2">Supported File Types</a></li>
<li><a href="#Attaching-Files" accesskey="3">Attaching Files</a></li>
<li><a href="#Managing-Attachments" accesskey="4">Managing Attachments</a></li>
<li><a href="#File-Size-Limits" accesskey="5">File Size Limits</a></li>
<li><a href="#How-Attachments-Work" accesskey="6">How Attachments Work</a></li>
<li><a href="#File-Attachment-Workflow-Examples" accesskey="7">File Attachment Workflow Examples</a></li>
<li><a href="#Context-Considerations" accesskey="8">Context Considerations</a></li>
<li><a href="#Best-Practices" accesskey="9">Best Practices</a></li>
<li><a href="#Troubleshooting-Attachments">Troubleshooting Attachments</a></li>
</ul>
<div class="section-level-extent" id="Overview">
<h3 class="section"><span>9.1 Overview<a class="copiable-link" href="#Overview"> &para;</a></span></h3>

<p>File attachments allow you to include the contents of text files, code files, documentation, and configuration files directly in your conversations with AI models. This feature is particularly useful for:
</p>
<ul class="itemize mark-bullet">
<li>Code review and analysis
</li><li>Documentation generation
</li><li>Configuration file troubleshooting
</li><li>Multi-file project discussions
</li><li>Research with multiple text sources
</li></ul>

</div>
<div class="section-level-extent" id="Supported-File-Types">
<h3 class="section"><span>9.2 Supported File Types<a class="copiable-link" href="#Supported-File-Types"> &para;</a></span></h3>

<p>Ollama Buddy supports a wide range of file types by default:
</p>
<dl class="table">
<dt>Text and Documentation</dt>
<dd><p><code class="code">.txt</code>, <code class="code">.md</code>, <code class="code">.org</code>
</p>
</dd>
<dt>Programming Languages</dt>
<dd><p><code class="code">.py</code>, <code class="code">.js</code>, <code class="code">.el</code>, <code class="code">.cpp</code>, <code class="code">.c</code>, <code class="code">.java</code>
</p>
</dd>
<dt>Web Technologies</dt>
<dd><p><code class="code">.html</code>, <code class="code">.css</code>, <code class="code">.json</code>, <code class="code">.xml</code>
</p>
</dd>
<dt>Configuration Files</dt>
<dd><p><code class="code">.yaml</code>, <code class="code">.yml</code>, <code class="code">.toml</code>, <code class="code">.ini</code>, <code class="code">.cfg</code>
</p>
</dd>
<dt>Scripts</dt>
<dd><p><code class="code">.sh</code>, <code class="code">.sql</code>
</p></dd>
</dl>

<p>You can customize supported file types by modifying <code class="code">ollama-buddy-supported-file-types</code>.
</p>
</div>
<div class="section-level-extent" id="Attaching-Files">
<h3 class="section"><span>9.3 Attaching Files<a class="copiable-link" href="#Attaching-Files"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Basic-File-Attachment" accesskey="1">Basic File Attachment</a></li>
<li><a href="#Dired-Integration" accesskey="2">Dired Integration</a></li>
</ul>
<div class="subsection-level-extent" id="Basic-File-Attachment">
<h4 class="subsection"><span>9.3.1 Basic File Attachment<a class="copiable-link" href="#Basic-File-Attachment"> &para;</a></span></h4>

<p>To attach a single file:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c A</code> to open the attachment menu
</li><li> Press <code class="code">a</code> for &quot;Attach file&quot;
</li><li> Select the file from the file browser
</li><li> The file will be attached and its contents included in future prompts
</li></ol>

<p>Alternatively, you can use <code class="code">C-c C-a</code> directly.
</p>
</div>
<div class="subsection-level-extent" id="Dired-Integration">
<h4 class="subsection"><span>9.3.2 Dired Integration<a class="copiable-link" href="#Dired-Integration"> &para;</a></span></h4>

<p>When working in Dired, you can attach files directly:
</p>
<dl class="table">
<dt>Attach file at point</dt>
<dd><p>Position the cursor on a file and press <code class="code">C-c C-a</code>
</p>
</dd>
<dt>Attach multiple marked files</dt>
<dd><p>Mark files with <code class="code">m</code>, then run <code class="code">M-x ollama-buddy-dired-attach-marked-files</code>
</p></dd>
</dl>

</div>
</div>
<div class="section-level-extent" id="Managing-Attachments">
<h3 class="section"><span>9.4 Managing Attachments<a class="copiable-link" href="#Managing-Attachments"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Viewing-Attachments" accesskey="1">Viewing Attachments</a></li>
<li><a href="#Detaching-Files" accesskey="2">Detaching Files</a></li>
<li><a href="#Clearing-All-Attachments" accesskey="3">Clearing All Attachments</a></li>
</ul>
<div class="subsection-level-extent" id="Viewing-Attachments">
<h4 class="subsection"><span>9.4.1 Viewing Attachments<a class="copiable-link" href="#Viewing-Attachments"> &para;</a></span></h4>

<p>To see currently attached files:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-show-attachments
</pre></div>
<p>or press <code class="code">C-c C-w</code>.
</p>
<p>This opens a dedicated buffer showing:
</p><ul class="itemize mark-bullet">
<li>File names and paths
</li><li>File sizes
</li><li>File content preview
</li></ul>

</div>
<div class="subsection-level-extent" id="Detaching-Files">
<h4 class="subsection"><span>9.4.2 Detaching Files<a class="copiable-link" href="#Detaching-Files"> &para;</a></span></h4>

<p>To remove a specific file:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-detach-file
</pre></div>
<p>or press <code class="code">C-c C-d</code>.
</p>
<p>You&rsquo;ll be prompted to select which file to detach from the list of currently attached files.
</p>
</div>
<div class="subsection-level-extent" id="Clearing-All-Attachments">
<h4 class="subsection"><span>9.4.3 Clearing All Attachments<a class="copiable-link" href="#Clearing-All-Attachments"> &para;</a></span></h4>

<p>To remove all attached files at once:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-clear-attachments
</pre></div>
<p>or press <code class="code">C-c 0</code>.
</p>
</div>
</div>
<div class="section-level-extent" id="File-Size-Limits">
<h3 class="section"><span>9.5 File Size Limits<a class="copiable-link" href="#File-Size-Limits"> &para;</a></span></h3>

<p>Ollama Buddy enforces file size limits to prevent overwhelming the context window:
</p>
<ul class="itemize mark-bullet">
<li>Default maximum file size: 10MB
</li><li>Configurable via <code class="code">ollama-buddy-max-file-size</code>
</li><li>Files exceeding the limit will trigger an error
</li></ul>

<p>Example configuration:
</p><div class="example">
<pre class="example-preformatted">;; Set maximum file size to 5MB
(setq ollama-buddy-max-file-size (* 5 1024 1024))
</pre></div>

</div>
<div class="section-level-extent" id="How-Attachments-Work">
<h3 class="section"><span>9.6 How Attachments Work<a class="copiable-link" href="#How-Attachments-Work"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Context-Integration" accesskey="1">Context Integration</a></li>
<li><a href="#Session-Persistence" accesskey="2">Session Persistence</a></li>
</ul>
<div class="subsection-level-extent" id="Context-Integration">
<h4 class="subsection"><span>9.6.1 Context Integration<a class="copiable-link" href="#Context-Integration"> &para;</a></span></h4>

<p>When files are attached:
</p>
<ol class="enumerate">
<li> File contents are read and stored in memory
</li><li> Content is included in the prompt context when sending requests
</li><li> Token counting includes attachment content
</li><li> Files are formatted with clear delimiters showing filename and type
</li></ol>

</div>
<div class="subsection-level-extent" id="Session-Persistence">
<h4 class="subsection"><span>9.6.2 Session Persistence<a class="copiable-link" href="#Session-Persistence"> &para;</a></span></h4>

<p>File attachments are preserved across sessions:
</p>
<ul class="itemize mark-bullet">
<li>Saving a session (<code class="code">C-c S</code>) includes all attached files
</li><li>Loading a session (<code class="code">C-c L</code>) restores attachments
</li><li>Session files store both file paths and content
</li><li>Attachment metadata is preserved (size, type, attachment time)
</li></ul>

</div>
</div>
<div class="section-level-extent" id="File-Attachment-Workflow-Examples">
<h3 class="section"><span>9.7 File Attachment Workflow Examples<a class="copiable-link" href="#File-Attachment-Workflow-Examples"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Code-Review-Workflow" accesskey="1">Code Review Workflow</a></li>
<li><a href="#Multi_002dFile-Analysis" accesskey="2">Multi-File Analysis</a></li>
<li><a href="#Configuration-Troubleshooting" accesskey="3">Configuration Troubleshooting</a></li>
</ul>
<div class="subsection-level-extent" id="Code-Review-Workflow">
<h4 class="subsection"><span>9.7.1 Code Review Workflow<a class="copiable-link" href="#Code-Review-Workflow"> &para;</a></span></h4>

<ol class="enumerate">
<li> Attach source files using <code class="code">C-c C-a</code>
</li><li> Set a system prompt for code review: &quot;You are an expert code reviewer&quot;
</li><li> Ask questions about the code: &quot;What potential issues do you see in this code?&quot;
</li><li> The AI can reference all attached files in its analysis
</li></ol>

</div>
<div class="subsection-level-extent" id="Multi_002dFile-Analysis">
<h4 class="subsection"><span>9.7.2 Multi-File Analysis<a class="copiable-link" href="#Multi_002dFile-Analysis"> &para;</a></span></h4>

<ol class="enumerate">
<li> Use Dired to mark multiple related files
</li><li> Attach them all with <code class="code">M-x ollama-buddy-dired-attach-marked-files</code>
</li><li> Ask for analysis: &quot;Compare the approaches used in these files&quot;
</li><li> The AI can cross-reference content between files
</li></ol>

</div>
<div class="subsection-level-extent" id="Configuration-Troubleshooting">
<h4 class="subsection"><span>9.7.3 Configuration Troubleshooting<a class="copiable-link" href="#Configuration-Troubleshooting"> &para;</a></span></h4>

<ol class="enumerate">
<li> Attach configuration files (.yaml, .json, .ini)
</li><li> Describe the issue: &quot;This configuration isn&rsquo;t working as expected&quot;
</li><li> The AI can analyze the config and suggest fixes
</li></ol>

</div>
</div>
<div class="section-level-extent" id="Context-Considerations">
<h3 class="section"><span>9.8 Context Considerations<a class="copiable-link" href="#Context-Considerations"> &para;</a></span></h3>

<p>File attachments impact context usage:
</p>
<ul class="itemize mark-bullet">
<li>Each attached file counts toward the total token limit
</li><li>Large files can quickly fill available context
</li><li>Monitor context usage with <code class="code">C-c C</code> when using attachments
</li><li>Consider detaching unnecessary files to free up context
</li></ul>

</div>
<div class="section-level-extent" id="Best-Practices">
<h3 class="section"><span>9.9 Best Practices<a class="copiable-link" href="#Best-Practices"> &para;</a></span></h3>

<ol class="enumerate">
<li> Start with smaller files to avoid context issues
</li><li> Use descriptive filenames for clarity
</li><li> Remove attachments when no longer needed
</li><li> Monitor context usage with large files
</li><li> Use attachment history to avoid re-attaching the same files
</li></ol>

</div>
<div class="section-level-extent" id="Troubleshooting-Attachments">
<h3 class="section"><span>9.10 Troubleshooting Attachments<a class="copiable-link" href="#Troubleshooting-Attachments"> &para;</a></span></h3>

<dl class="table">
<dt>File won&rsquo;t attach</dt>
<dd><ul class="itemize mark-bullet">
<li>Check if file type is supported (or override with &quot;y&quot;)
</li><li>Verify file size is under the limit
</li><li>Ensure file exists and is readable
</li></ul>

</dd>
<dt>Context errors with attachments</dt>
<dd><ul class="itemize mark-bullet">
<li>Remove some attachments with <code class="code">C-c C-d</code>
</li><li>Switch to a model with larger context
</li><li>Reduce conversation history length
</li></ul>

</dd>
<dt>Attachments not showing in session</dt>
<dd><ul class="itemize mark-bullet">
<li>Ensure you saved the session after attaching files
</li><li>Check that the session file includes attachment data
</li><li>Verify file paths are still valid when loading
</li></ul>
</dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Web-Search">
<div class="nav-panel">
<p>
Next: <a href="#Parameter-Control" accesskey="n" rel="next">Parameter Control</a>, Previous: <a href="#File-Attachments" accesskey="p" rel="prev">File Attachments</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Web-Search-1"><span>10 Web Search<a class="copiable-link" href="#Web-Search-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview-1" accesskey="1">Overview</a></li>
<li><a href="#How-It-Works" accesskey="2">How It Works</a></li>
<li><a href="#Configuration-2" accesskey="3">Configuration</a></li>
<li><a href="#Usage" accesskey="4">Usage</a></li>
<li><a href="#Viewing-Web-Search-Results" accesskey="5">Viewing Web Search Results</a></li>
<li><a href="#Managing-Web-Searches" accesskey="6">Managing Web Searches</a></li>
<li><a href="#Provider-Support" accesskey="7">Provider Support</a></li>
<li><a href="#Troubleshooting-1" accesskey="8">Troubleshooting</a></li>
</ul>
<div class="section-level-extent" id="Overview-1">
<h3 class="section"><span>10.1 Overview<a class="copiable-link" href="#Overview-1"> &para;</a></span></h3>

<p>Web search integration allows Ollama Buddy to fetch real-time information from the web and include it as context for your LLM conversations. This is particularly useful for:
</p>
<ul class="itemize mark-bullet">
<li>Getting current information beyond the model&rsquo;s training cutoff
</li><li>Researching topics and summarizing findings
</li><li>Fact-checking with up-to-date sources
</li><li>Comparing information from multiple web sources
</li></ul>

</div>
<div class="section-level-extent" id="How-It-Works">
<h3 class="section"><span>10.2 How It Works<a class="copiable-link" href="#How-It-Works"> &para;</a></span></h3>

<p>The web search feature implements a multi-stage pipeline. Content can be retrieved using two methods, controlled by <code class="code">ollama-buddy-web-search-content-source</code>:
</p>
<ul class="mini-toc">
<li><a href="#eww-Mode-_0028Default_002c-Recommended_0029" accesskey="1">eww Mode (Default, Recommended)</a></li>
<li><a href="#API-Mode-_0028Experimental_0029" accesskey="2">API Mode (Experimental)</a></li>
</ul>
<div class="subsection-level-extent" id="eww-Mode-_0028Default_002c-Recommended_0029">
<h4 class="subsection"><span>10.2.1 eww Mode (Default, Recommended)<a class="copiable-link" href="#eww-Mode-_0028Default_002c-Recommended_0029"> &para;</a></span></h4>

<p>When <code class="code">ollama-buddy-web-search-content-source</code> is set to <code class="code">'eww</code> (the default):
</p>
<ol class="enumerate">
<li> <strong class="strong">Query to Ollama API</strong>: Search queries are sent to Ollama&rsquo;s Web Search API (<code class="code">https://ollama.com/api/web_search</code>) via REST with Bearer token authentication.

</li><li> <strong class="strong">URL Extraction</strong>: The API returns search results containing URLs.

</li><li> <strong class="strong">eww/shr Processing</strong>: Each URL is fetched and processed through Emacs&rsquo; built-in eww/shr HTML renderer:
<ul class="itemize mark-minus">
<li>HTML is parsed using <code class="code">libxml-parse-html-region</code>
</li><li>Main content is extracted (looking for <code class="code">&lt;article&gt;</code>, <code class="code">&lt;main&gt;</code>, or content divs)
</li><li>Content is rendered to clean plain text using <code class="code">shr-insert-document</code>
</li><li>Org-mode special characters are escaped (<code class="code">*</code> and <code class="code">#</code> at line starts become <code class="code">,*</code> and <code class="code">,#</code>)
</li></ul>

</li><li> <strong class="strong">Context Attachment</strong>: The cleaned text is formatted with org headings and attached to the conversation context.

</li><li> <strong class="strong">LLM Submission</strong>: The search results are included when sending prompts to any configured LLM provider.
</li></ol>

<p>This mode produces cleaner, more complete content but requires additional HTTP requests to fetch each URL.
</p>
</div>
<div class="subsection-level-extent" id="API-Mode-_0028Experimental_0029">
<h4 class="subsection"><span>10.2.2 API Mode (Experimental)<a class="copiable-link" href="#API-Mode-_0028Experimental_0029"> &para;</a></span></h4>

<p>When <code class="code">ollama-buddy-web-search-content-source</code> is set to <code class="code">'api</code>:
</p>
<ol class="enumerate">
<li> <strong class="strong">Query to Ollama API</strong>: Search queries are sent to the Web Search API.

</li><li> <strong class="strong">Direct Content</strong>: Content snippets returned directly by the Ollama API are used without fetching individual URLs.

</li><li> <strong class="strong">Context Attachment</strong>: The API-provided content is formatted and attached to the conversation context.
</li></ol>

<p>This mode is faster (no additional HTTP requests) but the content quality depends on what the Ollama API returns, which may be less refined than eww-rendered pages. Use this mode when speed is more important than content completeness.
</p>
</div>
</div>
<div class="section-level-extent" id="Configuration-2">
<h3 class="section"><span>10.3 Configuration<a class="copiable-link" href="#Configuration-2"> &para;</a></span></h3>

<p>To use web search, you need an API key from Ollama:
</p>
<ol class="enumerate">
<li> Visit <a class="url" href="https://ollama.com/settings/keys">https://ollama.com/settings/keys</a>
</li><li> Create a new API key
</li><li> Configure in Emacs:
</li></ol>

<div class="example lisp">
<pre class="lisp-preformatted">;; Required: Set your API key
(setq ollama-buddy-web-search-api-key &quot;your-api-key-here&quot;)

;; Optional customizations
(setq ollama-buddy-web-search-max-results 5)        ; Number of URLs to fetch (default: 5)
(setq ollama-buddy-web-search-snippet-length 2000)  ; Max chars per result (default: 500)
(setq ollama-buddy-web-search-include-urls nil)     ; Include URLs in context (default: nil)
(setq ollama-buddy-web-search-content-source 'eww)  ; Content source: 'eww (default) or 'api

;; Load the module
(require 'ollama-buddy-web-search nil t)
</pre></div>

</div>
<div class="section-level-extent" id="Usage">
<h3 class="section"><span>10.4 Usage<a class="copiable-link" href="#Usage"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Inline-Search-Syntax" accesskey="1">Inline Search Syntax</a></li>
<li><a href="#Manual-Search-Commands" accesskey="2">Manual Search Commands</a></li>
<li><a href="#Transient-Menu" accesskey="3">Transient Menu</a></li>
</ul>
<div class="subsection-level-extent" id="Inline-Search-Syntax">
<h4 class="subsection"><span>10.4.1 Inline Search Syntax<a class="copiable-link" href="#Inline-Search-Syntax"> &para;</a></span></h4>

<p>The most convenient way to use web search is with inline syntax directly in your prompts:
</p>
<div class="example">
<pre class="example-preformatted">Tell me about @search(latest emacs 30 features) and summarize the key points.
</pre></div>

<p>Multiple searches can be combined:
</p>
<div class="example">
<pre class="example-preformatted">Compare @search(rust async programming) with @search(go concurrency model)
</pre></div>

<p>When you send the prompt, Ollama Buddy will:
</p><ol class="enumerate">
<li> Extract all <code class="code">@search(query)</code> patterns
</li><li> Perform each search and fetch URL contents
</li><li> Attach results to the conversation context
</li><li> Replace the search syntax with just the query text
</li><li> Send the prompt with the attached web context
</li></ol>

</div>
<div class="subsection-level-extent" id="Manual-Search-Commands">
<h4 class="subsection"><span>10.4.2 Manual Search Commands<a class="copiable-link" href="#Manual-Search-Commands"> &para;</a></span></h4>

<dl class="table">
<dt><kbd class="kbd">C-c / s</kbd></dt>
<dd><p>Search and display results in a buffer (<code class="code">ollama-buddy-web-search</code>)
</p>
</dd>
<dt><kbd class="kbd">C-c / a</kbd></dt>
<dd><p>Search and attach results to conversation context (<code class="code">ollama-buddy-web-search-attach</code>)
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="Transient-Menu">
<h4 class="subsection"><span>10.4.3 Transient Menu<a class="copiable-link" href="#Transient-Menu"> &para;</a></span></h4>

<p>Access the web search menu via <code class="code">C-c O /</code> which provides:
</p>
<ul class="itemize mark-bullet">
<li>Search &amp; Display - perform search and show results
</li><li>Search &amp; Attach - perform search and attach to context
</li><li>Show Attachments - view all attachments including web searches
</li><li>Clear All - remove all attachments and web searches
</li></ul>

</div>
</div>
<div class="section-level-extent" id="Viewing-Web-Search-Results">
<h3 class="section"><span>10.5 Viewing Web Search Results<a class="copiable-link" href="#Viewing-Web-Search-Results"> &para;</a></span></h3>

<p>Web search results are integrated into the attachment system. Use <code class="code">C-c C-w</code> to view all attachments, which displays both file attachments and web searches in an org-mode buffer:
</p>
<div class="example">
<pre class="example-preformatted">* Web Searches (1)

** what is the latest manchester united result
:PROPERTIES:
:RESULTS: 5
:TOKENS: ~1234
:SIZE: 5678 bytes
:TIME: 2024-01-15 10:30:00
:END:

*** 1. First Result Title
:PROPERTIES:
:URL: https://example.com/page1
:END:
#+begin_example
Full content from this URL...
#+end_example

*** 2. Second Result Title
...
</pre></div>

<p>The org-mode structure allows you to collapse/expand individual results.
</p>
</div>
<div class="section-level-extent" id="Managing-Web-Searches">
<h3 class="section"><span>10.6 Managing Web Searches<a class="copiable-link" href="#Managing-Web-Searches"> &para;</a></span></h3>

<p>Web searches are treated as attachments:
</p>
<ul class="itemize mark-bullet">
<li>Status line shows <code class="code">ðN</code> when N web searches are attached
</li><li><code class="code">C-c 0</code> clears all attachments including web searches
</li><li>Web search context is automatically included in prompts to all LLM providers
</li></ul>

</div>
<div class="section-level-extent" id="Provider-Support">
<h3 class="section"><span>10.7 Provider Support<a class="copiable-link" href="#Provider-Support"> &para;</a></span></h3>

<p>Web search works with all configured LLM providers:
</p>
<dl class="table">
<dt>Local Ollama</dt>
<dd><p>Models with <code class="code">o:</code> prefix or no prefix
</p>
</dd>
<dt>OpenAI</dt>
<dd><p>Models with <code class="code">a:</code> prefix
</p>
</dd>
<dt>Anthropic Claude</dt>
<dd><p>Models with <code class="code">c:</code> prefix
</p>
</dd>
<dt>Google Gemini</dt>
<dd><p>Models with <code class="code">g:</code> prefix
</p>
</dd>
<dt>X Grok</dt>
<dd><p>Models with <code class="code">k:</code> prefix
</p>
</dd>
<dt>GitHub Copilot</dt>
<dd><p>Models with <code class="code">p:</code> prefix
</p>
</dd>
<dt>Mistral Codestral</dt>
<dd><p>Models with <code class="code">s:</code> prefix
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Troubleshooting-1">
<h3 class="section"><span>10.8 Troubleshooting<a class="copiable-link" href="#Troubleshooting-1"> &para;</a></span></h3>

<dl class="table">
<dt>401 Unauthorized error</dt>
<dd><p>Ensure you&rsquo;re using an API key from <a class="url" href="https://ollama.com/settings/keys">https://ollama.com/settings/keys</a>, not the token from <code class="code">ollama signin</code> (cloud authentication).
</p>
</dd>
<dt>Empty search results</dt>
<dd><p>Some websites may block automated requests or have content that doesn&rsquo;t extract well. Try different search queries.
</p>
</dd>
<dt>Slow searches</dt>
<dd><p>URL fetching is asynchronous for display and attach commands. Inline searches in prompts use synchronous fetching to ensure results are ready before sending.
</p></dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Parameter-Control">
<div class="nav-panel">
<p>
Next: <a href="#Session-Management" accesskey="n" rel="next">Session Management</a>, Previous: <a href="#Web-Search" accesskey="p" rel="prev">Web Search</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Parameter-Control-2"><span>11 Parameter Control<a class="copiable-link" href="#Parameter-Control-2"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Understanding-Parameters" accesskey="1">Understanding Parameters</a></li>
<li><a href="#Viewing-Current-Parameters" accesskey="2">Viewing Current Parameters</a></li>
<li><a href="#Editing-Parameters" accesskey="3">Editing Parameters</a></li>
<li><a href="#Parameter-Profiles" accesskey="4">Parameter Profiles</a></li>
<li><a href="#Command_002dSpecific-Parameters" accesskey="5">Command-Specific Parameters</a></li>
<li><a href="#Reset-Parameters" accesskey="6">Reset Parameters</a></li>
<li><a href="#Displaying-Parameters-in-Header" accesskey="7">Displaying Parameters in Header</a></li>
</ul>
<div class="section-level-extent" id="Understanding-Parameters">
<h3 class="section"><span>11.1 Understanding Parameters<a class="copiable-link" href="#Understanding-Parameters"> &para;</a></span></h3>

<p>Ollama&rsquo;s models support various parameters that control their behavior:
</p>
<dl class="table">
<dt>temperature</dt>
<dd><p>Controls randomness (0.0-1.0+), higher values produce more creative outputs
</p>
</dd>
<dt>top_k</dt>
<dd><p>Limits token selection to top K most probable tokens
</p>
</dd>
<dt>top_p</dt>
<dd><p>Nucleus sampling threshold (0.0-1.0)
</p>
</dd>
<dt>repeat_penalty</dt>
<dd><p>Penalty for repeating tokens (higher values reduce repetition)
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Viewing-Current-Parameters">
<h3 class="section"><span>11.2 Viewing Current Parameters<a class="copiable-link" href="#Viewing-Current-Parameters"> &para;</a></span></h3>

<p>To view all current parameters:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-params-display
</pre></div>
<p>or press <code class="code">C-c G</code> in the chat buffer.
</p>
<p>Parameters that have been modified from default values are marked with an asterisk (*).
</p>
</div>
<div class="section-level-extent" id="Editing-Parameters">
<h3 class="section"><span>11.3 Editing Parameters<a class="copiable-link" href="#Editing-Parameters"> &para;</a></span></h3>

<p>To edit parameters:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c P</code> to open the parameter menu
</li><li> Select the parameter you want to modify
</li><li> Enter the new value
</li></ol>

<p>You can also use <code class="code">M-x ollama-buddy-params-edit</code> and select from a completion list.
</p>
</div>
<div class="section-level-extent" id="Parameter-Profiles">
<h3 class="section"><span>11.4 Parameter Profiles<a class="copiable-link" href="#Parameter-Profiles"> &para;</a></span></h3>

<p>Ollama Buddy comes with predefined parameter profiles for different use cases:
</p>
<dl class="table">
<dt>Default</dt>
<dd><p>Standard balanced settings
</p>
</dd>
<dt>Creative</dt>
<dd><p>Higher temperature, lower penalties for more creative responses
</p>
</dd>
<dt>Precise</dt>
<dd><p>Lower temperature, higher penalties for more deterministic responses
</p></dd>
</dl>

<p>To apply a profile:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-transient-profile-menu
</pre></div>
<p>or press <code class="code">C-c p</code> and select a profile.
</p>
</div>
<div class="section-level-extent" id="Command_002dSpecific-Parameters">
<h3 class="section"><span>11.5 Command-Specific Parameters<a class="copiable-link" href="#Command_002dSpecific-Parameters"> &para;</a></span></h3>

<p>Some commands have pre-configured parameters. For example:
</p><ul class="itemize mark-bullet">
<li>The &quot;Refactor Code&quot; command uses lower temperature for more deterministic results
</li><li>The &quot;Creative Writing&quot; command uses higher temperature for more varied outputs
</li></ul>

<p>These parameters are automatically applied when you use these commands and restored afterward.
</p>
</div>
<div class="section-level-extent" id="Reset-Parameters">
<h3 class="section"><span>11.6 Reset Parameters<a class="copiable-link" href="#Reset-Parameters"> &para;</a></span></h3>

<p>To reset all parameters to default values:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-params-reset
</pre></div>
<p>or press <code class="code">C-c K</code> in the chat buffer.
</p>
</div>
<div class="section-level-extent" id="Displaying-Parameters-in-Header">
<h3 class="section"><span>11.7 Displaying Parameters in Header<a class="copiable-link" href="#Displaying-Parameters-in-Header"> &para;</a></span></h3>

<p>To toggle whether modified parameters are shown in the header:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-params-in-header
</pre></div>
<p>or press <code class="code">C-c F</code> in the chat buffer.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="Session-Management">
<div class="nav-panel">
<p>
Next: <a href="#User-System-Prompts" accesskey="n" rel="next">User System Prompts</a>, Previous: <a href="#Parameter-Control" accesskey="p" rel="prev">Parameter Control</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Session-Management-1"><span>12 Session Management<a class="copiable-link" href="#Session-Management-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Understanding-Sessions" accesskey="1">Understanding Sessions</a></li>
<li><a href="#Creating-a-New-Session" accesskey="2">Creating a New Session</a></li>
<li><a href="#Saving-a-Session" accesskey="3">Saving a Session</a></li>
<li><a href="#Loading-a-Session" accesskey="4">Loading a Session</a></li>
<li><a href="#Managing-Sessions" accesskey="5">Managing Sessions</a></li>
<li><a href="#Conversation-History" accesskey="6">Conversation History</a></li>
</ul>
<div class="section-level-extent" id="Understanding-Sessions">
<h3 class="section"><span>12.1 Understanding Sessions<a class="copiable-link" href="#Understanding-Sessions"> &para;</a></span></h3>

<p>Sessions in Ollama Buddy allow you to:
</p><ul class="itemize mark-bullet">
<li>Save the entire conversation history
</li><li>Save the current model selection
</li><li>Restore previous conversations later
</li><li>Switch between different conversation contexts
</li></ul>

</div>
<div class="section-level-extent" id="Creating-a-New-Session">
<h3 class="section"><span>12.2 Creating a New Session<a class="copiable-link" href="#Creating-a-New-Session"> &para;</a></span></h3>

<p>To start a fresh session:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-sessions-new
</pre></div>
<p>or press <code class="code">C-c N</code> in the chat buffer.
</p>
<p>This will clear the current conversation history and let you start fresh.
</p>
</div>
<div class="section-level-extent" id="Saving-a-Session">
<h3 class="section"><span>12.3 Saving a Session<a class="copiable-link" href="#Saving-a-Session"> &para;</a></span></h3>

<p>To save the current session:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-sessions-save
</pre></div>
<p>or press <code class="code">C-c S</code> in the chat buffer.
</p>
<p>You&rsquo;ll be prompted to enter a name for the session.
</p>
</div>
<div class="section-level-extent" id="Loading-a-Session">
<h3 class="section"><span>12.4 Loading a Session<a class="copiable-link" href="#Loading-a-Session"> &para;</a></span></h3>

<p>To load a previously saved session:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-sessions-load
</pre></div>
<p>or press <code class="code">C-c L</code> in the chat buffer.
</p>
<p>You&rsquo;ll be presented with a list of saved sessions to choose from.
</p>
</div>
<div class="section-level-extent" id="Managing-Sessions">
<h3 class="section"><span>12.5 Managing Sessions<a class="copiable-link" href="#Managing-Sessions"> &para;</a></span></h3>

<p>To see a list of all saved sessions:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-sessions-list
</pre></div>
<p>or press <code class="code">C-c Q</code> in the chat buffer.
</p>
<p>From this view, you can see:
</p><ul class="itemize mark-bullet">
<li>Session names
</li><li>Last modified times
</li><li>Which models are used in each session
</li></ul>

<p>To delete a session:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-sessions-delete
</pre></div>
<p>or press <code class="code">C-c Z</code> in the chat buffer.
</p>
</div>
<div class="section-level-extent" id="Conversation-History">
<h3 class="section"><span>12.6 Conversation History<a class="copiable-link" href="#Conversation-History"> &para;</a></span></h3>

<p>Sessions save the conversation history for each model separately.
</p>
<p>To view the current conversation history:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-history-edit
</pre></div>
<p>or press <code class="code">C-c J</code> in the chat buffer.
</p>
<p>To clear the history:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-clear-history
</pre></div>
<p>or press <code class="code">C-c X</code> in the chat buffer.
</p>
<p>To toggle whether history is used in requests:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-history
</pre></div>
<p>or press <code class="code">C-c H</code> in the chat buffer.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="User-System-Prompts">
<div class="nav-panel">
<p>
Next: <a href="#Roles-and-Commands" accesskey="n" rel="next">Roles and Commands</a>, Previous: <a href="#Session-Management" accesskey="p" rel="prev">Session Management</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="User-System-Prompts-1"><span>13 User System Prompts<a class="copiable-link" href="#User-System-Prompts-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview-2" accesskey="1">Overview</a></li>
<li><a href="#Accessing-the-System-Prompts-Menu" accesskey="2">Accessing the System Prompts Menu</a></li>
<li><a href="#Saving-System-Prompts" accesskey="3">Saving System Prompts</a></li>
<li><a href="#Loading-Saved-Prompts" accesskey="4">Loading Saved Prompts</a></li>
<li><a href="#Managing-Your-Prompt-Library" accesskey="5">Managing Your Prompt Library</a></li>
<li><a href="#Categories-and-Organization" accesskey="6">Categories and Organization</a></li>
<li><a href="#Prompt-Storage-Format" accesskey="7">Prompt Storage Format</a></li>
<li><a href="#Best-Practices-for-System-Prompts" accesskey="8">Best Practices for System Prompts</a></li>
<li><a href="#Example-System-Prompts" accesskey="9">Example System Prompts</a></li>
<li><a href="#Workflow-Examples">Workflow Examples</a></li>
<li><a href="#Integration-with-Roles">Integration with Roles</a></li>
</ul>
<div class="section-level-extent" id="Overview-2">
<h3 class="section"><span>13.1 Overview<a class="copiable-link" href="#Overview-2"> &para;</a></span></h3>

<p>The User System Prompts feature allows you to save, organize, and reuse effective system prompts for your conversations with AI models. This feature is particularly valuable for:
</p>
<ul class="itemize mark-bullet">
<li>Building a personal library of effective prompts
</li><li>Maintaining context continuity across sessions
</li><li>Sharing prompt templates with teammates
</li><li>Refining your prompts over time
</li><li>Categorizing prompts by domain or purpose
</li></ul>

<p>System prompts play a crucial role in guiding AI behavior and response quality. A well-crafted system prompt can dramatically improve the relevance, accuracy, and style of AI responses.
</p>
</div>
<div class="section-level-extent" id="Accessing-the-System-Prompts-Menu">
<h3 class="section"><span>13.2 Accessing the System Prompts Menu<a class="copiable-link" href="#Accessing-the-System-Prompts-Menu"> &para;</a></span></h3>

<p>To access the system prompts menu:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-transient-user-prompts-menu
</pre></div>
<p>or press <code class="code">C-c s</code> in the chat buffer.
</p>
<p>This opens a transient menu with the following options:
</p>
<dl class="table">
<dt>Save current (S)</dt>
<dd><p>Save your active system prompt for future reuse
</p>
</dd>
<dt>Load prompt (L)</dt>
<dd><p>Select a previously saved prompt to apply
</p>
</dd>
<dt>Create new (N)</dt>
<dd><p>Start fresh with a new prompt
</p>
</dd>
<dt>List all Prompts (l)</dt>
<dd><p>View your entire prompt collection
</p>
</dd>
<dt>Edit prompt (e)</dt>
<dd><p>Modify an existing prompt
</p>
</dd>
<dt>Set with current prompt (s)</dt>
<dd><p>Set the current text as a system prompt
</p>
</dd>
<dt>Delete prompt (d)</dt>
<dd><p>Remove prompts you no longer need
</p>
</dd>
<dt>Reset prompt (r)</dt>
<dd><p>Clear the system prompt setting
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Saving-System-Prompts">
<h3 class="section"><span>13.3 Saving System Prompts<a class="copiable-link" href="#Saving-System-Prompts"> &para;</a></span></h3>

<p>To save a system prompt:
</p>
<ol class="enumerate">
<li> Set a system prompt by typing it and pressing <code class="code">C-c s s</code>
</li><li> Open the system prompts menu with <code class="code">C-c s</code>
</li><li> Press <code class="code">S</code> to save the current system prompt
</li><li> Enter a category (from predefined options or create your own)
</li><li> Enter a descriptive title for your prompt
</li><li> The prompt will be saved to your prompts directory
</li></ol>

</div>
<div class="section-level-extent" id="Loading-Saved-Prompts">
<h3 class="section"><span>13.4 Loading Saved Prompts<a class="copiable-link" href="#Loading-Saved-Prompts"> &para;</a></span></h3>

<p>To load a previously saved prompt:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c s</code> to open the system prompts menu
</li><li> Press <code class="code">L</code> to list available prompts
</li><li> Select a prompt from the completion interface
</li><li> The prompt will be loaded and set as your current system prompt
</li></ol>

<p>Prompts are displayed in the format &quot;<code class="code">category: title</code>&quot; for easy selection.
</p>
</div>
<div class="section-level-extent" id="Managing-Your-Prompt-Library">
<h3 class="section"><span>13.5 Managing Your Prompt Library<a class="copiable-link" href="#Managing-Your-Prompt-Library"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Viewing-All-Prompts" accesskey="1">Viewing All Prompts</a></li>
<li><a href="#Editing-Prompts" accesskey="2">Editing Prompts</a></li>
<li><a href="#Creating-New-Prompts" accesskey="3">Creating New Prompts</a></li>
<li><a href="#Deleting-Prompts" accesskey="4">Deleting Prompts</a></li>
</ul>
<div class="subsection-level-extent" id="Viewing-All-Prompts">
<h4 class="subsection"><span>13.5.1 Viewing All Prompts<a class="copiable-link" href="#Viewing-All-Prompts"> &para;</a></span></h4>

<p>To view your entire prompt collection:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-user-prompts-list
</pre></div>
<p>or press <code class="code">C-c s l</code>.
</p>
<p>This displays a buffer showing:
</p><ul class="itemize mark-bullet">
<li>Prompts organized by category
</li><li>Prompt titles
</li><li>Preview of prompt content
</li></ul>

</div>
<div class="subsection-level-extent" id="Editing-Prompts">
<h4 class="subsection"><span>13.5.2 Editing Prompts<a class="copiable-link" href="#Editing-Prompts"> &para;</a></span></h4>

<p>To edit an existing prompt:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-user-prompts-edit
</pre></div>
<p>or press <code class="code">C-c s e</code>.
</p>
<p>This opens the prompt file in an Org mode buffer where you can make changes and save.
</p>
</div>
<div class="subsection-level-extent" id="Creating-New-Prompts">
<h4 class="subsection"><span>13.5.3 Creating New Prompts<a class="copiable-link" href="#Creating-New-Prompts"> &para;</a></span></h4>

<p>To create a new prompt from scratch:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-user-prompts-create-new
</pre></div>
<p>or press <code class="code">C-c s N</code>.
</p>
<p>This opens a template with Org headers where you can enter your prompt content.
</p>
</div>
<div class="subsection-level-extent" id="Deleting-Prompts">
<h4 class="subsection"><span>13.5.4 Deleting Prompts<a class="copiable-link" href="#Deleting-Prompts"> &para;</a></span></h4>

<p>To delete a prompt:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-user-prompts-delete
</pre></div>
<p>or press <code class="code">C-c s d</code>.
</p>
<p>You&rsquo;ll be asked to confirm before the prompt is deleted.
</p>
</div>
</div>
<div class="section-level-extent" id="Categories-and-Organization">
<h3 class="section"><span>13.6 Categories and Organization<a class="copiable-link" href="#Categories-and-Organization"> &para;</a></span></h3>

<p>Prompts are organized into categories for easier management. Default categories include:
</p>
<ul class="itemize mark-bullet">
<li>general - General-purpose system prompts
</li><li>coding - Programming-specific prompts
</li><li>writing - Content creation and editing prompts
</li><li>analysis - Data and research analysis prompts
</li><li>creative - Prompts for creative tasks
</li><li>technical - Technical documentation and explanation prompts
</li><li>documentation - Documentation-focused prompts
</li></ul>

<p>You can customize the default categories:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-user-prompts-default-categories
      '(&quot;general&quot; &quot;coding&quot; &quot;writing&quot; &quot;analysis&quot; &quot;creative&quot; &quot;custom&quot;))
</pre></div>

</div>
<div class="section-level-extent" id="Prompt-Storage-Format">
<h3 class="section"><span>13.7 Prompt Storage Format<a class="copiable-link" href="#Prompt-Storage-Format"> &para;</a></span></h3>

<p>System prompts are stored as Org mode files with a specific naming convention:
</p>
<div class="example">
<pre class="example-preformatted">category__title__system.org
</pre></div>

<p>Each file contains:
</p><ul class="itemize mark-bullet">
<li>Org properties with metadata (title, category, date)
</li><li>The full prompt content
</li></ul>

<p>Example prompt file content:
</p><div class="example">
<pre class="example-preformatted">#+TITLE: Python Expert
#+CATEGORY: coding
#+DATE: 2025-05-19 14:32:45

You are a Python programming expert with deep knowledge of both modern and 
legacy Python code. When analyzing or writing code:

1. Prioritize readability and maintainability over clever tricks
2. Follow PEP 8 conventions
3. Include docstrings and comments for non-obvious operations
4. Explain your thinking step-by-step
5. Provide examples when helpful

When asked to debug, first identify the likely cause before suggesting fixes.
</pre></div>

</div>
<div class="section-level-extent" id="Best-Practices-for-System-Prompts">
<h3 class="section"><span>13.8 Best Practices for System Prompts<a class="copiable-link" href="#Best-Practices-for-System-Prompts"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Components-of-Effective-Prompts" accesskey="1">Components of Effective Prompts</a></li>
<li><a href="#Example-Patterns" accesskey="2">Example Patterns</a></li>
</ul>
<div class="subsection-level-extent" id="Components-of-Effective-Prompts">
<h4 class="subsection"><span>13.8.1 Components of Effective Prompts<a class="copiable-link" href="#Components-of-Effective-Prompts"> &para;</a></span></h4>

<p>Well-designed system prompts typically include:
</p>
<ul class="itemize mark-bullet">
<li>Clear role definition (who/what the AI is supposed to be)
</li><li>Guidelines for response style and format
</li><li>Constraints or limitations to observe
</li><li>Specific instructions for handling certain types of queries
</li><li>Examples of desired responses (optional)
</li></ul>

</div>
<div class="subsection-level-extent" id="Example-Patterns">
<h4 class="subsection"><span>13.8.2 Example Patterns<a class="copiable-link" href="#Example-Patterns"> &para;</a></span></h4>

<dl class="table">
<dt>Expert Role</dt>
<dd><p>&quot;You are a [domain] expert with [X years] of experience in [specific areas]...&quot;
</p>
</dd>
<dt>Response Format</dt>
<dd><p>&quot;Format your responses with a brief summary first, followed by detailed analysis...&quot;
</p>
</dd>
<dt>Specific Guidelines</dt>
<dd><p>&quot;When responding to code queries, always include sample code and explain line-by-line...&quot;
</p>
</dd>
<dt>Thinking Process</dt>
<dd><p>&quot;Think step-by-step, breaking down complex problems into smaller components...&quot;
</p></dd>
</dl>

</div>
</div>
<div class="section-level-extent" id="Example-System-Prompts">
<h3 class="section"><span>13.9 Example System Prompts<a class="copiable-link" href="#Example-System-Prompts"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Technical-Writing-Assistant" accesskey="1">Technical Writing Assistant</a></li>
<li><a href="#Code-Reviewer" accesskey="2">Code Reviewer</a></li>
</ul>
<div class="subsection-level-extent" id="Technical-Writing-Assistant">
<h4 class="subsection"><span>13.9.1 Technical Writing Assistant<a class="copiable-link" href="#Technical-Writing-Assistant"> &para;</a></span></h4>

<div class="example">
<pre class="example-preformatted">You are a technical writing expert who specializes in creating clear, concise, 
and accessible documentation. Your writing should:

1. Use plain language and avoid jargon where possible
2. Include appropriate headings and structural elements
3. Provide concrete examples that illustrate complex concepts
4. Use active voice and direct instructions for procedures
5. Anticipate common user questions and address them proactively

When presented with technical content, focus on making it understandable to 
the target audience while preserving technical accuracy.
</pre></div>

</div>
<div class="subsection-level-extent" id="Code-Reviewer">
<h4 class="subsection"><span>13.9.2 Code Reviewer<a class="copiable-link" href="#Code-Reviewer"> &para;</a></span></h4>

<div class="example">
<pre class="example-preformatted">You are an experienced code reviewer with expertise in software engineering 
best practices. When reviewing code:

1. Identify potential bugs, edge cases, and performance issues
2. Suggest improvements to readability and maintainability
3. Highlight security vulnerabilities or potential risks
4. Reference design patterns or library functions that could improve the implementation
5. Provide specific, actionable feedback with examples

Balance constructive criticism with acknowledgment of well-written code.
</pre></div>

</div>
</div>
<div class="section-level-extent" id="Workflow-Examples">
<h3 class="section"><span>13.10 Workflow Examples<a class="copiable-link" href="#Workflow-Examples"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Python-Code-Assistance" accesskey="1">Python Code Assistance</a></li>
<li><a href="#Technical-Writing-Help" accesskey="2">Technical Writing Help</a></li>
</ul>
<div class="subsection-level-extent" id="Python-Code-Assistance">
<h4 class="subsection"><span>13.10.1 Python Code Assistance<a class="copiable-link" href="#Python-Code-Assistance"> &para;</a></span></h4>

<ol class="enumerate">
<li> Load your &quot;Python Expert&quot; system prompt with <code class="code">C-c s L</code>
</li><li> Ask coding questions or paste code for analysis
</li><li> The AI responds with Python-specific expertise
</li><li> Save the conversation as a session for future reference
</li></ol>

</div>
<div class="subsection-level-extent" id="Technical-Writing-Help">
<h4 class="subsection"><span>13.10.2 Technical Writing Help<a class="copiable-link" href="#Technical-Writing-Help"> &para;</a></span></h4>

<ol class="enumerate">
<li> Create a new system prompt for technical writing (<code class="code">C-c s N</code>)
</li><li> Define the AI&rsquo;s role as a technical writing assistant
</li><li> Save the prompt in the &quot;writing&quot; category
</li><li> Load this prompt whenever you need help with documentation
</li><li> The AI consistently provides responses optimized for technical writing
</li></ol>

</div>
</div>
<div class="section-level-extent" id="Integration-with-Roles">
<h3 class="section"><span>13.11 Integration with Roles<a class="copiable-link" href="#Integration-with-Roles"> &para;</a></span></h3>

<p>System prompts can be integrated with Ollama Buddy roles for more specialized workflows:
</p>
<ol class="enumerate">
<li> Create a system prompt for a specific purpose
</li><li> Test and refine it through direct interaction
</li><li> Once effective, save it to your prompt library
</li><li> Reference this prompt in a custom role definition
</li></ol>

<p>This creates a reusable AI assistant configuration that can be shared and improved over time.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="Roles-and-Commands">
<div class="nav-panel">
<p>
Next: <a href="#Fabric-Pattern-Integration" accesskey="n" rel="next">Fabric Pattern Integration</a>, Previous: <a href="#User-System-Prompts" accesskey="p" rel="prev">User System Prompts</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Roles-and-Commands-1"><span>14 Roles and Commands<a class="copiable-link" href="#Roles-and-Commands-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Understanding-Roles" accesskey="1">Understanding Roles</a></li>
<li><a href="#Role-File-Naming-Convention" accesskey="2">Role File Naming Convention</a></li>
<li><a href="#Built_002din-Commands" accesskey="3">Built-in Commands</a></li>
<li><a href="#Creating-Custom-Roles" accesskey="4">Creating Custom Roles</a></li>
<li><a href="#Switching-Roles" accesskey="5">Switching Roles</a></li>
<li><a href="#Managing-Role-Files" accesskey="6">Managing Role Files</a></li>
<li><a href="#Advanced-Role-Customization" accesskey="7">Advanced Role Customization</a></li>
<li><a href="#Role-Examples" accesskey="8">Role Examples</a></li>
<li><a href="#Tips-for-Effective-Role-Usage" accesskey="9">Tips for Effective Role Usage</a></li>
</ul>
<div class="section-level-extent" id="Understanding-Roles">
<h3 class="section"><span>14.1 Understanding Roles<a class="copiable-link" href="#Understanding-Roles"> &para;</a></span></h3>

<p>Roles in Ollama Buddy are collections of commands with specific configurations:
</p><ul class="itemize mark-bullet">
<li>Each role has its own set of commands
</li><li>Commands can use specific models
</li><li>Commands can have specialized system prompts
</li><li>Commands can have specialized parameters
</li></ul>

<p>This allows you to create specialized assistants for different workflows.
</p>
</div>
<div class="section-level-extent" id="Role-File-Naming-Convention">
<h3 class="section"><span>14.2 Role File Naming Convention<a class="copiable-link" href="#Role-File-Naming-Convention"> &para;</a></span></h3>

<p>The file naming convention is critical to understand how roles, preset files, and menu configurations work together:
</p>
<dl class="table">
<dt>Required filename format</dt>
<dd><p><code class="code">ollama-buddy--preset__ROLE-NAME.el</code>
</p><ul class="itemize mark-bullet">
<li>The double underscore <code class="code">__</code> separates the prefix from your role name
</li><li>The role name portion becomes the identifier shown when switching roles
</li><li>Example: <code class="code">ollama-buddy--preset__programmer.el</code> creates a role named &quot;programmer&quot;
</li></ul>
</dd>
</dl>

<p>This naming convention is how Ollama Buddy discovers and identifies role files. When you run <code class="code">ollama-buddy-roles-switch-role</code>, the system:
</p>
<ol class="enumerate">
<li> Scans the <code class="code">ollama-buddy-roles-directory</code> for files matching the pattern
</li><li> Extracts the role name from each filename (the part after <code class="code">__</code>)
</li><li> Presents these names in the role selection interface
</li><li> When selected, loads the corresponding file which redefines <code class="code">ollama-buddy-command-definitions</code>
</li><li> This redefinition immediately changes the available commands in your Ollama Buddy menu
</li></ol>

<p>The relationship chain works like this:
</p><div class="example">
<pre class="example-preformatted">ollama-buddy--preset__ROLE-NAME.el â Defines ollama-buddy-command-definitions â Controls menu content
</pre></div>

<p>When creating roles using the interactive role creator (<code class="code">C-c E</code>), this naming convention is automatically handled for you. When creating roles manually, you must follow this pattern for Ollama Buddy to recognize your role files correctly.
</p>
</div>
<div class="section-level-extent" id="Built_002din-Commands">
<h3 class="section"><span>14.3 Built-in Commands<a class="copiable-link" href="#Built_002din-Commands"> &para;</a></span></h3>

<p>Ollama Buddy comes with several built-in commands:
</p>
<dl class="table">
<dt>refactor-code</dt>
<dd><p>Improves code while maintaining functionality
</p>
</dd>
<dt>describe-code</dt>
<dd><p>Explains what code does and how it works
</p>
</dd>
<dt>git-commit</dt>
<dd><p>Generates meaningful commit messages
</p>
</dd>
<dt>dictionary-lookup</dt>
<dd><p>Provides comprehensive word definitions
</p>
</dd>
<dt>synonym</dt>
<dd><p>Suggests alternative words with context
</p>
</dd>
<dt>proofread</dt>
<dd><p>Corrects grammar, style, and spelling
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Creating-Custom-Roles">
<h3 class="section"><span>14.4 Creating Custom Roles<a class="copiable-link" href="#Creating-Custom-Roles"> &para;</a></span></h3>

<p>There are two ways to create custom roles:
</p>
<ul class="mini-toc">
<li><a href="#Interactive-Role-Creator" accesskey="1">Interactive Role Creator</a></li>
<li><a href="#Manual-Role-Creation" accesskey="2">Manual Role Creation</a></li>
</ul>
<div class="subsection-level-extent" id="Interactive-Role-Creator">
<h4 class="subsection"><span>14.4.1 Interactive Role Creator<a class="copiable-link" href="#Interactive-Role-Creator"> &para;</a></span></h4>

<p>The most user-friendly approach:
</p>
<ol class="enumerate">
<li> Press <code class="code">C-c E</code> or run <code class="code">M-x ollama-buddy-role-creator-create-new-role</code>
</li><li> Enter a name for your role (e.g., &quot;programmer&quot;)
</li><li> For each command you want to add:
  <ul class="itemize mark-bullet">
<li>Specify a command name (e.g., &quot;refactor-code&quot;)
  </li><li>Choose a key shortcut for the menu
  </li><li>Add a description
  </li><li>Optionally specify a model
  </li><li>Optionally add prompt prefixes and system messages
  </li></ul>
</li></ol>

<p>The interactive creator automatically handles file naming and placement.
</p>
</div>
<div class="subsection-level-extent" id="Manual-Role-Creation">
<h4 class="subsection"><span>14.4.2 Manual Role Creation<a class="copiable-link" href="#Manual-Role-Creation"> &para;</a></span></h4>

<p>For more advanced customization, create role files manually:
</p>
<ol class="enumerate">
<li> Create a file named <code class="code">ollama-buddy--preset__your-role-name.el</code> in your <code class="code">ollama-buddy-roles-directory</code>
</li><li> Structure your file like this:
</li></ol>

<div class="example">
<pre class="example-preformatted">;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands - always include these
    (open-chat
     :key ?o
     :description &quot;Open chat buffer&quot;
     :action ollama-buddy--open-chat)
    
    (show-models
     :key ?v
     :description &quot;View model status&quot;
     :action ollama-buddy-show-model-status)
    
    (switch-role
     :key ?R
     :description &quot;Switch roles&quot;
     :action ollama-buddy-roles-switch-role)
    
    (create-role
     :key ?E
     :description &quot;Create new role&quot;
     :action ollama-buddy-role-creator-create-new-role)
    
    (open-roles-directory
     :key ?D
     :description &quot;Open roles directory&quot;
     :action ollama-buddy-roles-open-directory)
    
    ;; Custom commands for this role
    (refactor-code
     :key ?r
     :description &quot;Refactor code&quot;
     :model &quot;codellama:7b&quot;
     :prompt &quot;Refactor this code to improve readability and efficiency:&quot;
     :system &quot;You are an expert software engineer who improves code quality.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description &quot;Explain code&quot;
     :model &quot;deepseek-r1:7b&quot;
     :prompt &quot;Explain what this code does in detail:&quot;
     :system &quot;You are a programming teacher who explains code clearly.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (git-commit
     :key ?g
     :description &quot;Git commit message&quot;
     :prompt &quot;Write a concise git commit message for these changes:&quot;
     :system &quot;You are a version control expert who creates clear commit messages.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
</pre></div>

</div>
</div>
<div class="section-level-extent" id="Switching-Roles">
<h3 class="section"><span>14.5 Switching Roles<a class="copiable-link" href="#Switching-Roles"> &para;</a></span></h3>

<p>To switch between roles:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-roles-switch-role
</pre></div>
<p>or press <code class="code">C-c R</code> in the chat buffer.
</p>
<p>You&rsquo;ll be presented with a list of available roles to choose from.
</p>
</div>
<div class="section-level-extent" id="Managing-Role-Files">
<h3 class="section"><span>14.6 Managing Role Files<a class="copiable-link" href="#Managing-Role-Files"> &para;</a></span></h3>

<p>Roles are stored as Elisp files in the <code class="code">ollama-buddy-roles-directory</code>.
</p>
<p>To locate your roles directory:
</p><div class="example">
<pre class="example-preformatted">;; Check where your roles are stored
(message ollama-buddy-roles-directory)
</pre></div>

<p>By default, this is set to <code class="code">~/.emacs.d/ollama-buddy-presets/</code>, but you can customize it:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-roles-directory &quot;/your/custom/path/to/presets&quot;)
</pre></div>

<p>To open this directory:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-roles-open-directory
</pre></div>
<p>or press <code class="code">C-c D</code> in the chat buffer.
</p>
</div>
<div class="section-level-extent" id="Advanced-Role-Customization">
<h3 class="section"><span>14.7 Advanced Role Customization<a class="copiable-link" href="#Advanced-Role-Customization"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Command_002dSpecific-Models" accesskey="1">Command-Specific Models</a></li>
<li><a href="#Command_002dSpecific-Parameters-1" accesskey="2">Command-Specific Parameters</a></li>
<li><a href="#Creating-New-Commands" accesskey="3">Creating New Commands</a></li>
</ul>
<div class="subsection-level-extent" id="Command_002dSpecific-Models">
<h4 class="subsection"><span>14.7.1 Command-Specific Models<a class="copiable-link" href="#Command_002dSpecific-Models"> &para;</a></span></h4>

<p>Assign specific models to commands for optimal performance:
</p>
<div class="example">
<pre class="example-preformatted">(ollama-buddy-add-model-to-menu-entry 'refactor-code &quot;codellama:7b&quot;)
</pre></div>

</div>
<div class="subsection-level-extent" id="Command_002dSpecific-Parameters-1">
<h4 class="subsection"><span>14.7.2 Command-Specific Parameters<a class="copiable-link" href="#Command_002dSpecific-Parameters-1"> &para;</a></span></h4>

<p>Optimize parameters for specific commands:
</p>
<div class="example">
<pre class="example-preformatted">(ollama-buddy-add-parameters-to-command 'refactor-code
  'temperature 0.2
  'top_p 0.7
  'repeat_penalty 1.3)
</pre></div>

</div>
<div class="subsection-level-extent" id="Creating-New-Commands">
<h4 class="subsection"><span>14.7.3 Creating New Commands<a class="copiable-link" href="#Creating-New-Commands"> &para;</a></span></h4>

<p>Add entirely new commands to your menu:
</p>
<div class="example">
<pre class="example-preformatted">(ollama-buddy-update-menu-entry 'my-new-command
  :key ?z
  :description &quot;My new awesome command&quot;
  :prompt &quot;Here is what I want you to do:&quot;
  :system &quot;You are an expert system specialized in this task.&quot;
  :action (lambda () (ollama-buddy--send-with-command 'my-new-command)))
</pre></div>

</div>
</div>
<div class="section-level-extent" id="Role-Examples">
<h3 class="section"><span>14.8 Role Examples<a class="copiable-link" href="#Role-Examples"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Programming-Role" accesskey="1">Programming Role</a></li>
<li><a href="#Writing-Role" accesskey="2">Writing Role</a></li>
</ul>
<div class="subsection-level-extent" id="Programming-Role">
<h4 class="subsection"><span>14.8.1 Programming Role<a class="copiable-link" href="#Programming-Role"> &para;</a></span></h4>

<p>A complete example of a programming-focused role:
</p>
<div class="example">
<pre class="example-preformatted">;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description &quot;Open chat buffer&quot; :action ollama-buddy--open-chat)
    (show-models :key ?v :description &quot;View model status&quot; :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description &quot;Switch roles&quot; :action ollama-buddy-roles-switch-role)
    
    ;; Programming-specific commands
    (refactor-code
     :key ?r
     :description &quot;Refactor code&quot;
     :model &quot;codellama:7b&quot;
     :prompt &quot;Refactor this code to improve readability and efficiency:&quot;
     :system &quot;You are an expert software engineer who improves code quality.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description &quot;Explain code&quot;
     :model &quot;deepseek-r1:7b&quot;
     :prompt &quot;Explain what this code does in detail:&quot;
     :system &quot;You are a programming teacher who explains code clearly.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (add-tests
     :key ?t
     :description &quot;Generate tests&quot;
     :model &quot;qwen2.5-coder:7b&quot;
     :prompt &quot;Generate unit tests for this code:&quot;
     :system &quot;You are a test automation expert who creates comprehensive test cases.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'add-tests)))
    
    (git-commit
     :key ?g
     :description &quot;Git commit message&quot;
     :prompt &quot;Write a concise git commit message for these changes:&quot;
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
</pre></div>

</div>
<div class="subsection-level-extent" id="Writing-Role">
<h4 class="subsection"><span>14.8.2 Writing Role<a class="copiable-link" href="#Writing-Role"> &para;</a></span></h4>

<p>A complete example of a writing-focused role:
</p>
<div class="example">
<pre class="example-preformatted">;; ollama-buddy preset for role: writer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description &quot;Open chat buffer&quot; :action ollama-buddy--open-chat)
    (show-models :key ?v :description &quot;View model status&quot; :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description &quot;Switch roles&quot; :action ollama-buddy-roles-switch-role)
    
    ;; Writing-focused commands
    (summarize
     :key ?s
     :description &quot;Summarize text&quot;
     :prompt &quot;Summarize the following text in a concise manner:&quot;
     :system &quot;You are an expert at extracting the key points from any text.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'summarize)))
    
    (proofread
     :key ?p
     :description &quot;Proofread text&quot;
     :model &quot;deepseek-r1:7b&quot;
     :prompt &quot;Proofread the following text and correct any errors:&quot;
     :system &quot;You are a professional editor who identifies and corrects grammar and style errors.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'proofread)))
    
    (rewrite
     :key ?r
     :description &quot;Rewrite text&quot;
     :prompt &quot;Rewrite the following text to improve clarity and flow:&quot;
     :system &quot;You are a skilled writer who can improve any text while preserving its meaning.&quot;
     :action (lambda () (ollama-buddy--send-with-command 'rewrite)))
    
    (brainstorm
     :key ?b
     :description &quot;Brainstorm ideas&quot;
     :model &quot;llama3.2:3b&quot;
     :prompt &quot;Generate creative ideas related to the following topic:&quot;
     :parameters ((temperature . 1.0) (top_p . 0.95))
     :action (lambda () (ollama-buddy--send-with-command 'brainstorm)))
    ))
</pre></div>

</div>
</div>
<div class="section-level-extent" id="Tips-for-Effective-Role-Usage">
<h3 class="section"><span>14.9 Tips for Effective Role Usage<a class="copiable-link" href="#Tips-for-Effective-Role-Usage"> &para;</a></span></h3>

<ol class="enumerate">
<li> Group related commands: Create roles around specific workflows or tasks
</li><li> Match models to tasks: Use lightweight models for simple tasks and more powerful models for complex ones
</li><li> Customize system prompts: Craft specific system prompts to guide the model for each command
</li><li> Use the roles directory: Press <code class="code">C-c D</code> to quickly access and manage your role files
</li><li> Create specialized roles: Consider roles for programming, writing, translation, or domain-specific knowledge
</li></ol>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Fabric-Pattern-Integration">
<div class="nav-panel">
<p>
Next: <a href="#Awesome-ChatGPT-Prompts" accesskey="n" rel="next">Awesome ChatGPT Prompts</a>, Previous: <a href="#Roles-and-Commands" accesskey="p" rel="prev">Roles and Commands</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Fabric-Pattern-Integration-1"><span>15 Fabric Pattern Integration<a class="copiable-link" href="#Fabric-Pattern-Integration-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#What-are-Fabric-Patterns_003f" accesskey="1">What are Fabric Patterns?</a></li>
<li><a href="#Setting-Up-Fabric-Integration" accesskey="2">Setting Up Fabric Integration</a></li>
<li><a href="#Using-Fabric-Patterns" accesskey="3">Using Fabric Patterns</a></li>
<li><a href="#Browsing-Available-Patterns" accesskey="4">Browsing Available Patterns</a></li>
<li><a href="#Viewing-Pattern-Details" accesskey="5">Viewing Pattern Details</a></li>
<li><a href="#Updating-Patterns" accesskey="6">Updating Patterns</a></li>
<li><a href="#Using-Patterns-by-Category" accesskey="7">Using Patterns by Category</a></li>
</ul>
<div class="section-level-extent" id="What-are-Fabric-Patterns_003f">
<h3 class="section"><span>15.1 What are Fabric Patterns?<a class="copiable-link" href="#What-are-Fabric-Patterns_003f"> &para;</a></span></h3>

<p>Fabric patterns are pre-defined prompt templates from Daniel Miessler&rsquo;s Fabric project (<a class="url" href="https://github.com/danielmiessler/fabric">https://github.com/danielmiessler/fabric</a>). They provide optimized prompts for various tasks, categorized as:
</p>
<ul class="itemize mark-bullet">
<li>universal - General-purpose patterns
</li><li>code - Programming and development
</li><li>writing - Content creation and editing
</li><li>analysis - Data and concept examination
</li></ul>

</div>
<div class="section-level-extent" id="Setting-Up-Fabric-Integration">
<h3 class="section"><span>15.2 Setting Up Fabric Integration<a class="copiable-link" href="#Setting-Up-Fabric-Integration"> &para;</a></span></h3>

<p>To set up Fabric integration:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-setup
</pre></div>

<p>This will:
</p><ol class="enumerate">
<li> Clone the Fabric repository (or set up sparse checkout)
</li><li> Populate available patterns
</li><li> Make patterns available for use
</li></ol>

</div>
<div class="section-level-extent" id="Using-Fabric-Patterns">
<h3 class="section"><span>15.3 Using Fabric Patterns<a class="copiable-link" href="#Using-Fabric-Patterns"> &para;</a></span></h3>

<p>To use a Fabric pattern:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-send
</pre></div>
<p>or press <code class="code">C-c f</code> and then <code class="code">s</code>.
</p>
<p>You&rsquo;ll be prompted to:
</p><ol class="enumerate">
<li> Select a pattern
</li><li> Enter text to process (or use selected text)
</li></ol>

<p>The pattern will be used as a system prompt for your request.
</p>
</div>
<div class="section-level-extent" id="Browsing-Available-Patterns">
<h3 class="section"><span>15.4 Browsing Available Patterns<a class="copiable-link" href="#Browsing-Available-Patterns"> &para;</a></span></h3>

<p>To see all available patterns:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-list-patterns
</pre></div>
<p>or press <code class="code">C-c f</code> and then <code class="code">l</code>.
</p>
<p>This shows:
</p><ul class="itemize mark-bullet">
<li>Pattern names
</li><li>Categories
</li><li>Descriptions
</li></ul>

</div>
<div class="section-level-extent" id="Viewing-Pattern-Details">
<h3 class="section"><span>15.5 Viewing Pattern Details<a class="copiable-link" href="#Viewing-Pattern-Details"> &para;</a></span></h3>

<p>To see the full content of a specific pattern:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-show-pattern
</pre></div>
<p>or press <code class="code">C-c f</code> and then <code class="code">v</code>.
</p>
<p>Select a pattern to see:
</p><ul class="itemize mark-bullet">
<li>The system prompt content
</li><li>Full description
</li></ul>

</div>
<div class="section-level-extent" id="Updating-Patterns">
<h3 class="section"><span>15.6 Updating Patterns<a class="copiable-link" href="#Updating-Patterns"> &para;</a></span></h3>

<p>To sync with the latest patterns from GitHub:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-sync-patterns
</pre></div>
<p>or press <code class="code">C-c f</code> and then <code class="code">S</code>.
</p>
</div>
<div class="section-level-extent" id="Using-Patterns-by-Category">
<h3 class="section"><span>15.7 Using Patterns by Category<a class="copiable-link" href="#Using-Patterns-by-Category"> &para;</a></span></h3>

<p>You can quickly access patterns by category:
</p><ul class="itemize mark-bullet">
<li><code class="code">C-c f u</code> - Universal patterns
</li><li><code class="code">C-c f c</code> - Code patterns
</li><li><code class="code">C-c f w</code> - Writing patterns
</li><li><code class="code">C-c f a</code> - Analysis patterns
</li></ul>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Awesome-ChatGPT-Prompts">
<div class="nav-panel">
<p>
Next: <a href="#Remote-Providers" accesskey="n" rel="next">Remote Providers</a>, Previous: <a href="#Fabric-Pattern-Integration" accesskey="p" rel="prev">Fabric Pattern Integration</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Awesome-ChatGPT-Prompts-1"><span>16 Awesome ChatGPT Prompts<a class="copiable-link" href="#Awesome-ChatGPT-Prompts-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#What-is-Awesome-ChatGPT-Prompts_003f" accesskey="1">What is Awesome ChatGPT Prompts?</a></li>
<li><a href="#Setting-Up-Awesome-ChatGPT-Prompts" accesskey="2">Setting Up Awesome ChatGPT Prompts</a></li>
<li><a href="#Using-Awesome-ChatGPT-Prompts" accesskey="3">Using Awesome ChatGPT Prompts</a></li>
<li><a href="#Browsing-Available-Prompts" accesskey="4">Browsing Available Prompts</a></li>
<li><a href="#Categorized-Browsing" accesskey="5">Categorized Browsing</a></li>
<li><a href="#Viewing-Prompt-Details" accesskey="6">Viewing Prompt Details</a></li>
<li><a href="#Updating-Prompts" accesskey="7">Updating Prompts</a></li>
<li><a href="#Setting-Without-Sending" accesskey="8">Setting Without Sending</a></li>
<li><a href="#Example-Usage" accesskey="9">Example Usage</a></li>
</ul>
<div class="section-level-extent" id="What-is-Awesome-ChatGPT-Prompts_003f">
<h3 class="section"><span>16.1 What is Awesome ChatGPT Prompts?<a class="copiable-link" href="#What-is-Awesome-ChatGPT-Prompts_003f"> &para;</a></span></h3>

<p>Awesome ChatGPT Prompts is a curated collection of prompt templates created by the community and maintained in the GitHub repository at <a class="url" href="https://github.com/f/awesome-chatgpt-prompts">https://github.com/f/awesome-chatgpt-prompts</a>. These prompts are designed to make ChatGPT (and other LLMs) act as various specialized personas or experts, such as:
</p>
<ul class="itemize mark-bullet">
<li>Writing professionals (poets, storytellers, copywriters)
</li><li>Technical experts (programmers, researchers, scientists)
</li><li>Creative professionals (artists, designers, photographers)
</li><li>Business experts (marketers, consultants, strategists)
</li><li>And many more specialized roles
</li></ul>

</div>
<div class="section-level-extent" id="Setting-Up-Awesome-ChatGPT-Prompts">
<h3 class="section"><span>16.2 Setting Up Awesome ChatGPT Prompts<a class="copiable-link" href="#Setting-Up-Awesome-ChatGPT-Prompts"> &para;</a></span></h3>

<p>To set up the Awesome ChatGPT Prompts integration:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-setup
</pre></div>

<p>This will:
</p><ol class="enumerate">
<li> Create a sparse checkout of the Awesome ChatGPT Prompts repository
</li><li> Download only the necessary files (prompts.csv and README)
</li><li> Populate and categorize the available prompts
</li></ol>

</div>
<div class="section-level-extent" id="Using-Awesome-ChatGPT-Prompts">
<h3 class="section"><span>16.3 Using Awesome ChatGPT Prompts<a class="copiable-link" href="#Using-Awesome-ChatGPT-Prompts"> &para;</a></span></h3>

<p>To use an Awesome ChatGPT Prompt:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-send
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">s</code>.
</p>
<p>You&rsquo;ll be prompted to:
</p><ol class="enumerate">
<li> Select a prompt from the categorized list
</li><li> Enter text to process (or use selected text)
</li></ol>

<p>The selected prompt will be used as a system prompt for your request, transforming how the AI responds to your text.
</p>
</div>
<div class="section-level-extent" id="Browsing-Available-Prompts">
<h3 class="section"><span>16.4 Browsing Available Prompts<a class="copiable-link" href="#Browsing-Available-Prompts"> &para;</a></span></h3>

<p>To see all available prompts:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-list-prompts
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">l</code>.
</p>
<p>This shows:
</p><ul class="itemize mark-bullet">
<li>Prompt titles
</li><li>Categories
</li><li>Preview of prompt content
</li></ul>

</div>
<div class="section-level-extent" id="Categorized-Browsing">
<h3 class="section"><span>16.5 Categorized Browsing<a class="copiable-link" href="#Categorized-Browsing"> &para;</a></span></h3>

<p>Ollama Buddy automatically categorizes the Awesome ChatGPT Prompts into useful groups:
</p><ul class="itemize mark-bullet">
<li>writing - For writing, poetry, and creative content
</li><li>code - For programming and development
</li><li>business - For marketing, entrepreneurship, and business strategy
</li><li>academic - For educational and research content
</li><li>creative - For artistic and design-related prompts
</li><li>philosophy - For philosophical reasoning and ethics
</li><li>health - For medical, fitness, and wellness
</li><li>legal - For law-related prompts
</li><li>finance - For financial advice and analysis
</li><li>other - Miscellaneous prompts
</li></ul>

<p>To browse by category:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-show-prompts-menu
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">c</code>.
</p>
</div>
<div class="section-level-extent" id="Viewing-Prompt-Details">
<h3 class="section"><span>16.6 Viewing Prompt Details<a class="copiable-link" href="#Viewing-Prompt-Details"> &para;</a></span></h3>

<p>To see the full content of a specific prompt:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-show-prompt
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">v</code>.
</p>
<p>Select a prompt to see its complete template.
</p>
</div>
<div class="section-level-extent" id="Updating-Prompts">
<h3 class="section"><span>16.7 Updating Prompts<a class="copiable-link" href="#Updating-Prompts"> &para;</a></span></h3>

<p>To sync with the latest prompts from GitHub:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-sync-prompts
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">S</code>.
</p>
</div>
<div class="section-level-extent" id="Setting-Without-Sending">
<h3 class="section"><span>16.8 Setting Without Sending<a class="copiable-link" href="#Setting-Without-Sending"> &para;</a></span></h3>

<p>To set a prompt as the system prompt without sending text:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-awesome-set-system-prompt
</pre></div>
<p>or press <code class="code">C-c w</code> and then <code class="code">p</code>.
</p>
<p>This is useful when you want to set up a specific persona before starting a conversation.
</p>
</div>
<div class="section-level-extent" id="Example-Usage">
<h3 class="section"><span>16.9 Example Usage<a class="copiable-link" href="#Example-Usage"> &para;</a></span></h3>

<p>Some popular prompts include:
</p><ul class="itemize mark-bullet">
<li>&quot;Act as a poet&quot; - Transforms your text into poetry
</li><li>&quot;Act as a Linux terminal&quot; - Simulates a Linux terminal interface
</li><li>&quot;Act as a gaslighter&quot; - Responds in a deliberately confusing manner
</li><li>&quot;Act as a javascript console&quot; - Simulates a JavaScript console
</li><li>&quot;Act as an English translator&quot; - Translates text to proper English
</li></ul>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Remote-Providers">
<div class="nav-panel">
<p>
Next: <a href="#Ollama-Cloud-Models" accesskey="n" rel="next">Ollama Cloud Models</a>, Previous: <a href="#Awesome-ChatGPT-Prompts" accesskey="p" rel="prev">Awesome ChatGPT Prompts</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Remote-Providers-1"><span>17 Remote Providers<a class="copiable-link" href="#Remote-Providers-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview-3" accesskey="1">Overview</a></li>
<li><a href="#Status-Line-Indicators" accesskey="2">Status Line Indicators</a></li>
<li><a href="#Setting-Up-API-Access" accesskey="3">Setting Up API Access</a></li>
<li><a href="#Selecting-Remote-Models" accesskey="4">Selecting Remote Models</a></li>
<li><a href="#Provider-Configuration" accesskey="5">Provider Configuration</a></li>
<li><a href="#History-Management" accesskey="6">History Management</a></li>
</ul>
<div class="section-level-extent" id="Overview-3">
<h3 class="section"><span>17.1 Overview<a class="copiable-link" href="#Overview-3"> &para;</a></span></h3>

<p>Ollama Buddy integrates with multiple commercial AI services, each loaded on-demand:
</p>
<dl class="table">
<dt>OpenAI (prefix: <code class="code">a:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-openai nil t)</code>
</p>
</dd>
<dt>Claude/Anthropic (prefix: <code class="code">c:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-claude nil t)</code>
</p>
</dd>
<dt>Google Gemini (prefix: <code class="code">g:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-gemini nil t)</code>
</p>
</dd>
<dt>Grok/X (prefix: <code class="code">k:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-grok nil t)</code>
</p>
</dd>
<dt>GitHub Copilot (prefix: <code class="code">p:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-copilot nil t)</code>
</p>
</dd>
<dt>Codestral/Mistral (prefix: <code class="code">s:</code>)</dt>
<dd><p>Load with <code class="code">(require 'ollama-buddy-codestral nil t)</code>
</p></dd>
</dl>

<p>Local Ollama models use the <code class="code">o:</code> prefix when remote models are available, or no prefix when only local models are present.
</p>
</div>
<div class="section-level-extent" id="Status-Line-Indicators">
<h3 class="section"><span>17.2 Status Line Indicators<a class="copiable-link" href="#Status-Line-Indicators"> &para;</a></span></h3>

<p>The header line shows compact indicators for loaded providers:
</p>
<ul class="itemize mark-bullet">
<li><code class="code">â</code> - Currently using an Ollama cloud model
</li><li><code class="code">a</code> - OpenAI provider loaded
</li><li><code class="code">c</code> - Claude provider loaded
</li><li><code class="code">g</code> - Gemini provider loaded
</li><li><code class="code">k</code> - Grok provider loaded
</li><li><code class="code">p</code> - GitHub Copilot provider loaded
</li><li><code class="code">s</code> - Codestral provider loaded
</li></ul>

<p>Example: <code class="code">âacgkps</code> means cloud model with all six external providers loaded.
</p>
</div>
<div class="section-level-extent" id="Setting-Up-API-Access">
<h3 class="section"><span>17.3 Setting Up API Access<a class="copiable-link" href="#Setting-Up-API-Access"> &para;</a></span></h3>

<p>Before using commercial APIs, you need to set up API keys. The recommended approach is to use Emacs&rsquo; built-in auth-source:
</p>
<div class="example">
<pre class="example-preformatted">;; Add to ~/.authinfo.gpg (encrypted)
machine ollama-buddy-openai login apikey password YOUR_OPENAI_API_KEY
machine ollama-buddy-claude login apikey password YOUR_CLAUDE_API_KEY
machine ollama-buddy-gemini login apikey password YOUR_GEMINI_API_KEY
machine ollama-buddy-grok login apikey password YOUR_GROK_API_KEY
machine ollama-buddy-copilot login apikey password YOUR_GITHUB_TOKEN
machine ollama-buddy-codestral login apikey password YOUR_CODESTRAL_API_KEY
</pre></div>

<p>Then configure with auth-source:
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password :host &quot;ollama-buddy-openai&quot; :user &quot;apikey&quot;))
</pre></div>

</div>
<div class="section-level-extent" id="Selecting-Remote-Models">
<h3 class="section"><span>17.4 Selecting Remote Models<a class="copiable-link" href="#Selecting-Remote-Models"> &para;</a></span></h3>

<p>All remote models appear in the model selection list with their provider prefix:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy--swap-model
</pre></div>
<p>or press <code class="code">C-c m</code>.
</p>
<p>Models will appear like: <code class="code">a:gpt-4</code>, <code class="code">c:claude-3-opus</code>, <code class="code">g:gemini-pro</code>, etc.
</p>
</div>
<div class="section-level-extent" id="Provider-Configuration">
<h3 class="section"><span>17.5 Provider Configuration<a class="copiable-link" href="#Provider-Configuration"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#OpenAI-Configuration" accesskey="1">OpenAI Configuration</a></li>
<li><a href="#Claude-Configuration" accesskey="2">Claude Configuration</a></li>
<li><a href="#Gemini-Configuration" accesskey="3">Gemini Configuration</a></li>
<li><a href="#Grok-Configuration" accesskey="4">Grok Configuration</a></li>
<li><a href="#GitHub-Copilot-Configuration" accesskey="5">GitHub Copilot Configuration</a></li>
<li><a href="#Codestral-Configuration" accesskey="6">Codestral Configuration</a></li>
</ul>
<div class="subsection-level-extent" id="OpenAI-Configuration">
<h4 class="subsection"><span>17.5.1 OpenAI Configuration<a class="copiable-link" href="#OpenAI-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-openai-api-key</code></dt>
<dd><p>Your OpenAI API key.
</p>
</dd>
<dt><code class="code">ollama-buddy-openai-default-model</code></dt>
<dd><p>Default OpenAI model to use (e.g., &quot;gpt-4&quot;).
</p>
</dd>
<dt><code class="code">ollama-buddy-openai-temperature</code></dt>
<dd><p>Default temperature for OpenAI requests (0.0-2.0).
</p>
</dd>
<dt><code class="code">ollama-buddy-openai-max-tokens</code></dt>
<dd><p>Maximum tokens to generate (nil for API default).
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="Claude-Configuration">
<h4 class="subsection"><span>17.5.2 Claude Configuration<a class="copiable-link" href="#Claude-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-claude-api-key</code></dt>
<dd><p>Your Anthropic Claude API key.
</p>
</dd>
<dt><code class="code">ollama-buddy-claude-default-model</code></dt>
<dd><p>Default Claude model to use.
</p>
</dd>
<dt><code class="code">ollama-buddy-claude-temperature</code></dt>
<dd><p>Default temperature for Claude requests (0.0-1.0).
</p>
</dd>
<dt><code class="code">ollama-buddy-claude-max-tokens</code></dt>
<dd><p>Maximum tokens to generate.
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="Gemini-Configuration">
<h4 class="subsection"><span>17.5.3 Gemini Configuration<a class="copiable-link" href="#Gemini-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-gemini-api-key</code></dt>
<dd><p>Your Google Gemini API key.
</p>
</dd>
<dt><code class="code">ollama-buddy-gemini-default-model</code></dt>
<dd><p>Default Gemini model to use.
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="Grok-Configuration">
<h4 class="subsection"><span>17.5.4 Grok Configuration<a class="copiable-link" href="#Grok-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-grok-api-key</code></dt>
<dd><p>Your X/Grok API key.
</p>
</dd>
<dt><code class="code">ollama-buddy-grok-default-model</code></dt>
<dd><p>Default Grok model to use.
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="GitHub-Copilot-Configuration">
<h4 class="subsection"><span>17.5.5 GitHub Copilot Configuration<a class="copiable-link" href="#GitHub-Copilot-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-copilot-api-key</code></dt>
<dd><p>Your GitHub personal access token with Copilot scope.
Get your token from <a class="url" href="https://github.com/settings/tokens">https://github.com/settings/tokens</a>.
</p>
</dd>
<dt><code class="code">ollama-buddy-copilot-default-model</code></dt>
<dd><p>Default Copilot model to use (e.g., &quot;gpt-4o&quot;).
</p>
</dd>
<dt><code class="code">ollama-buddy-copilot-available-models</code></dt>
<dd><p>List of models available through GitHub Copilot.
</p></dd>
</dl>

<p>Note: GitHub Copilot requires an active Copilot subscription.
</p>
</div>
<div class="subsection-level-extent" id="Codestral-Configuration">
<h4 class="subsection"><span>17.5.6 Codestral Configuration<a class="copiable-link" href="#Codestral-Configuration"> &para;</a></span></h4>
<dl class="table">
<dt><code class="code">ollama-buddy-codestral-api-key</code></dt>
<dd><p>Your Mistral Codestral API key.
</p>
</dd>
<dt><code class="code">ollama-buddy-codestral-default-model</code></dt>
<dd><p>Default Codestral model to use.
</p></dd>
</dl>

</div>
</div>
<div class="section-level-extent" id="History-Management">
<h3 class="section"><span>17.6 History Management<a class="copiable-link" href="#History-Management"> &para;</a></span></h3>

<p>Each provider maintains its own conversation history, ensuring context is preserved appropriately per service.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="Ollama-Cloud-Models">
<div class="nav-panel">
<p>
Next: <a href="#Global-System-Prompt" accesskey="n" rel="next">Global System Prompt</a>, Previous: <a href="#Remote-Providers" accesskey="p" rel="prev">Remote Providers</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Ollama-Cloud-Models-1"><span>18 Ollama Cloud Models<a class="copiable-link" href="#Ollama-Cloud-Models-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview-4" accesskey="1">Overview</a></li>
<li><a href="#Available-Cloud-Models" accesskey="2">Available Cloud Models</a></li>
<li><a href="#Selecting-Cloud-Models" accesskey="3">Selecting Cloud Models</a></li>
<li><a href="#Cloud-Authentication" accesskey="4">Cloud Authentication</a></li>
<li><a href="#Cloud-Model-Indicator" accesskey="5">Cloud Model Indicator</a></li>
<li><a href="#Configuration-3" accesskey="6">Configuration</a></li>
</ul>
<div class="section-level-extent" id="Overview-4">
<h3 class="section"><span>18.1 Overview<a class="copiable-link" href="#Overview-4"> &para;</a></span></h3>

<p>Ollama cloud models run on ollama.com infrastructure and provide access to large models without local hardware requirements. These models require authentication via the Ollama CLI.
</p>
</div>
<div class="section-level-extent" id="Available-Cloud-Models">
<h3 class="section"><span>18.2 Available Cloud Models<a class="copiable-link" href="#Available-Cloud-Models"> &para;</a></span></h3>

<p>Cloud models are configured via <code class="code">ollama-buddy-cloud-models</code>:
</p>
<div class="example">
<pre class="example-preformatted">(setq ollama-buddy-cloud-models
  '(&quot;qwen3-coder:480b-cloud&quot;
    &quot;kimi-k2.5:cloud&quot;
    &quot;deepseek-v3.1:671b-cloud&quot;
    &quot;gpt-oss:120b-cloud&quot;
    &quot;gpt-oss:20b-cloud&quot;
    &quot;glm-4.7:cloud&quot;
    &quot;minimax-m2.1:cloud&quot;))
</pre></div>

<p>Cloud models have a <code class="code">-cloud</code> suffix in their names.
</p>
</div>
<div class="section-level-extent" id="Selecting-Cloud-Models">
<h3 class="section"><span>18.3 Selecting Cloud Models<a class="copiable-link" href="#Selecting-Cloud-Models"> &para;</a></span></h3>

<p>To select a cloud model:
</p><ul class="itemize mark-bullet">
<li>Use <code class="code">C-u C-c m</code> (universal argument with model switch)
</li><li>Use the transient menu: <code class="code">C-c O</code> then &quot;Model &gt; Cloud&quot;
</li></ul>

</div>
<div class="section-level-extent" id="Cloud-Authentication">
<h3 class="section"><span>18.4 Cloud Authentication<a class="copiable-link" href="#Cloud-Authentication"> &para;</a></span></h3>

<p>Authentication is handled via the Ollama CLI:
</p>
<dl class="table">
<dt>Sign In (<code class="code">M-x ollama-buddy-cloud-signin</code>)</dt>
<dd><p>Opens your browser for Ollama cloud login. Accessible from transient menu &quot;Cloud Auth &gt; Sign In&quot;.
</p>
</dd>
<dt>Sign Out (<code class="code">M-x ollama-buddy-cloud-signout</code>)</dt>
<dd><p>Signs out from cloud services. Accessible from transient menu &quot;Cloud Auth &gt; Sign Out&quot;.
</p>
</dd>
<dt>Check Status (<code class="code">M-x ollama-buddy-cloud-status</code>)</dt>
<dd><p>Verifies your authentication status. Accessible from transient menu &quot;Cloud Auth &gt; Auth Status&quot;.
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Cloud-Model-Indicator">
<h3 class="section"><span>18.5 Cloud Model Indicator<a class="copiable-link" href="#Cloud-Model-Indicator"> &para;</a></span></h3>

<p>When using a cloud model, the header line displays a <code class="code">â</code> symbol to indicate cloud model usage.
</p>
</div>
<div class="section-level-extent" id="Configuration-3">
<h3 class="section"><span>18.6 Configuration<a class="copiable-link" href="#Configuration-3"> &para;</a></span></h3>

<dl class="table">
<dt><code class="code">ollama-buddy-cloud-models</code></dt>
<dd><p>List of available Ollama cloud models.
</p>
</dd>
<dt><code class="code">ollama-buddy-ollama-executable</code></dt>
<dd><p>Path to the ollama CLI executable (default: &quot;ollama&quot;). Used for signin/signout commands.
</p></dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="Global-System-Prompt">
<div class="nav-panel">
<p>
Next: <a href="#Advanced-Usage" accesskey="n" rel="next">Advanced Usage</a>, Previous: <a href="#Ollama-Cloud-Models" accesskey="p" rel="prev">Ollama Cloud Models</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Global-System-Prompt-1"><span>19 Global System Prompt<a class="copiable-link" href="#Global-System-Prompt-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Overview-5" accesskey="1">Overview</a></li>
<li><a href="#Configuration-4" accesskey="2">Configuration</a></li>
<li><a href="#Toggling-Global-Prompt" accesskey="3">Toggling Global Prompt</a></li>
<li><a href="#How-It-Works-1" accesskey="4">How It Works</a></li>
</ul>
<div class="section-level-extent" id="Overview-5">
<h3 class="section"><span>19.1 Overview<a class="copiable-link" href="#Overview-5"> &para;</a></span></h3>

<p>The global system prompt provides baseline formatting instructions that are automatically prepended to all requests, regardless of the session-specific system prompt. This ensures consistent response formatting across all interactions.
</p>
</div>
<div class="section-level-extent" id="Configuration-4">
<h3 class="section"><span>19.2 Configuration<a class="copiable-link" href="#Configuration-4"> &para;</a></span></h3>

<dl class="table">
<dt><code class="code">ollama-buddy-global-system-prompt</code></dt>
<dd><p>The global prompt content prepended to all requests.
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-global-system-prompt
  &quot;Format responses in plain prose. Avoid markdown tables unless
specifically requested. Use clear paragraphs and bullet points
for structured information.&quot;)
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-global-system-prompt-enabled</code></dt>
<dd><p>Whether to enable the global system prompt (default: t).
</p><div class="example">
<pre class="example-preformatted">(setq ollama-buddy-global-system-prompt-enabled t)
</pre></div>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Toggling-Global-Prompt">
<h3 class="section"><span>19.3 Toggling Global Prompt<a class="copiable-link" href="#Toggling-Global-Prompt"> &para;</a></span></h3>

<p>To toggle the global system prompt on or off:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-global-system-prompt
</pre></div>
<p>or use the transient menu &quot;Settings &gt; Global Prompt&quot;.
</p>
</div>
<div class="section-level-extent" id="How-It-Works-1">
<h3 class="section"><span>19.4 How It Works<a class="copiable-link" href="#How-It-Works-1"> &para;</a></span></h3>

<p>When enabled, the global system prompt is combined with any session-specific system prompt:
</p>
<ul class="itemize mark-bullet">
<li>If both global and session prompts exist, they are combined with two newlines between them
</li><li>If only the global prompt exists, it is used alone
</li><li>If only a session prompt exists, it is used alone
</li><li>If neither exists, no system prompt is sent
</li></ul>

<p>This allows you to maintain consistent formatting while still using specialized prompts for specific tasks.
</p>
<hr>
</div>
</div>
<div class="chapter-level-extent" id="Advanced-Usage">
<div class="nav-panel">
<p>
Next: <a href="#API-Reference" accesskey="n" rel="next">API Reference</a>, Previous: <a href="#Global-System-Prompt" accesskey="p" rel="prev">Global System Prompt</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Advanced-Usage-1"><span>20 Advanced Usage<a class="copiable-link" href="#Advanced-Usage-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Managing-Token-Usage" accesskey="1">Managing Token Usage</a></li>
<li><a href="#Customizing-the-Interface" accesskey="2">Customizing the Interface</a></li>
<li><a href="#Editing-Conversation-History" accesskey="3">Editing Conversation History</a></li>
<li><a href="#Advanced-System-Prompt-Management" accesskey="4">Advanced System Prompt Management</a></li>
<li><a href="#Using-Direct-API-Access" accesskey="5">Using Direct API Access</a></li>
</ul>
<div class="section-level-extent" id="Managing-Token-Usage">
<h3 class="section"><span>20.1 Managing Token Usage<a class="copiable-link" href="#Managing-Token-Usage"> &para;</a></span></h3>

<p>Ollama Buddy can track token usage statistics:
</p>
<p>To toggle token statistics display after responses:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-token-display
</pre></div>
<p>or press <code class="code">C-c T</code> in the chat buffer.
</p>
<p>To view detailed token usage statistics:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-display-token-stats
</pre></div>
<p>or press <code class="code">C-c #</code> in the chat buffer.
</p>
<p>This displays a combined view with token usage graphs by model, average token rates, and recent interactions.
</p>
</div>
<div class="section-level-extent" id="Customizing-the-Interface">
<h3 class="section"><span>20.2 Customizing the Interface<a class="copiable-link" href="#Customizing-the-Interface"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Streaming-Options" accesskey="1">Streaming Options</a></li>
<li><a href="#Debug-Mode" accesskey="2">Debug Mode</a></li>
</ul>
<div class="subsection-level-extent" id="Streaming-Options">
<h4 class="subsection"><span>20.2.1 Streaming Options<a class="copiable-link" href="#Streaming-Options"> &para;</a></span></h4>

<p>Control how responses are displayed:
</p>
<dl class="table">
<dt><code class="code">ollama-buddy-streaming-enabled</code></dt>
<dd><p>Toggle streaming mode where responses appear token by token.
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-streaming
</pre></div>

</dd>
<dt><code class="code">ollama-buddy-auto-scroll</code></dt>
<dd><p>Enable auto-scrolling during streaming output.
</p>
</dd>
<dt><code class="code">ollama-buddy-pulse-response</code></dt>
<dd><p>Flash the response when streaming completes.
</p></dd>
</dl>

</div>
<div class="subsection-level-extent" id="Debug-Mode">
<h4 class="subsection"><span>20.2.2 Debug Mode<a class="copiable-link" href="#Debug-Mode"> &para;</a></span></h4>

<p>For advanced troubleshooting, you can enable debug mode:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-debug-mode
</pre></div>
<p>or press <code class="code">C-c B</code> in the chat buffer.
</p>
<p>This shows raw JSON messages in a debug buffer.
</p>
</div>
</div>
<div class="section-level-extent" id="Editing-Conversation-History">
<h3 class="section"><span>20.3 Editing Conversation History<a class="copiable-link" href="#Editing-Conversation-History"> &para;</a></span></h3>

<p>To manually edit conversation history:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-history-edit
</pre></div>
<p>or press <code class="code">C-c J</code> in the chat buffer.
</p>
<p>This opens an editable buffer with the conversation history. You can modify it and press <code class="code">C-c C-c</code> to save or <code class="code">C-c C-k</code> to cancel.
</p>
<p>To edit history for a specific model, use <code class="code">C-u C-c J</code>.
</p>
</div>
<div class="section-level-extent" id="Advanced-System-Prompt-Management">
<h3 class="section"><span>20.4 Advanced System Prompt Management<a class="copiable-link" href="#Advanced-System-Prompt-Management"> &para;</a></span></h3>

<p>For more control over system prompts:
</p>
<ul class="mini-toc">
<li><a href="#Setting-a-system-prompt-without-sending" accesskey="1">Setting a system prompt without sending</a></li>
<li><a href="#Using-a-system-prompt-from-Fabric" accesskey="2">Using a system prompt from Fabric</a></li>
</ul>
<div class="subsection-level-extent" id="Setting-a-system-prompt-without-sending">
<h4 class="subsection"><span>20.4.1 Setting a system prompt without sending<a class="copiable-link" href="#Setting-a-system-prompt-without-sending"> &para;</a></span></h4>
<div class="example">
<pre class="example-preformatted">(ollama-buddy-set-system-prompt)
</pre></div>
<p>Enter your system prompt, then press <code class="code">C-c s</code>.
</p>
</div>
<div class="subsection-level-extent" id="Using-a-system-prompt-from-Fabric">
<h4 class="subsection"><span>20.4.2 Using a system prompt from Fabric<a class="copiable-link" href="#Using-a-system-prompt-from-Fabric"> &para;</a></span></h4>
<div class="example">
<pre class="example-preformatted">M-x ollama-buddy-fabric-set-system-prompt
</pre></div>
<p>or press <code class="code">C-c f p</code>.
</p>
</div>
</div>
<div class="section-level-extent" id="Using-Direct-API-Access">
<h3 class="section"><span>20.5 Using Direct API Access<a class="copiable-link" href="#Using-Direct-API-Access"> &para;</a></span></h3>

<p>For direct programmatic access to Ollama:
</p>
<div class="example">
<pre class="example-preformatted">(ollama-buddy--make-request &quot;/api/tags&quot; &quot;GET&quot;)
</pre></div>

<p>Or with a payload:
</p><div class="example">
<pre class="example-preformatted">(ollama-buddy--make-request &quot;/api/chat&quot; &quot;POST&quot; 
                           (json-encode '((model . &quot;llama3:latest&quot;)
                                         (prompt . &quot;Hello&quot;))))
</pre></div>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="API-Reference">
<div class="nav-panel">
<p>
Next: <a href="#FAQ" accesskey="n" rel="next">Frequently Asked Questions</a>, Previous: <a href="#Advanced-Usage" accesskey="p" rel="prev">Advanced Usage</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="API-Reference-1"><span>21 API Reference<a class="copiable-link" href="#API-Reference-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Interactive-Functions" accesskey="1">Interactive Functions</a></li>
<li><a href="#Core-Functions" accesskey="2">Core Functions</a></li>
<li><a href="#Customization-Functions" accesskey="3">Customization Functions</a></li>
</ul>
<div class="section-level-extent" id="Interactive-Functions">
<h3 class="section"><span>21.1 Interactive Functions<a class="copiable-link" href="#Interactive-Functions"> &para;</a></span></h3>

<dl class="table">
<dt><code class="code">ollama-buddy-menu</code></dt>
<dd><p>Display the main Ollama Buddy menu.
</p>
</dd>
<dt><code class="code">ollama-buddy-transient-menu</code></dt>
<dd><p>Display the transient-based menu.
</p>
</dd>
<dt><code class="code">ollama-buddy--open-chat</code></dt>
<dd><p>Open the chat buffer.
</p>
</dd>
<dt><code class="code">ollama-buddy--send-prompt</code></dt>
<dd><p>Send the current prompt to the AI.
</p>
</dd>
<dt><code class="code">ollama-buddy--swap-model</code></dt>
<dd><p>Switch to a different model. With prefix argument, select from cloud models.
</p>
</dd>
<dt><code class="code">ollama-buddy--swap-model-cloud</code></dt>
<dd><p>Switch to an Ollama cloud model.
</p>
</dd>
<dt><code class="code">ollama-buddy-cloud-signin</code></dt>
<dd><p>Sign in to Ollama cloud services.
</p>
</dd>
<dt><code class="code">ollama-buddy-cloud-signout</code></dt>
<dd><p>Sign out from Ollama cloud services.
</p>
</dd>
<dt><code class="code">ollama-buddy-cloud-status</code></dt>
<dd><p>Check Ollama cloud authentication status.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-global-system-prompt</code></dt>
<dd><p>Toggle the global system prompt on or off.
</p>
</dd>
<dt><code class="code">ollama-buddy-unload-model</code></dt>
<dd><p>Unload a specific model to free up resources.
</p>
</dd>
<dt><code class="code">ollama-buddy-unload-all-models</code></dt>
<dd><p>Unload all currently running models.
</p>
</dd>
<dt><code class="code">ollama-buddy-manage-models</code></dt>
<dd><p>Display and manage available models.
</p>
</dd>
<dt><code class="code">ollama-buddy-pull-model</code></dt>
<dd><p>Pull a new model from Ollama Hub.
</p>
</dd>
<dt><code class="code">ollama-buddy-import-gguf-file</code></dt>
<dd><p>Import a GGUF file to create a custom model.
</p>
</dd>
<dt><code class="code">ollama-buddy-set-system-prompt</code></dt>
<dd><p>Set the current prompt as the system prompt.
</p>
</dd>
<dt><code class="code">ollama-buddy-reset-system-prompt</code></dt>
<dd><p>Reset the system prompt to default (none).
</p>
</dd>
<dt><code class="code">ollama-buddy-sessions-save</code></dt>
<dd><p>Save the current conversation as a session.
</p>
</dd>
<dt><code class="code">ollama-buddy-sessions-load</code></dt>
<dd><p>Load a previously saved session.
</p>
</dd>
<dt><code class="code">ollama-buddy-sessions-list</code></dt>
<dd><p>Display a list of saved sessions.
</p>
</dd>
<dt><code class="code">ollama-buddy-sessions-delete</code></dt>
<dd><p>Delete a saved session.
</p>
</dd>
<dt><code class="code">ollama-buddy-sessions-new</code></dt>
<dd><p>Start a new session.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-history</code></dt>
<dd><p>Toggle conversation history on/off.
</p>
</dd>
<dt><code class="code">ollama-buddy-clear-history</code></dt>
<dd><p>Clear the conversation history.
</p>
</dd>
<dt><code class="code">ollama-buddy-history-edit</code></dt>
<dd><p>Display the conversation history.
</p>
</dd>
<dt><code class="code">ollama-buddy-roles-switch-role</code></dt>
<dd><p>Switch to a different role.
</p>
</dd>
<dt><code class="code">ollama-buddy-role-creator-create-new-role</code></dt>
<dd><p>Create a new role.
</p>
</dd>
<dt><code class="code">ollama-buddy-params-display</code></dt>
<dd><p>Display current parameter settings.
</p>
</dd>
<dt><code class="code">ollama-buddy-params-edit</code></dt>
<dd><p>Edit a specific parameter.
</p>
</dd>
<dt><code class="code">ollama-buddy-params-reset</code></dt>
<dd><p>Reset all parameters to defaults.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-params-in-header</code></dt>
<dd><p>Toggle display of parameters in header.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-token-display</code></dt>
<dd><p>Toggle display of token statistics.
</p>
</dd>
<dt><code class="code">ollama-buddy-display-token-stats</code></dt>
<dd><p>Display token usage statistics with graphs and recent interactions.
</p>
</dd>
<dt><code class="code">ollama-buddy-fabric-setup</code></dt>
<dd><p>Set up Fabric pattern integration.
</p>
</dd>
<dt><code class="code">ollama-buddy-fabric-sync-patterns</code></dt>
<dd><p>Sync with the latest Fabric patterns.
</p>
</dd>
<dt><code class="code">ollama-buddy-fabric-list-patterns</code></dt>
<dd><p>List available Fabric patterns.
</p>
</dd>
<dt><code class="code">ollama-buddy-fabric-send</code></dt>
<dd><p>Apply a Fabric pattern to selected text.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-markdown-conversion</code></dt>
<dd><p>Toggle Markdown to Org conversion.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-streaming</code></dt>
<dd><p>Toggle streaming mode for responses.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-reasoning-visibility</code></dt>
<dd><p>Toggle visibility of reasoning/thinking sections in responses.
</p>
</dd>
<dt><code class="code">ollama-buddy-switch-communication-backend</code></dt>
<dd><p>Interactively switch between network-process and curl backends.
</p>
</dd>
<dt><code class="code">ollama-buddy-test-communication-backend</code></dt>
<dd><p>Test the current communication backend.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-debug-mode</code></dt>
<dd><p>Toggle display of debug information.
</p>
</dd>
<dt><code class="code">ollama-buddy-set-model-context-size</code></dt>
<dd><p>Set the context size for a specific model.
</p>
</dd>
<dt><code class="code">ollama-buddy-toggle-context-percentage</code></dt>
<dd><p>Toggle context percentage display in the status bar.
</p>
</dd>
<dt><code class="code">ollama-buddy-show-context-info</code></dt>
<dd><p>Display detailed context usage information.
</p>
</dd>
<dt><code class="code">ollama-buddy-set-max-history-length</code></dt>
<dd><p>Set the maximum number of message pairs to keep in history.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-save</code></dt>
<dd><p>Save the current system prompt.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-load</code></dt>
<dd><p>Load a previously saved system prompt.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-list</code></dt>
<dd><p>Display a list of all saved user system prompts.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-edit</code></dt>
<dd><p>Edit a user system prompt.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-delete</code></dt>
<dd><p>Delete a user system prompt.
</p>
</dd>
<dt><code class="code">ollama-buddy-user-prompts-create-new</code></dt>
<dd><p>Create a new system prompt from scratch.
</p>
</dd>
<dt><code class="code">ollama-buddy-transient-user-prompts-menu</code></dt>
<dd><p>Display the transient menu for user system prompts.
</p>
</dd>
</dl>

</div>
<div class="section-level-extent" id="Core-Functions">
<h3 class="section"><span>21.2 Core Functions<a class="copiable-link" href="#Core-Functions"> &para;</a></span></h3>

<dl class="table">
<dt><code class="code">ollama-buddy--send</code></dt>
<dd><p>Send a prompt to Ollama.
</p>
</dd>
<dt><code class="code">ollama-buddy--make-request</code></dt>
<dd><p>Make a generic request to the Ollama API.
</p>
</dd>
<dt><code class="code">ollama-buddy--get-models</code></dt>
<dd><p>Get a list of available models.
</p>
</dd>
<dt><code class="code">ollama-buddy--get-valid-model</code></dt>
<dd><p>Get a valid model with fallback handling.
</p>
</dd>
<dt><code class="code">ollama-buddy--add-to-history</code></dt>
<dd><p>Add a message to the conversation history.
</p>
</dd>
<dt><code class="code">ollama-buddy--get-history-for-request</code></dt>
<dd><p>Get history for the current request.
</p>
</dd>
<dt><code class="code">ollama-buddy--prepare-prompt-area</code></dt>
<dd><p>Prepare the prompt area in the buffer.
</p>
</dd>
<dt><code class="code">ollama-buddy--update-status</code></dt>
<dd><p>Update the status display.
</p></dd>
</dl>

</div>
<div class="section-level-extent" id="Customization-Functions">
<h3 class="section"><span>21.3 Customization Functions<a class="copiable-link" href="#Customization-Functions"> &para;</a></span></h3>

<dl class="table">
<dt><code class="code">ollama-buddy-update-command-with-params</code></dt>
<dd><p>Update a command definition with new properties and parameters.
</p>
</dd>
<dt><code class="code">ollama-buddy-update-menu-entry</code></dt>
<dd><p>Update a menu entry&rsquo;s properties.
</p>
</dd>
<dt><code class="code">ollama-buddy-add-model-to-menu-entry</code></dt>
<dd><p>Associate a specific model with a menu entry.
</p>
</dd>
<dt><code class="code">ollama-buddy-add-parameters-to-command</code></dt>
<dd><p>Add specific parameters to a command definition.
</p></dd>
</dl>

<hr>
</div>
</div>
<div class="chapter-level-extent" id="FAQ">
<div class="nav-panel">
<p>
Next: <a href="#Troubleshooting" accesskey="n" rel="next">Troubleshooting</a>, Previous: <a href="#API-Reference" accesskey="p" rel="prev">API Reference</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Frequently-Asked-Questions"><span>22 Frequently Asked Questions<a class="copiable-link" href="#Frequently-Asked-Questions"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#General-Questions" accesskey="1">General Questions</a></li>
<li><a href="#Usage-Questions" accesskey="2">Usage Questions</a></li>
<li><a href="#Troubleshooting-2" accesskey="3">Troubleshooting</a></li>
</ul>
<div class="section-level-extent" id="General-Questions">
<h3 class="section"><span>22.1 General Questions<a class="copiable-link" href="#General-Questions"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f" accesskey="1">What is the difference between Ollama Buddy and other AI assistants?</a></li>
<li><a href="#Does-Ollama-Buddy-require-an-internet-connection_003f" accesskey="2">Does Ollama Buddy require an internet connection?</a></li>
<li><a href="#How-do-I-use-Ollama-cloud-models_003f" accesskey="3">How do I use Ollama cloud models?</a></li>
<li><a href="#Which-models-work-best-with-Ollama-Buddy_003f" accesskey="4">Which models work best with Ollama Buddy?</a></li>
<li><a href="#How-much-RAM-do-I-need_003f" accesskey="5">How much RAM do I need?</a></li>
</ul>
<div class="subsection-level-extent" id="What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f">
<h4 class="subsection"><span>22.1.1 What is the difference between Ollama Buddy and other AI assistants?<a class="copiable-link" href="#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f"> &para;</a></span></h4>
<p>Ollama Buddy integrates with Ollama to run LLMs locally, offering privacy, customization, and seamless Emacs integration without relying on external API services.
</p>
</div>
<div class="subsection-level-extent" id="Does-Ollama-Buddy-require-an-internet-connection_003f">
<h4 class="subsection"><span>22.1.2 Does Ollama Buddy require an internet connection?<a class="copiable-link" href="#Does-Ollama-Buddy-require-an-internet-connection_003f"> &para;</a></span></h4>
<p>For local Ollama models, no internet connection is required after pulling them. Internet is needed for:
</p><ul class="itemize mark-bullet">
<li>Pulling new models from Ollama
</li><li>Using Ollama cloud models
</li><li>Using remote providers (OpenAI, Claude, Gemini, Grok, Codestral)
</li><li>Syncing Fabric patterns or Awesome ChatGPT Prompts
</li></ul>

</div>
<div class="subsection-level-extent" id="How-do-I-use-Ollama-cloud-models_003f">
<h4 class="subsection"><span>22.1.3 How do I use Ollama cloud models?<a class="copiable-link" href="#How-do-I-use-Ollama-cloud-models_003f"> &para;</a></span></h4>
<p>Cloud models require authentication:
</p><ol class="enumerate">
<li> Run <code class="code">M-x ollama-buddy-cloud-signin</code> to sign in
</li><li> Select cloud models with <code class="code">C-u C-c m</code>
</li><li> The <code class="code">â</code> symbol in the header indicates cloud model usage
</li></ol>

</div>
<div class="subsection-level-extent" id="Which-models-work-best-with-Ollama-Buddy_003f">
<h4 class="subsection"><span>22.1.4 Which models work best with Ollama Buddy?<a class="copiable-link" href="#Which-models-work-best-with-Ollama-Buddy_003f"> &para;</a></span></h4>
<p>Most models supported by Ollama work well. Popular choices include:
</p><ul class="itemize mark-bullet">
<li>llama3:latest - Good general purpose assistant
</li><li>codellama:latest - Excellent for code-related tasks
</li><li>mistral:latest - Good balance of performance and quality
</li><li>phi:latest - Smaller model that works well on limited hardware
</li></ul>

</div>
<div class="subsection-level-extent" id="How-much-RAM-do-I-need_003f">
<h4 class="subsection"><span>22.1.5 How much RAM do I need?<a class="copiable-link" href="#How-much-RAM-do-I-need_003f"> &para;</a></span></h4>
<p>It depends on the model:
</p><ul class="itemize mark-bullet">
<li>Small models (7B) - 8GB minimum, 16GB recommended
</li><li>Medium models (13B) - 16GB minimum, 24GB+ recommended
</li><li>Large models (34B+) - 32GB+ recommended
</li></ul>

<p>Quantized models (e.g., Q4_K_M variants) require less RAM.
</p>
</div>
</div>
<div class="section-level-extent" id="Usage-Questions">
<h3 class="section"><span>22.2 Usage Questions<a class="copiable-link" href="#Usage-Questions"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f" accesskey="1">How do I cancel a request that&rsquo;s taking too long?</a></li>
<li><a href="#How-can-I-save-my-conversations_003f" accesskey="2">How can I save my conversations?</a></li>
<li><a href="#Can-I-use-multiple-models-in-the-same-conversation_003f" accesskey="3">Can I use multiple models in the same conversation?</a></li>
<li><a href="#How-do-I-clear-the-conversation-history_003f" accesskey="4">How do I clear the conversation history?</a></li>
<li><a href="#How-can-I-create-a-custom-command_003f" accesskey="5">How can I create a custom command?</a></li>
<li><a href="#How-can-I-manage-context-windows_003f" accesskey="6">How can I manage context windows?</a></li>
<li><a href="#What-happens-when-I-exceed-the-context-limit_003f" accesskey="7">What happens when I exceed the context limit?</a></li>
<li><a href="#How-do-I-create-effective-system-prompts_003f" accesskey="8">How do I create effective system prompts?</a></li>
<li><a href="#Where-are-my-system-prompts-stored_003f" accesskey="9">Where are my system prompts stored?</a></li>
</ul>
<div class="subsection-level-extent" id="How-do-I-cancel-a-request-that_0027s-taking-too-long_003f">
<h4 class="subsection"><span>22.2.1 How do I cancel a request that&rsquo;s taking too long?<a class="copiable-link" href="#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f"> &para;</a></span></h4>
<p>Press <code class="code">C-c k</code> in the chat buffer or select &quot;Kill Request&quot; from the menu.
</p>
</div>
<div class="subsection-level-extent" id="How-can-I-save-my-conversations_003f">
<h4 class="subsection"><span>22.2.2 How can I save my conversations?<a class="copiable-link" href="#How-can-I-save-my-conversations_003f"> &para;</a></span></h4>
<p>Use <code class="code">C-c S</code> to save the current session, giving it a name. You can restore it later with <code class="code">C-c L</code>.
</p>
</div>
<div class="subsection-level-extent" id="Can-I-use-multiple-models-in-the-same-conversation_003f">
<h4 class="subsection"><span>22.2.3 Can I use multiple models in the same conversation?<a class="copiable-link" href="#Can-I-use-multiple-models-in-the-same-conversation_003f"> &para;</a></span></h4>
<p>Yes, you can switch models at any time with <code class="code">C-c m</code>. Each model maintains its own conversation history.
</p>
</div>
<div class="subsection-level-extent" id="How-do-I-clear-the-conversation-history_003f">
<h4 class="subsection"><span>22.2.4 How do I clear the conversation history?<a class="copiable-link" href="#How-do-I-clear-the-conversation-history_003f"> &para;</a></span></h4>
<p>Press <code class="code">C-c X</code> to clear history, or <code class="code">C-c N</code> to start a completely new session.
</p>
</div>
<div class="subsection-level-extent" id="How-can-I-create-a-custom-command_003f">
<h4 class="subsection"><span>22.2.5 How can I create a custom command?<a class="copiable-link" href="#How-can-I-create-a-custom-command_003f"> &para;</a></span></h4>
<p>The easiest way is through the role creator: press <code class="code">C-c E</code> and follow the prompts to create commands with specific prompts, models, and parameters.
</p>
</div>
<div class="subsection-level-extent" id="How-can-I-manage-context-windows_003f">
<h4 class="subsection"><span>22.2.6 How can I manage context windows?<a class="copiable-link" href="#How-can-I-manage-context-windows_003f"> &para;</a></span></h4>
<p>Ollama Buddy provides several options:
</p><ul class="itemize mark-bullet">
<li>Enable context monitoring with <code class="code">(setq ollama-buddy-show-context-percentage t)</code>
</li><li>Use <kbd class="kbd">C-c C</kbd> to check current context usage
</li><li>Limit history length with <kbd class="kbd">C-c Y</kbd>
</li><li>Set model-specific context sizes with <kbd class="kbd">C-c $</kbd>
</li></ul>

</div>
<div class="subsection-level-extent" id="What-happens-when-I-exceed-the-context-limit_003f">
<h4 class="subsection"><span>22.2.7 What happens when I exceed the context limit?<a class="copiable-link" href="#What-happens-when-I-exceed-the-context-limit_003f"> &para;</a></span></h4>
<p>When context monitoring is enabled:
</p><ul class="itemize mark-bullet">
<li>You&rsquo;ll get a warning when approaching the limit (85-100%)
</li><li>You&rsquo;ll get an error dialog at or above 100%
</li><li>You can choose to proceed anyway or modify your content
</li></ul>

</div>
<div class="subsection-level-extent" id="How-do-I-create-effective-system-prompts_003f">
<h4 class="subsection"><span>22.2.8 How do I create effective system prompts?<a class="copiable-link" href="#How-do-I-create-effective-system-prompts_003f"> &para;</a></span></h4>
<p>Effective system prompts typically include:
</p><ul class="itemize mark-bullet">
<li>Clear role definition for the AI
</li><li>Specific guidelines for response format and style
</li><li>Examples of desired output (when applicable)
</li><li>Constraints or limitations to observe
</li></ul>
<p>Start simple, test the response, and refine iteratively.
</p>
</div>
<div class="subsection-level-extent" id="Where-are-my-system-prompts-stored_003f">
<h4 class="subsection"><span>22.2.9 Where are my system prompts stored?<a class="copiable-link" href="#Where-are-my-system-prompts-stored_003f"> &para;</a></span></h4>
<p>System prompts are stored as .org files in the directory specified by &lsquo;ollama-buddy-user-prompts-directory&lsquo;, which defaults to &lsquo;~/.emacs.d/ollama-buddy-user-prompts/&lsquo;.
</p>
</div>
</div>
<div class="section-level-extent" id="Troubleshooting-2">
<h3 class="section"><span>22.3 Troubleshooting<a class="copiable-link" href="#Troubleshooting-2"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Ollama-Buddy-shows-_0022OFFLINE_0022-status" accesskey="1">Ollama Buddy shows &quot;OFFLINE&quot; status</a></li>
<li><a href="#Responses-are-slow-or-the-model-seems-to-hang" accesskey="2">Responses are slow or the model seems to hang</a></li>
<li><a href="#Getting-_0022error-parsing-model_0022-when-pulling-a-model" accesskey="3">Getting &quot;error parsing model&quot; when pulling a model</a></li>
<li><a href="#Model-responses-are-low-quality-or-truncated" accesskey="4">Model responses are low quality or truncated</a></li>
<li><a href="#How-do-system-prompts-differ-from-regular-prompts_003f" accesskey="5">How do system prompts differ from regular prompts?</a></li>
<li><a href="#What-is-the-global-system-prompt_003f" accesskey="6">What is the global system prompt?</a></li>
<li><a href="#How-do-I-use-image-analysis_002fvision-features_003f" accesskey="7">How do I use image analysis/vision features?</a></li>
<li><a href="#Can-I-share-system-prompts-between-different-installations_003f" accesskey="8">Can I share system prompts between different installations?</a></li>
</ul>
<div class="subsection-level-extent" id="Ollama-Buddy-shows-_0022OFFLINE_0022-status">
<h4 class="subsection"><span>22.3.1 Ollama Buddy shows &quot;OFFLINE&quot; status<a class="copiable-link" href="#Ollama-Buddy-shows-_0022OFFLINE_0022-status"> &para;</a></span></h4>
<p>Ensure that:
</p><ul class="itemize mark-bullet">
<li>Ollama is installed and running
</li><li>The hostname and port are correctly configured (<code class="code">ollama-buddy-host</code> and <code class="code">ollama-buddy-port</code>)
</li><li>Your firewall isn&rsquo;t blocking connections
</li></ul>

</div>
<div class="subsection-level-extent" id="Responses-are-slow-or-the-model-seems-to-hang">
<h4 class="subsection"><span>22.3.2 Responses are slow or the model seems to hang<a class="copiable-link" href="#Responses-are-slow-or-the-model-seems-to-hang"> &para;</a></span></h4>
<p>Try:
</p><ul class="itemize mark-bullet">
<li>Using a smaller model
</li><li>Adjusting the <code class="code">num_ctx</code> parameter to a smaller value
</li><li>Setting <code class="code">low_vram</code> to <code class="code">t</code> if you have limited GPU memory
</li><li>Checking CPU/RAM usage to ensure your system isn&rsquo;t overloaded
</li></ul>

</div>
<div class="subsection-level-extent" id="Getting-_0022error-parsing-model_0022-when-pulling-a-model">
<h4 class="subsection"><span>22.3.3 Getting &quot;error parsing model&quot; when pulling a model<a class="copiable-link" href="#Getting-_0022error-parsing-model_0022-when-pulling-a-model"> &para;</a></span></h4>
<p>This usually means:
</p><ul class="itemize mark-bullet">
<li>The model name is incorrect
</li><li>The model is not available in the Ollama repository
</li><li>You have network connectivity issues
</li></ul>

</div>
<div class="subsection-level-extent" id="Model-responses-are-low-quality-or-truncated">
<h4 class="subsection"><span>22.3.4 Model responses are low quality or truncated<a class="copiable-link" href="#Model-responses-are-low-quality-or-truncated"> &para;</a></span></h4>
<p>Try:
</p><ul class="itemize mark-bullet">
<li>Increasing the <code class="code">temperature</code> parameter for more creative responses
</li><li>Increasing <code class="code">num_predict</code> for longer responses
</li><li>Using a more capable model
</li><li>Providing clearer instructions in your prompt
</li></ul>

</div>
<div class="subsection-level-extent" id="How-do-system-prompts-differ-from-regular-prompts_003f">
<h4 class="subsection"><span>22.3.5 How do system prompts differ from regular prompts?<a class="copiable-link" href="#How-do-system-prompts-differ-from-regular-prompts_003f"> &para;</a></span></h4>
<p>System prompts provide overall instructions to the AI about its role and how to respond, while regular prompts are your specific questions or requests. System prompts persist across the conversation, affecting all responses, while regular prompts are one-time interactions.
</p>
</div>
<div class="subsection-level-extent" id="What-is-the-global-system-prompt_003f">
<h4 class="subsection"><span>22.3.6 What is the global system prompt?<a class="copiable-link" href="#What-is-the-global-system-prompt_003f"> &para;</a></span></h4>
<p>The global system prompt is a baseline prompt automatically prepended to all requests. It&rsquo;s useful for consistent formatting instructions across all models. Toggle it with <code class="code">M-x ollama-buddy-toggle-global-system-prompt</code>.
</p>
</div>
<div class="subsection-level-extent" id="How-do-I-use-image-analysis_002fvision-features_003f">
<h4 class="subsection"><span>22.3.7 How do I use image analysis/vision features?<a class="copiable-link" href="#How-do-I-use-image-analysis_002fvision-features_003f"> &para;</a></span></h4>
<ol class="enumerate">
<li> Select a vision-capable model (e.g., gemma3:4b, llama3.2:3b)
</li><li> Include an image path in your prompt
</li><li> The image will be automatically detected and encoded
</li></ol>

<p>Supported formats: PNG, JPG, JPEG, WebP, GIF.
</p>
</div>
<div class="subsection-level-extent" id="Can-I-share-system-prompts-between-different-installations_003f">
<h4 class="subsection"><span>22.3.8 Can I share system prompts between different installations?<a class="copiable-link" href="#Can-I-share-system-prompts-between-different-installations_003f"> &para;</a></span></h4>
<p>Yes, the user system prompts are stored as plain text .org files in your &lsquo;ollama-buddy-user-prompts-directory&lsquo;. You can copy these files to share them with colleagues or between different machines.
</p>
<hr>
</div>
</div>
</div>
<div class="chapter-level-extent" id="Troubleshooting">
<div class="nav-panel">
<p>
Next: <a href="#Contributing" accesskey="n" rel="next">Contributing</a>, Previous: <a href="#FAQ" accesskey="p" rel="prev">Frequently Asked Questions</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Troubleshooting-3"><span>23 Troubleshooting<a class="copiable-link" href="#Troubleshooting-3"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Common-Issues" accesskey="1">Common Issues</a></li>
<li><a href="#Debugging" accesskey="2">Debugging</a></li>
</ul>
<div class="section-level-extent" id="Common-Issues">
<h3 class="section"><span>23.1 Common Issues<a class="copiable-link" href="#Common-Issues"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Connection-Problems" accesskey="1">Connection Problems</a></li>
<li><a href="#Model-Problems" accesskey="2">Model Problems</a></li>
<li><a href="#Interface-Issues" accesskey="3">Interface Issues</a></li>
</ul>
<div class="subsection-level-extent" id="Connection-Problems">
<h4 class="subsection"><span>23.1.1 Connection Problems<a class="copiable-link" href="#Connection-Problems"> &para;</a></span></h4>

<dl class="table">
<dt>Symptom: Unable to connect to Ollama server</dt>
<dd><ul class="itemize mark-bullet">
<li>Check if Ollama is running with <code class="code">ps aux | grep ollama</code>
</li><li>Verify host and port settings (<code class="code">ollama-buddy-host</code> and <code class="code">ollama-buddy-port</code>)
</li><li>Try connecting to Ollama directly: <code class="code">curl http://localhost:11434/api/tags</code>
</li></ul>

</dd>
<dt>Symptom: Connection breaks during long responses</dt>
<dd><ul class="itemize mark-bullet">
<li>This can happen with very large responses
</li><li>Try setting a lower <code class="code">num_predict</code> value
</li><li>Check if your OS has any network timeout settings
</li></ul>
</dd>
</dl>

</div>
<div class="subsection-level-extent" id="Model-Problems">
<h4 class="subsection"><span>23.1.2 Model Problems<a class="copiable-link" href="#Model-Problems"> &para;</a></span></h4>

<dl class="table">
<dt>Symptom: Model loads but gives poor responses</dt>
<dd><ul class="itemize mark-bullet">
<li>Try a different model
</li><li>Adjust parameters (increase temperature for more creativity)
</li><li>Provide clearer or more detailed prompts
</li><li>Check if the model is appropriate for your task
</li></ul>

</dd>
<dt>Symptom: Model fails to load or crashes</dt>
<dd><ul class="itemize mark-bullet">
<li>Check system memory usage
</li><li>Try a smaller quantized model
</li><li>Adjust <code class="code">num_ctx</code> to a smaller value
</li><li>Set <code class="code">low_vram</code> to <code class="code">t</code> if using GPU
</li></ul>
</dd>
</dl>

</div>
<div class="subsection-level-extent" id="Interface-Issues">
<h4 class="subsection"><span>23.1.3 Interface Issues<a class="copiable-link" href="#Interface-Issues"> &para;</a></span></h4>

<dl class="table">
<dt>Symptom: Chat buffer becomes unresponsive</dt>
<dd><ul class="itemize mark-bullet">
<li>Cancel any running requests with <code class="code">C-c k</code>
</li><li>Check if Emacs is using high CPU
</li><li>Try disabling token statistics display
</li><li>Close and reopen the chat buffer
</li></ul>

</dd>
<dt>Symptom: Markdown conversion issues</dt>
<dd><ul class="itemize mark-bullet">
<li>Toggle markdown conversion off with <code class="code">C-c C-o</code>
</li><li>Check if the response contains complex formatting
</li><li>Try editing the history to fix formatting issues
</li></ul>
</dd>
</dl>

</div>
</div>
<div class="section-level-extent" id="Debugging">
<h3 class="section"><span>23.2 Debugging<a class="copiable-link" href="#Debugging"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Enable-Debug-Mode" accesskey="1">Enable Debug Mode</a></li>
<li><a href="#Check-Logs" accesskey="2">Check Logs</a></li>
<li><a href="#Report-Issues" accesskey="3">Report Issues</a></li>
</ul>
<div class="subsection-level-extent" id="Enable-Debug-Mode">
<h4 class="subsection"><span>23.2.1 Enable Debug Mode<a class="copiable-link" href="#Enable-Debug-Mode"> &para;</a></span></h4>

<p>To get more information about what&rsquo;s happening:
</p><div class="example">
<pre class="example-preformatted">M-x ollama-buddy-toggle-debug-mode
</pre></div>

<p>This opens a debug buffer showing raw JSON communication with Ollama.
</p>
</div>
<div class="subsection-level-extent" id="Check-Logs">
<h4 class="subsection"><span>23.2.2 Check Logs<a class="copiable-link" href="#Check-Logs"> &para;</a></span></h4>

<p>Ollama logs can be useful for troubleshooting:
</p><div class="example">
<pre class="example-preformatted">tail -f ~/.ollama/logs/ollama.log
</pre></div>

</div>
<div class="subsection-level-extent" id="Report-Issues">
<h4 class="subsection"><span>23.2.3 Report Issues<a class="copiable-link" href="#Report-Issues"> &para;</a></span></h4>

<p>If you encounter a bug:
</p><ol class="enumerate">
<li> Enable debug mode
</li><li> Reproduce the issue
</li><li> Copy the debug output
</li><li> Report the issue on GitHub with:
  <ul class="itemize mark-bullet">
<li>Emacs version
  </li><li>Ollama version
  </li><li>Model used
  </li><li>Debug output
  </li><li>Steps to reproduce
  </li></ul>
</li></ol>

<hr>
</div>
</div>
</div>
<div class="chapter-level-extent" id="Contributing">
<div class="nav-panel">
<p>
Next: <a href="#Index" accesskey="n" rel="next">Index</a>, Previous: <a href="#Troubleshooting" accesskey="p" rel="prev">Troubleshooting</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="chapter" id="Contributing-1"><span>24 Contributing<a class="copiable-link" href="#Contributing-1"> &para;</a></span></h2>

<ul class="mini-toc">
<li><a href="#Getting-Started" accesskey="1">Getting Started</a></li>
<li><a href="#Development-Setup" accesskey="2">Development Setup</a></li>
<li><a href="#Coding-Guidelines" accesskey="3">Coding Guidelines</a></li>
<li><a href="#Testing" accesskey="4">Testing</a></li>
<li><a href="#Feature-Requests-and-Bug-Reports" accesskey="5">Feature Requests and Bug Reports</a></li>
</ul>
<div class="section-level-extent" id="Getting-Started">
<h3 class="section"><span>24.1 Getting Started<a class="copiable-link" href="#Getting-Started"> &para;</a></span></h3>

<p>Ollama Buddy is an open-source project, and contributions are welcome!
</p>
<ol class="enumerate">
<li> Fork the repository: <a class="url" href="https://github.com/captainflasmr/ollama-buddy">https://github.com/captainflasmr/ollama-buddy</a>
</li><li> Clone your fork: <code class="code">git clone https://github.com/YOUR-USERNAME/ollama-buddy.git</code>
</li><li> Create a branch: <code class="code">git checkout -b my-feature-branch</code>
</li><li> Make your changes
</li><li> Test thoroughly
</li><li> Commit with a clear message
</li><li> Push to your fork
</li><li> Create a pull request
</li></ol>

</div>
<div class="section-level-extent" id="Development-Setup">
<h3 class="section"><span>24.2 Development Setup<a class="copiable-link" href="#Development-Setup"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Required-Tools" accesskey="1">Required Tools</a></li>
<li><a href="#Recommended-Packages" accesskey="2">Recommended Packages</a></li>
</ul>
<div class="subsection-level-extent" id="Required-Tools">
<h4 class="subsection"><span>24.2.1 Required Tools<a class="copiable-link" href="#Required-Tools"> &para;</a></span></h4>

<ul class="itemize mark-bullet">
<li>Emacs 28.1+
</li><li>Ollama installed and running
</li><li>Git
</li></ul>

</div>
<div class="subsection-level-extent" id="Recommended-Packages">
<h4 class="subsection"><span>24.2.2 Recommended Packages<a class="copiable-link" href="#Recommended-Packages"> &para;</a></span></h4>

<ul class="itemize mark-bullet">
<li>package-lint
</li><li>flycheck
</li><li>elisp-lint
</li></ul>

</div>
</div>
<div class="section-level-extent" id="Coding-Guidelines">
<h3 class="section"><span>24.3 Coding Guidelines<a class="copiable-link" href="#Coding-Guidelines"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Follow Emacs Lisp conventions
</li><li>Use two spaces for indentation
</li><li>Add documentation strings to functions
</li><li>Keep line length under 80 characters
</li><li>Use prefix <code class="code">ollama-buddy--</code> for internal functions
</li><li>Use prefix <code class="code">ollama-buddy-</code> for public functions
</li></ul>

</div>
<div class="section-level-extent" id="Testing">
<h3 class="section"><span>24.4 Testing<a class="copiable-link" href="#Testing"> &para;</a></span></h3>

<ul class="mini-toc">
<li><a href="#Run-Existing-Tests" accesskey="1">Run Existing Tests</a></li>
<li><a href="#Adding-New-Tests" accesskey="2">Adding New Tests</a></li>
</ul>
<div class="subsection-level-extent" id="Run-Existing-Tests">
<h4 class="subsection"><span>24.4.1 Run Existing Tests<a class="copiable-link" href="#Run-Existing-Tests"> &para;</a></span></h4>

<p>The package includes comprehensive tests:
</p>
<div class="example">
<pre class="example-preformatted">M-x ollama-buddy-run-tests
M-x ollama-buddy-integration-run-tests
M-x ollama-buddy-fabric-run-tests
M-x ollama-buddy-parameter-run-tests
</pre></div>

</div>
<div class="subsection-level-extent" id="Adding-New-Tests">
<h4 class="subsection"><span>24.4.2 Adding New Tests<a class="copiable-link" href="#Adding-New-Tests"> &para;</a></span></h4>

<p>When adding features, please also add tests:
</p><ul class="itemize mark-bullet">
<li>Unit tests for individual functions
</li><li>Integration tests for API interactions
</li><li>Parameter tests for parameter handling
</li></ul>

</div>
</div>
<div class="section-level-extent" id="Feature-Requests-and-Bug-Reports">
<h3 class="section"><span>24.5 Feature Requests and Bug Reports<a class="copiable-link" href="#Feature-Requests-and-Bug-Reports"> &para;</a></span></h3>

<ul class="itemize mark-bullet">
<li>Use GitHub Issues for bug reports and feature requests
</li><li>Provide clear steps to reproduce bugs
</li><li>For feature requests, explain the use case
</li></ul>

<ul class="mini-toc">
<li><a href="#User-System-Prompts-Issues" accesskey="1">User System Prompts Issues</a></li>
</ul>
<div class="subsection-level-extent" id="User-System-Prompts-Issues">
<h4 class="subsection"><span>24.5.1 User System Prompts Issues<a class="copiable-link" href="#User-System-Prompts-Issues"> &para;</a></span></h4>

<dl class="table">
<dt>Symptom: Cannot save system prompts</dt>
<dd><ul class="itemize mark-bullet">
<li>Check if &lsquo;ollama-buddy-user-prompts-directory&lsquo; exists and is writable
</li><li>Ensure you have set a system prompt before trying to save it
</li><li>Check for any error messages in the minibuffer or *Messages* buffer
</li></ul>

</dd>
<dt>Symptom: Prompts not appearing in the load menu</dt>
<dd><ul class="itemize mark-bullet">
<li>Verify that prompts are saved in the correct format (category__title__system.org)
</li><li>Check if the prompts directory contains files with proper formatting
</li><li>Try refreshing the prompts cache with &lsquo;M-x ollama-buddy-user-prompts&ndash;refresh-cache&lsquo;
</li></ul>
</dd>
</dl>

<hr>
</div>
</div>
</div>
<div class="unnumbered-level-extent" id="Index">
<div class="nav-panel">
<p>
Previous: <a href="#Contributing" accesskey="p" rel="prev">Contributing</a>, Up: <a href="#Top" accesskey="u" rel="up">Ollama Buddy</a> &nbsp; [<a href="#SEC_Contents" title="Table of contents" rel="contents">Contents</a>][<a href="#Index" title="Index" rel="index">Index</a>]</p>
</div>
<h2 class="unnumbered" id="Index-1"><span>Index<a class="copiable-link" href="#Index-1"> &para;</a></span></h2>


</div>
</div>



</body>
</html>
