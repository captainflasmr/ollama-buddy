This is ollama-buddy.info, produced by makeinfo version 7.2 from
ollama-buddy.texi.

Copyright © 2024 James Dyer

     Permission is granted to copy, distribute and/or modify this
     document under the terms of the GNU Free Documentation License,
     Version 1.3 or any later version published by the Free Software
     Foundation; with no Invariant Sections, no Front-Cover Texts, and
     no Back-Cover Texts.
INFO-DIR-SECTION Emacs
START-INFO-DIR-ENTRY
* Ollama Buddy: (ollama-buddy). AI assistant integration with Ollama.
END-INFO-DIR-ENTRY


File: ollama-buddy.info,  Node: Top,  Next: Introduction,  Up: (dir)

Ollama Buddy
************

Ollama Buddy is a comprehensive Emacs package that provides seamless
integration with Ollama, allowing you to leverage powerful large
language models (LLMs) directly within your Emacs workflow.

* Menu:

* Introduction::                What is Ollama Buddy?
* Installation::                How to install the package
* Configuration::               Basic and advanced configuration
* Quick Start::                 Getting started with basic commands
* Core Features::               Explanation of main capabilities
* Chat Interface::              Using the chat UI
* Working with Models::         Managing and using different models
* Context Management::          Managing context
* File Attachments::            Including files in conversations
* Parameter Control::           Customizing AI behavior with parameters
* Session Management::          Saving and loading conversations
* User System Prompts::         Saving and managing system prompts
* Roles and Commands::          Creating custom commands and roles
* Fabric Pattern Integration::  Using predefined prompt patterns
* Awesome ChatGPT Prompts::     Using the Awesome ChatGPT Prompts collection
* ChatGPT and Claude Support::  Using commercial LLM APIs
* Advanced Usage::              Tips and techniques for power users
* API Reference::               Comprehensive function documentation
* FAQ::                         Frequently asked questions
* Troubleshooting::             Common problems and solutions
* Contributing::                How to contribute to Ollama Buddy
* Index::                       Complete index


File: ollama-buddy.info,  Node: Introduction,  Next: Installation,  Prev: Top,  Up: Top

1 Introduction
**************

1.1 What is Ollama Buddy?
=========================

Ollama Buddy is an Emacs package that provides a friendly AI assistant
interface to Ollama, a tool for running large language models (LLMs)
locally on your computer.  It allows you to interact with AI models
directly from within Emacs for various tasks such as:

   • Code refactoring and explanation
   • Writing assistance and proofreading
   • Generating Git commit messages
   • Dictionary lookups and language assistance
   • Custom AI-powered workflows via roles
   • Using pre-built prompt templates from Fabric
   • Utilizing Awesome ChatGPT Prompts
   • Integrating with Claude and OpenAI's commercial APIs
   • Including files as context in conversations

   Instead of context-switching to web interfaces or terminal
applications, Ollama Buddy brings the power of local LLMs right into
your Emacs workflow.

1.2 Why Use Ollama Buddy?
=========================

   • *Privacy*: All interactions happen locally with Ollama - no data
     sent to external services unless you use commercial APIs
   • *Integration*: Seamlessly fits into your existing Emacs workflow
   • *Flexibility*: Supports multiple models, parameter tuning, and
     custom commands
   • *Efficiency*: Quick access to AI assistance without leaving your
     editor
   • *Extensibility*: Create custom roles and commands for your specific
     needs
   • *File Support*: Include text files, code, and documentation
     directly in conversations

1.3 Prerequisites
=================

Before using Ollama Buddy, you need:

   • Emacs 28.1 or later
   • Ollama installed and running on your system (see
     <https://ollama.ai>)
   • At least one language model pulled into Ollama
   • (Optional) API keys for OpenAI or Claude if you want to use those
     services


File: ollama-buddy.info,  Node: Installation,  Next: Configuration,  Prev: Introduction,  Up: Top

2 Installation
**************

2.1 Installing Ollama
=====================

Before installing Ollama Buddy, you need to install Ollama itself:

  1. Visit <https://ollama.ai> and download the installer for your
     platform
  2. Install and run Ollama according to the instructions
  3. Pull at least one model using ‘ollama pull llama3:latest’ (or
     another model of your choice)

2.2 Package Installation
========================

2.2.1 Using package.el
----------------------

The recommended way to install Ollama Buddy is through MELPA:

     M-x package-install RET ollama-buddy RET

2.2.2 Using use-package
-----------------------

If you use ‘use-package’, add the following to your Emacs configuration:

     (use-package ollama-buddy
       :ensure t
       :bind ("C-c o" . ollama-buddy-menu))

   With a default model:

     (use-package ollama-buddy
       :ensure t
       :bind ("C-c o" . ollama-buddy-menu)
       :custom (ollama-buddy-default-model "llama3:latest"))

2.2.3 Manual Installation
-------------------------

To install manually:

  1. Clone the repository:
          git clone https://github.com/captainflasmr/ollama-buddy.git

  2. Add to your configuration:
          (add-to-list 'load-path "/path/to/ollama-buddy")
          (require 'ollama-buddy)
          (global-set-key (kbd "C-c o") #'ollama-buddy-menu)

2.3 Dependencies
================

Ollama Buddy requires the following Emacs packages:

   • transient
   • json
   • cl-lib

   These should be automatically installed if you use package.el or
use-package.

2.4 API Key Setup
=================

If you want to use OpenAI or Claude integration, you'll need to set up
API keys securely:

  1. Use Emacs built-in auth-source for secure storage
  2. Add to your auth sources (e.g., ~/.authinfo.gpg):
          machine api.openai.com login apikey password YOUR_OPENAI_API_KEY_HERE
          machine api.anthropic.com login apikey password YOUR_CLAUDE_API_KEY_HERE
  3. Alternatively, set the variables directly (less secure):
          (setq ollama-buddy-openai-api-key "your-openai-key")
          (setq ollama-buddy-claude-api-key "your-claude-key")


File: ollama-buddy.info,  Node: Configuration,  Next: Quick Start,  Prev: Installation,  Up: Top

3 Configuration
***************

3.1 Basic Configuration
=======================

Here are the essential configuration options:

‘ollama-buddy-default-model’
     Set your preferred default model.
          (setq ollama-buddy-default-model "llama3:latest")

‘ollama-buddy-host’
     Host where Ollama server is running (default: "localhost").
          (setq ollama-buddy-host "localhost")

‘ollama-buddy-port’
     Port where Ollama server is running (default: 11434).
          (setq ollama-buddy-port 11434)

3.2 Display Options
===================

Customize the appearance and behavior of Ollama Buddy:

‘ollama-buddy-convert-markdown-to-org’
     Whether to automatically convert markdown to org-mode format in
     responses (default: t).
          (setq ollama-buddy-convert-markdown-to-org t)

‘ollama-buddy-enable-model-colors’
     Whether to show model names with distinctive colors (default: t).
          (setq ollama-buddy-enable-model-colors t)

‘ollama-buddy-display-token-stats’
     Whether to display token usage statistics after responses (default:
     nil).
          (setq ollama-buddy-display-token-stats t)

‘ollama-buddy-interface-level’
     Level of interface complexity ('basic or 'advanced).
          (setq ollama-buddy-interface-level 'advanced)

3.3 File Attachment Configuration
=================================

Configure file attachment behavior:

‘ollama-buddy-max-file-size’
     Maximum size for attached files in bytes (default: 10MB).
          (setq ollama-buddy-max-file-size (* 10 1024 1024))  ; 10MB

‘ollama-buddy-supported-file-types’
     List of regex patterns for supported file types (default includes
     text, code, and configuration files).
          (setq ollama-buddy-supported-file-types
                '("\\.txt$" "\\.md$" "\\.org$" "\\.py$" "\\.js$" "\\.el$"))

3.4 Directory Configuration
===========================

Customize where Ollama Buddy stores its files:

‘ollama-buddy-sessions-directory’
     Directory for storing session files.
          (setq ollama-buddy-sessions-directory
                (expand-file-name "ollama-buddy-sessions" user-emacs-directory))

‘ollama-buddy-roles-directory’
     Directory for storing role preset files.
          (setq ollama-buddy-roles-directory
                (expand-file-name "ollama-buddy-presets" user-emacs-directory))

‘ollama-buddy-modelfile-directory’
     Directory for storing temporary Modelfiles.
          (setq ollama-buddy-modelfile-directory
                (expand-file-name "ollama-buddy-modelfiles" user-emacs-directory))

‘ollama-buddy-awesome-local-dir’
     Directory for storing Awesome ChatGPT Prompts.
          (setq ollama-buddy-awesome-local-dir
                (expand-file-name "awesome-chatgpt-prompts" user-emacs-directory))

3.5 History and Session Configuration
=====================================

Configure how conversation history is managed:

‘ollama-buddy-history-enabled’
     Whether to use conversation history in Ollama requests (default:
     t).
          (setq ollama-buddy-history-enabled t)

‘ollama-buddy-max-history-length’
     Maximum number of message pairs to keep in conversation history
     (default: 10).
          (setq ollama-buddy-max-history-length 10)

‘ollama-buddy-show-history-indicator’
     Whether to show the history indicator in the header line (default:
     t).
          (setq ollama-buddy-show-history-indicator t)

3.6 Context Management Configuration
====================================

Configure how Ollama Buddy handles context management:

‘ollama-buddy-show-context-percentage’
     Whether to show context percentage in the status bar (default:
     nil).
          (setq ollama-buddy-show-context-percentage t)

‘ollama-buddy-fallback-context-sizes’
     Mapping of model names to their default context sizes.
          (setq ollama-buddy-fallback-context-sizes
            '(("llama3:8b" . 4096)
              ("codellama:7b" . 8192)))

‘ollama-buddy-max-history-length’
     Maximum number of message pairs to keep (affects context usage).
          (setq ollama-buddy-max-history-length 10)

3.7 External API Configuration
==============================

For OpenAI and Claude integration:

‘ollama-buddy-openai-api-key’
     Your OpenAI API key.
          (setq ollama-buddy-openai-api-key "your-openai-key")

‘ollama-buddy-claude-api-key’
     Your Claude API key.
          (setq ollama-buddy-claude-api-key "your-claude-key")

‘ollama-buddy-openai-default-model’
     Default model for OpenAI requests.
          (setq ollama-buddy-openai-default-model "gpt-4")

‘ollama-buddy-claude-default-model’
     Default model for Claude requests.
          (setq ollama-buddy-claude-default-model "claude-3-opus-20240229")

3.8 Awesome ChatGPT Prompts Configuration
=========================================

Configure the Awesome ChatGPT Prompts integration:

‘ollama-buddy-awesome-repo-url’
     URL of the Awesome ChatGPT Prompts GitHub repository.
          (setq ollama-buddy-awesome-repo-url "https://github.com/f/awesome-chatgpt-prompts.git")

‘ollama-buddy-awesome-update-on-startup’
     Whether to automatically update prompts when Emacs starts.
          (setq ollama-buddy-awesome-update-on-startup nil)

‘ollama-buddy-awesome-categorize-prompts’
     Whether to categorize prompts based on common keywords.
          (setq ollama-buddy-awesome-categorize-prompts t)


File: ollama-buddy.info,  Node: Quick Start,  Next: Core Features,  Prev: Configuration,  Up: Top

4 Quick Start
*************

4.1 Basic Usage
===============

  1. Launch Ollama Buddy:
          M-x ollama-buddy-menu
     or use your configured keybinding (e.g., ‘C-c o’).

  2. The menu will show available options.  Press the corresponding key
     for the action you want.

  3. To open the chat interface, press ‘o’ or select "Open Chat".

  4. In the chat buffer, type your prompt and press ‘C-c C-c’ to send
     it.

  5. The AI will respond in the chat buffer.

4.2 Common Operations
=====================

Sending text from a file
     Select text in any buffer, then press ‘C-c o’ and choose "Send
     Region" (or press ‘l’).

Refactoring code
     Select code, press ‘C-c o’, then choose "Refactor Code" (or press
     ‘r’).

Generating a commit message
     Select your changes, press ‘C-c o’, then choose "Git Commit
     Message" (or press ‘g’).

Changing models
     Press ‘C-c o’ followed by ‘m’ to switch between available models.

Attaching files
     Press ‘C-c o’ followed by ‘1’ for the attachment menu, then ‘a’ to
     attach a file.

Toggling reasoning visibility
     Press ‘C-c V’ to hide or show reasoning/thinking sections in
     responses.

Using Awesome ChatGPT Prompts
     Select text, press ‘C-c o’, then ‘a’ for the Awesome prompts menu,
     then ‘s’ to send with a prompt.

Using Fabric patterns
     Select text, press ‘C-c o’, then ‘f’ for the Fabric menu, then ‘s’
     to send with a pattern.

Getting help
     In the chat buffer, press ‘C-c h’ to display the help screen with
     available commands and models.


File: ollama-buddy.info,  Node: Core Features,  Next: Chat Interface,  Prev: Quick Start,  Up: Top

5 Core Features
***************

5.1 Chat Interface
==================

The chat interface is the main way to interact with Ollama Buddy:

   • Persistent conversation with history
   • Markdown to Org-mode conversion
   • Model-specific colors
   • System prompt support
   • Parameter customization
   • Reasoning/thinking section visibility control
   • Context window management and monitoring
   • Real-time context usage display
   • Context size validation before sending prompts
   • Customizable context thresholds and warnings
   • File attachment support

5.2 Pre-built Commands
======================

Ollama Buddy comes with several pre-built commands:

Code Refactoring
     Improves code while maintaining functionality

Code Description
     Explains what code does and how it works

Git Commit Messages
     Generates meaningful commit messages from code changes

Dictionary Lookups
     Provides comprehensive word definitions

Synonym Finder
     Suggests alternative words with context

Proofreading
     Corrects grammar, style, and spelling

5.3 Model Management
====================

   • Switch between any model available in Ollama
   • Use ChatGPT and Claude models with API keys
   • Pull new models directly from the interface
   • View model information and statistics
   • Delete models you no longer need
   • Import GGUF files to create new models

5.4 Parameter Control
=====================

   • Fine-tune model behavior with customizable parameters
   • Save and use parameter profiles for different use cases
   • Command-specific parameter settings
   • Real-time parameter adjustment

5.5 Roles and Custom Commands
=============================

   • Create custom command sets for specific workflows
   • Design specialized AI assistants with custom system prompts
   • Save and switch between different roles
   • Share role configurations across your team

5.6 Prompt Template Collections
===============================

   • Use pre-built prompt patterns from Fabric project
   • Utilize the Awesome ChatGPT Prompts collection
   • Apply specialized prompts to your content with one command
   • Browse prompts by category

5.7 External API Integration
============================

   • Connect to OpenAI's ChatGPT API
   • Connect to Anthropic's Claude API
   • Seamlessly switch between local and cloud models
   • Secure API key management

5.8 File Attachments
====================

   • Attach text files, code, and documentation to conversations
   • Automatic context inclusion with proper token counting
   • Session persistence for attachments
   • Support for various file types
   • Dired integration for bulk file attachment


File: ollama-buddy.info,  Node: Chat Interface,  Next: Working with Models,  Prev: Core Features,  Up: Top

6 Chat Interface
****************

6.1 Opening the Chat
====================

To open the chat interface:

  1. Use ‘M-x ollama-buddy-menu’ or your configured keybinding
  2. Press ‘o’ to select "Open Chat"
  3. A new buffer will open with the Ollama Buddy chat interface

6.2 Interface Overview
======================

The chat interface consists of:

   • A welcome message with available models
   • Conversation history (previous prompts and responses)
   • A prompt area for entering your queries
   • A header line with status information
   • A status bar showing context usage (when enabled)
   • Context warnings and validation
   • Attachment indicators when files are attached

6.3 Sending Prompts
===================

To send a prompt to the AI:

  1. Type your message in the prompt area (after ">> PROMPT:")
  2. Press ‘C-c C-c’ to send
  3. Wait for the AI to generate a response

   You can also:
   • Use ‘M-p’ and ‘M-n’ to navigate through prompt history
   • Press ‘C-c k’ to cancel a request if it's taking too long

6.4 System Prompts
==================

System prompts allow you to define the AI's behavior:

Setting a system prompt
     Type your system prompt, then press ‘C-c s’

Viewing the current system prompt
     Press ‘C-c C-s’

Resetting the system prompt
     Press ‘C-c r’

Using a pre-built prompt
     Use Fabric patterns (‘C-c f p’) or Awesome ChatGPT prompts (‘C-c w
     p’)

   Example system prompt:
     You are a programming expert who specializes in Python.
     Provide concise, efficient solutions with explanations.

6.5 Markdown to Org Conversion
==============================

By default, Ollama Buddy converts markdown in responses to Org-mode
syntax:

   • Code blocks are converted to Org-mode source blocks
   • Headers are converted to Org-mode headings
   • Lists are properly formatted
   • Links are converted to Org-mode format

   To toggle this feature:
     M-x ollama-buddy-toggle-markdown-conversion
   or press ‘C-c C-o’ in the chat buffer.

6.6 Reasoning Visibility Control
================================

Ollama Buddy can hide reasoning/thinking sections in responses, making
the output cleaner:

   • Toggle visibility with ‘C-c V’ or ‘M-x
     ollama-buddy-toggle-reasoning-visibility’
   • Configure markers with the ‘ollama-buddy-reasoning-markers’
     variable
   • When hidden, a status message shows the current reasoning section
     (e.g., "Think...")
   • Header line indicates when reasoning is hidden with "REASONING
     HIDDEN" text

   This feature helps focus on final answers while preserving the option
to view the full reasoning process.


File: ollama-buddy.info,  Node: Working with Models,  Next: Context Management,  Prev: Chat Interface,  Up: Top

7 Working with Models
*********************

7.1 Available Models
====================

Ollama Buddy displays available models in the chat interface.  Each
model is assigned a letter for quick selection.

   To view detailed model information:
     M-x ollama-buddy-show-model-status
   or press ‘C-c v’ in the chat buffer.

7.2 Switching Models
====================

To change the current model:

  1. Press ‘C-c m’ in the chat buffer
  2. Select a model from the completion list
  3. The new model will be used for future requests

   You can also switch models from the main menu with ‘m’.

7.3 Local vs. Cloud Models
==========================

Ollama Buddy supports both local Ollama models and cloud-based models:

   • Local models (via Ollama): llama3, codellama, mistral, etc.
   • OpenAI models: gpt-3.5-turbo, gpt-4, etc.
   • Claude models: claude-3-opus, claude-3-sonnet, etc.

   To use cloud models, you need to configure API keys as described in
the Installation chapter.

7.4 Managing Models
===================

Ollama Buddy provides a comprehensive model management interface.  To
access it:
     M-x ollama-buddy-manage-models
   or press ‘C-c W’ in the chat buffer.

   From this interface, you can:
   • See which models are currently running
   • Pull new models from Ollama Hub
   • Delete models you no longer need
   • View detailed model information
   • Select models for use

7.5 Pulling New Models
======================

To pull a new model:

  1. Open the model management interface with ‘C-c W’
  2. Click "[Pull Any Model]" or press the appropriate key
  3. Enter the model name (e.g., "phi:latest", "codellama:7b")
  4. Wait for the model to download

7.6 Importing GGUF Files
========================

You can import custom GGUF model files:

  1. Press ‘C-c W’ to open the model management interface
  2. Click "[Import GGUF File]" or press the appropriate key
  3. Select the GGUF file from your file system
  4. Enter a name for the model
  5. Optionally provide model parameters
  6. Wait for Ollama to create the model

7.7 Multishot Mode
==================

Multishot mode allows you to send the same prompt to multiple models
simultaneously:

  1. Type your prompt in the chat buffer
  2. Press ‘C-c M’
  3. Enter the sequence of model letters you want to use (e.g., "a,b,c"
     to use models a, b, and c)
  4. Note that each item should be separated with a comma
  5. Watch as Ollama Buddy processes your request with each model in
     sequence


File: ollama-buddy.info,  Node: Context Management,  Next: File Attachments,  Prev: Working with Models,  Up: Top

8 Context Management
********************

8.1 Understanding Context Windows
=================================

Context windows define how much text (measured in tokens) a model can
process at once.  This includes your current prompt, conversation
history, any system prompts, and attached files.  Understanding and
managing context is crucial for:

   • Preventing errors when context limits are exceeded
   • Optimizing model performance for different tasks
   • Managing longer conversations efficiently
   • Including files without exceeding context limits

8.2 Context Size Detection
==========================

Ollama Buddy uses multiple methods to determine a model's context size:

  1. Built-in mappings for popular models (llama3, mistral, codellama,
     etc.)
  2. Custom context sizes set via the ‘num_ctx’ parameter
  3. Manual configuration through interactive commands
  4. Fallback to reasonable defaults (4096 tokens) for unknown models

8.3 Enabling Context Monitoring
===============================

Context monitoring is disabled by default.  To enable it:

     (setq ollama-buddy-show-context-percentage t)

   With context monitoring enabled:
   • The status bar shows current/max context usage (e.g., "2048/8192")
   • Text formatting indicates usage levels:
        − Normal font: Under 85% usage
        − Bold and underlined: 85-100% usage
        − Inverted: At or exceeding 100% usage
   • Warnings appear before sending prompts that exceed limits

8.4 Context with File Attachments
=================================

File attachments are included in context calculations:

   • Each attached file contributes to the total token count
   • The context breakdown shows attachment tokens separately
   • File content is included in the request context
   • Large files can significantly impact context usage

8.5 Context Management Commands
===============================

Show Context Information (‘C-c C’)
     Displays a breakdown of current context usage, including:
        • Conversation history token count
        • System prompt token count
        • Attachment token count
        • Current prompt token count
        • Total usage percentage

Set Model Context Size (‘C-c $’)
     Manually configure the context size for a specific model.

Toggle Context Display (‘C-c %’)
     Show or hide the context percentage in the status bar.

8.6 Token Estimation
====================

Ollama Buddy estimates token counts using a heuristic approach:
   • Each word is multiplied by 1.3 (following common approximations)
   • This provides a reasonable estimate for most use cases
   • Actual token counts may vary slightly between models

8.7 Managing Context in Practice
================================

8.7.1 Workflow Strategies
-------------------------

8.7.1.1 Paste-and-Send Approach
...............................

  1. Paste your content into the chat buffer
  2. Press the send keybinding
  3. If context is exceeded, you'll get a warning dialog
  4. Choose whether to proceed or modify your content

8.7.1.2 Preemptive Checking
...........................

  1. Paste your content
  2. Use ‘C-c C’ to check context usage
  3. If too high:
        • Trim your current prompt
        • Edit conversation history (‘C-c J’)
        • Switch to a larger context model
        • Adjust system prompt length
        • Remove or reduce file attachments

8.7.1.3 History Length Management
.................................

Control context by limiting conversation history:
     (setq ollama-buddy-max-history-length 5)

   This keeps only the last 5 message pairs, reducing context usage.

8.7.2 Using num_ctx Parameter
-----------------------------

The ‘num_ctx’ parameter allows you to set a specific context size:
  1. Access the parameter menu with ‘C-c P’
  2. Select ‘num_ctx’
  3. Enter your desired context size
  4. Ollama Buddy will respect this limit

8.8 Context Display Configuration
=================================

Customize how context information is displayed:

‘ollama-buddy-show-context-percentage’
     Whether to show context percentage in the status bar (default:
     nil).

‘ollama-buddy-context-warning-threshold’
     Percentage at which to warn about high context usage (default: 90).

‘ollama-buddy-context-error-threshold’
     Percentage at which to block sending (default: 100).

8.9 Fallback Context Sizes
==========================

Ollama Buddy includes predefined context sizes for popular models.  You
can customize these via:

     (setq ollama-buddy-fallback-context-sizes
       '(("llama3:8b" . 4096)
         ("codellama:7b" . 8192)
         ("mistral:7b" . 8192)))

8.10 Troubleshooting Context Issues
===================================

Context warnings appear unexpectedly
        • Check if you have a long system prompt
        • Review conversation history length
        • Verify the model's actual context size
        • Check if files are attached and their sizes

Model responses are truncated
        • Increase the ‘num_ctx’ parameter
        • Reduce history length with ‘C-c Y’
        • Clear some conversation history
        • Remove large file attachments

Context calculations seem inaccurate
        • Remember that token estimation is approximate
        • Different models may tokenize text differently
        • Use ‘C-c C’ to see detailed breakdowns


File: ollama-buddy.info,  Node: File Attachments,  Next: Parameter Control,  Prev: Context Management,  Up: Top

9 File Attachments
******************

9.1 Overview
============

File attachments allow you to include the contents of text files, code
files, documentation, and configuration files directly in your
conversations with AI models.  This feature is particularly useful for:

   • Code review and analysis
   • Documentation generation
   • Configuration file troubleshooting
   • Multi-file project discussions
   • Research with multiple text sources

9.2 Supported File Types
========================

Ollama Buddy supports a wide range of file types by default:

Text and Documentation
     ‘.txt’, ‘.md’, ‘.org’

Programming Languages
     ‘.py’, ‘.js’, ‘.el’, ‘.cpp’, ‘.c’, ‘.java’

Web Technologies
     ‘.html’, ‘.css’, ‘.json’, ‘.xml’

Configuration Files
     ‘.yaml’, ‘.yml’, ‘.toml’, ‘.ini’, ‘.cfg’

Scripts
     ‘.sh’, ‘.sql’

   You can customize supported file types by modifying
‘ollama-buddy-supported-file-types’.

9.3 Attaching Files
===================

9.3.1 Basic File Attachment
---------------------------

To attach a single file:

  1. Press ‘C-c 1’ to open the attachment menu
  2. Press ‘a’ for "Attach file"
  3. Select the file from the file browser
  4. The file will be attached and its contents included in future
     prompts

   Alternatively, you can use ‘C-c C-a’ directly.

9.3.2 Dired Integration
-----------------------

When working in Dired, you can attach files directly:

Attach file at point
     Position the cursor on a file and press ‘C-c C-a’

Attach multiple marked files
     Mark files with ‘m’, then run ‘M-x
     ollama-buddy-dired-attach-marked-files’

9.4 Managing Attachments
========================

9.4.1 Viewing Attachments
-------------------------

To see currently attached files:
     M-x ollama-buddy-show-attachments
   or press ‘C-c C-w’.

   This opens a dedicated buffer showing:
   • File names and paths
   • File sizes
   • File content preview

9.4.2 Detaching Files
---------------------

To remove a specific file:
     M-x ollama-buddy-detach-file
   or press ‘C-c C-d’.

   You'll be prompted to select which file to detach from the list of
currently attached files.

9.4.3 Clearing All Attachments
------------------------------

To remove all attached files at once:
     M-x ollama-buddy-clear-attachments
   or press ‘C-c 0’.

9.5 File Size Limits
====================

Ollama Buddy enforces file size limits to prevent overwhelming the
context window:

   • Default maximum file size: 10MB
   • Configurable via ‘ollama-buddy-max-file-size’
   • Files exceeding the limit will trigger an error

   Example configuration:
     ;; Set maximum file size to 5MB
     (setq ollama-buddy-max-file-size (* 5 1024 1024))

9.6 How Attachments Work
========================

9.6.1 Context Integration
-------------------------

When files are attached:

  1. File contents are read and stored in memory
  2. Content is included in the prompt context when sending requests
  3. Token counting includes attachment content
  4. Files are formatted with clear delimiters showing filename and type

9.6.2 Session Persistence
-------------------------

File attachments are preserved across sessions:

   • Saving a session (‘C-c S’) includes all attached files
   • Loading a session (‘C-c L’) restores attachments
   • Session files store both file paths and content
   • Attachment metadata is preserved (size, type, attachment time)

9.7 File Attachment Workflow Examples
=====================================

9.7.1 Code Review Workflow
--------------------------

  1. Attach source files using ‘C-c C-a’
  2. Set a system prompt for code review: "You are an expert code
     reviewer"
  3. Ask questions about the code: "What potential issues do you see in
     this code?"
  4. The AI can reference all attached files in its analysis

9.7.2 Multi-File Analysis
-------------------------

  1. Use Dired to mark multiple related files
  2. Attach them all with ‘M-x ollama-buddy-dired-attach-marked-files’
  3. Ask for analysis: "Compare the approaches used in these files"
  4. The AI can cross-reference content between files

9.7.3 Configuration Troubleshooting
-----------------------------------

  1. Attach configuration files (.yaml, .json, .ini)
  2. Describe the issue: "This configuration isn't working as expected"
  3. The AI can analyze the config and suggest fixes

9.8 Context Considerations
==========================

File attachments impact context usage:

   • Each attached file counts toward the total token limit
   • Large files can quickly fill available context
   • Monitor context usage with ‘C-c C’ when using attachments
   • Consider detaching unnecessary files to free up context

9.9 Best Practices
==================

  1. Start with smaller files to avoid context issues
  2. Use descriptive filenames for clarity
  3. Remove attachments when no longer needed
  4. Monitor context usage with large files
  5. Use attachment history to avoid re-attaching the same files

9.10 Troubleshooting Attachments
================================

File won't attach
        • Check if file type is supported (or override with "y")
        • Verify file size is under the limit
        • Ensure file exists and is readable

Context errors with attachments
        • Remove some attachments with ‘C-c C-d’
        • Switch to a model with larger context
        • Reduce conversation history length

Attachments not showing in session
        • Ensure you saved the session after attaching files
        • Check that the session file includes attachment data
        • Verify file paths are still valid when loading


File: ollama-buddy.info,  Node: Parameter Control,  Next: Session Management,  Prev: File Attachments,  Up: Top

10 Parameter Control
********************

10.1 Understanding Parameters
=============================

Ollama's models support various parameters that control their behavior:

temperature
     Controls randomness (0.0-1.0+), higher values produce more creative
     outputs

top_k
     Limits token selection to top K most probable tokens

top_p
     Nucleus sampling threshold (0.0-1.0)

repeat_penalty
     Penalty for repeating tokens (higher values reduce repetition)

10.2 Viewing Current Parameters
===============================

To view all current parameters:
     M-x ollama-buddy-params-display
   or press ‘C-c G’ in the chat buffer.

   Parameters that have been modified from default values are marked
with an asterisk (*).

10.3 Editing Parameters
=======================

To edit parameters:

  1. Press ‘C-c P’ to open the parameter menu
  2. Select the parameter you want to modify
  3. Enter the new value

   You can also use ‘M-x ollama-buddy-params-edit’ and select from a
completion list.

10.4 Parameter Profiles
=======================

Ollama Buddy comes with predefined parameter profiles for different use
cases:

Default
     Standard balanced settings

Creative
     Higher temperature, lower penalties for more creative responses

Precise
     Lower temperature, higher penalties for more deterministic
     responses

   To apply a profile:
     M-x ollama-buddy-transient-profile-menu
   or press ‘C-c p’ and select a profile.

10.5 Command-Specific Parameters
================================

Some commands have pre-configured parameters.  For example:
   • The "Refactor Code" command uses lower temperature for more
     deterministic results
   • The "Creative Writing" command uses higher temperature for more
     varied outputs

   These parameters are automatically applied when you use these
commands and restored afterward.

10.6 Reset Parameters
=====================

To reset all parameters to default values:
     M-x ollama-buddy-params-reset
   or press ‘C-c K’ in the chat buffer.

10.7 Displaying Parameters in Header
====================================

To toggle whether modified parameters are shown in the header:
     M-x ollama-buddy-toggle-params-in-header
   or press ‘C-c F’ in the chat buffer.


File: ollama-buddy.info,  Node: Session Management,  Next: User System Prompts,  Prev: Parameter Control,  Up: Top

11 Session Management
*********************

11.1 Understanding Sessions
===========================

Sessions in Ollama Buddy allow you to:
   • Save the entire conversation history
   • Save the current model selection
   • Restore previous conversations later
   • Switch between different conversation contexts

11.2 Creating a New Session
===========================

To start a fresh session:
     M-x ollama-buddy-sessions-new
   or press ‘C-c N’ in the chat buffer.

   This will clear the current conversation history and let you start
fresh.

11.3 Saving a Session
=====================

To save the current session:
     M-x ollama-buddy-sessions-save
   or press ‘C-c S’ in the chat buffer.

   You'll be prompted to enter a name for the session.

11.4 Loading a Session
======================

To load a previously saved session:
     M-x ollama-buddy-sessions-load
   or press ‘C-c L’ in the chat buffer.

   You'll be presented with a list of saved sessions to choose from.

11.5 Managing Sessions
======================

To see a list of all saved sessions:
     M-x ollama-buddy-sessions-list
   or press ‘C-c Q’ in the chat buffer.

   From this view, you can see:
   • Session names
   • Last modified times
   • Which models are used in each session

   To delete a session:
     M-x ollama-buddy-sessions-delete
   or press ‘C-c Z’ in the chat buffer.

11.6 Conversation History
=========================

Sessions save the conversation history for each model separately.

   To view the current conversation history:
     M-x ollama-buddy-history-edit
   or press ‘C-c J’ in the chat buffer.

   To clear the history:
     M-x ollama-buddy-clear-history
   or press ‘C-c X’ in the chat buffer.

   To toggle whether history is used in requests:
     M-x ollama-buddy-toggle-history
   or press ‘C-c H’ in the chat buffer.


File: ollama-buddy.info,  Node: User System Prompts,  Next: Roles and Commands,  Prev: Session Management,  Up: Top

12 User System Prompts
**********************

12.1 Overview
=============

The User System Prompts feature allows you to save, organize, and reuse
effective system prompts for your conversations with AI models.  This
feature is particularly valuable for:

   • Building a personal library of effective prompts
   • Maintaining context continuity across sessions
   • Sharing prompt templates with teammates
   • Refining your prompts over time
   • Categorizing prompts by domain or purpose

   System prompts play a crucial role in guiding AI behavior and
response quality.  A well-crafted system prompt can dramatically improve
the relevance, accuracy, and style of AI responses.

12.2 Accessing the System Prompts Menu
======================================

To access the system prompts menu:
     M-x ollama-buddy-transient-user-prompts-menu
   or press ‘C-c s’ in the chat buffer.

   This opens a transient menu with the following options:

Save current (S)
     Save your active system prompt for future reuse

Load prompt (L)
     Select a previously saved prompt to apply

Create new (N)
     Start fresh with a new prompt

List all Prompts (l)
     View your entire prompt collection

Edit prompt (e)
     Modify an existing prompt

Set with current prompt (s)
     Set the current text as a system prompt

Delete prompt (d)
     Remove prompts you no longer need

Reset prompt (r)
     Clear the system prompt setting

12.3 Saving System Prompts
==========================

To save a system prompt:

  1. Set a system prompt by typing it and pressing ‘C-c s s’
  2. Open the system prompts menu with ‘C-c s’
  3. Press ‘S’ to save the current system prompt
  4. Enter a category (from predefined options or create your own)
  5. Enter a descriptive title for your prompt
  6. The prompt will be saved to your prompts directory

12.4 Loading Saved Prompts
==========================

To load a previously saved prompt:

  1. Press ‘C-c s’ to open the system prompts menu
  2. Press ‘L’ to list available prompts
  3. Select a prompt from the completion interface
  4. The prompt will be loaded and set as your current system prompt

   Prompts are displayed in the format "‘category: title’" for easy
selection.

12.5 Managing Your Prompt Library
=================================

12.5.1 Viewing All Prompts
--------------------------

To view your entire prompt collection:
     M-x ollama-buddy-user-prompts-list
   or press ‘C-c s l’.

   This displays a buffer showing:
   • Prompts organized by category
   • Prompt titles
   • Preview of prompt content

12.5.2 Editing Prompts
----------------------

To edit an existing prompt:
     M-x ollama-buddy-user-prompts-edit
   or press ‘C-c s e’.

   This opens the prompt file in an Org mode buffer where you can make
changes and save.

12.5.3 Creating New Prompts
---------------------------

To create a new prompt from scratch:
     M-x ollama-buddy-user-prompts-create-new
   or press ‘C-c s N’.

   This opens a template with Org headers where you can enter your
prompt content.

12.5.4 Deleting Prompts
-----------------------

To delete a prompt:
     M-x ollama-buddy-user-prompts-delete
   or press ‘C-c s d’.

   You'll be asked to confirm before the prompt is deleted.

12.6 Categories and Organization
================================

Prompts are organized into categories for easier management.  Default
categories include:

   • general - General-purpose system prompts
   • coding - Programming-specific prompts
   • writing - Content creation and editing prompts
   • analysis - Data and research analysis prompts
   • creative - Prompts for creative tasks
   • technical - Technical documentation and explanation prompts
   • documentation - Documentation-focused prompts

   You can customize the default categories:
     (setq ollama-buddy-user-prompts-default-categories
           '("general" "coding" "writing" "analysis" "creative" "custom"))

12.7 Prompt Storage Format
==========================

System prompts are stored as Org mode files with a specific naming
convention:

     category__title__system.org

   Each file contains:
   • Org properties with metadata (title, category, date)
   • The full prompt content

   Example prompt file content:
     #+TITLE: Python Expert
     #+CATEGORY: coding
     #+DATE: 2025-05-19 14:32:45

     You are a Python programming expert with deep knowledge of both modern and
     legacy Python code. When analyzing or writing code:

     1. Prioritize readability and maintainability over clever tricks
     2. Follow PEP 8 conventions
     3. Include docstrings and comments for non-obvious operations
     4. Explain your thinking step-by-step
     5. Provide examples when helpful

     When asked to debug, first identify the likely cause before suggesting fixes.

12.8 Best Practices for System Prompts
======================================

12.8.1 Components of Effective Prompts
--------------------------------------

Well-designed system prompts typically include:

   • Clear role definition (who/what the AI is supposed to be)
   • Guidelines for response style and format
   • Constraints or limitations to observe
   • Specific instructions for handling certain types of queries
   • Examples of desired responses (optional)

12.8.2 Example Patterns
-----------------------

Expert Role
     "You are a [domain] expert with [X years] of experience in
     [specific areas]..."

Response Format
     "Format your responses with a brief summary first, followed by
     detailed analysis..."

Specific Guidelines
     "When responding to code queries, always include sample code and
     explain line-by-line..."

Thinking Process
     "Think step-by-step, breaking down complex problems into smaller
     components..."

12.9 Example System Prompts
===========================

12.9.1 Technical Writing Assistant
----------------------------------

     You are a technical writing expert who specializes in creating clear, concise,
     and accessible documentation. Your writing should:

     1. Use plain language and avoid jargon where possible
     2. Include appropriate headings and structural elements
     3. Provide concrete examples that illustrate complex concepts
     4. Use active voice and direct instructions for procedures
     5. Anticipate common user questions and address them proactively

     When presented with technical content, focus on making it understandable to
     the target audience while preserving technical accuracy.

12.9.2 Code Reviewer
--------------------

     You are an experienced code reviewer with expertise in software engineering
     best practices. When reviewing code:

     1. Identify potential bugs, edge cases, and performance issues
     2. Suggest improvements to readability and maintainability
     3. Highlight security vulnerabilities or potential risks
     4. Reference design patterns or library functions that could improve the implementation
     5. Provide specific, actionable feedback with examples

     Balance constructive criticism with acknowledgment of well-written code.

12.10 Workflow Examples
=======================

12.10.1 Python Code Assistance
------------------------------

  1. Load your "Python Expert" system prompt with ‘C-c s L’
  2. Ask coding questions or paste code for analysis
  3. The AI responds with Python-specific expertise
  4. Save the conversation as a session for future reference

12.10.2 Technical Writing Help
------------------------------

  1. Create a new system prompt for technical writing (‘C-c s N’)
  2. Define the AI's role as a technical writing assistant
  3. Save the prompt in the "writing" category
  4. Load this prompt whenever you need help with documentation
  5. The AI consistently provides responses optimized for technical
     writing

12.11 Integration with Roles
============================

System prompts can be integrated with Ollama Buddy roles for more
specialized workflows:

  1. Create a system prompt for a specific purpose
  2. Test and refine it through direct interaction
  3. Once effective, save it to your prompt library
  4. Reference this prompt in a custom role definition

   This creates a reusable AI assistant configuration that can be shared
and improved over time.


File: ollama-buddy.info,  Node: Roles and Commands,  Next: Fabric Pattern Integration,  Prev: User System Prompts,  Up: Top

13 Roles and Commands
*********************

13.1 Understanding Roles
========================

Roles in Ollama Buddy are collections of commands with specific
configurations:
   • Each role has its own set of commands
   • Commands can use specific models
   • Commands can have specialized system prompts
   • Commands can have specialized parameters

   This allows you to create specialized assistants for different
workflows.

13.2 Role File Naming Convention
================================

The file naming convention is critical to understand how roles, preset
files, and menu configurations work together:

Required filename format
     ‘ollama-buddy--preset__ROLE-NAME.el’
        • The double underscore ‘__’ separates the prefix from your role
          name
        • The role name portion becomes the identifier shown when
          switching roles
        • Example: ‘ollama-buddy--preset__programmer.el’ creates a role
          named "programmer"

   This naming convention is how Ollama Buddy discovers and identifies
role files.  When you run ‘ollama-buddy-roles-switch-role’, the system:

  1. Scans the ‘ollama-buddy-roles-directory’ for files matching the
     pattern
  2. Extracts the role name from each filename (the part after ‘__’)
  3. Presents these names in the role selection interface
  4. When selected, loads the corresponding file which redefines
     ‘ollama-buddy-command-definitions’
  5. This redefinition immediately changes the available commands in
     your Ollama Buddy menu

   The relationship chain works like this:
     ollama-buddy--preset__ROLE-NAME.el → Defines ollama-buddy-command-definitions → Controls menu content

   When creating roles using the interactive role creator (‘C-c E’),
this naming convention is automatically handled for you.  When creating
roles manually, you must follow this pattern for Ollama Buddy to
recognize your role files correctly.

13.3 Built-in Commands
======================

Ollama Buddy comes with several built-in commands:

refactor-code
     Improves code while maintaining functionality

describe-code
     Explains what code does and how it works

git-commit
     Generates meaningful commit messages

dictionary-lookup
     Provides comprehensive word definitions

synonym
     Suggests alternative words with context

proofread
     Corrects grammar, style, and spelling

13.4 Creating Custom Roles
==========================

There are two ways to create custom roles:

13.4.1 Interactive Role Creator
-------------------------------

The most user-friendly approach:

  1. Press ‘C-c E’ or run ‘M-x
     ollama-buddy-role-creator-create-new-role’
  2. Enter a name for your role (e.g., "programmer")
  3. For each command you want to add:
        • Specify a command name (e.g., "refactor-code")
        • Choose a key shortcut for the menu
        • Add a description
        • Optionally specify a model
        • Optionally add prompt prefixes and system messages

   The interactive creator automatically handles file naming and
placement.

13.4.2 Manual Role Creation
---------------------------

For more advanced customization, create role files manually:

  1. Create a file named ‘ollama-buddy--preset__your-role-name.el’ in
     your ‘ollama-buddy-roles-directory’
  2. Structure your file like this:

     ;; ollama-buddy preset for role: programmer
     (require 'ollama-buddy)

     (setq ollama-buddy-command-definitions
       '(
         ;; Standard commands - always include these
         (open-chat
          :key ?o
          :description "Open chat buffer"
          :action ollama-buddy--open-chat)

         (show-models
          :key ?v
          :description "View model status"
          :action ollama-buddy-show-model-status)

         (switch-role
          :key ?R
          :description "Switch roles"
          :action ollama-buddy-roles-switch-role)

         (create-role
          :key ?E
          :description "Create new role"
          :action ollama-buddy-role-creator-create-new-role)

         (open-roles-directory
          :key ?D
          :description "Open roles directory"
          :action ollama-buddy-roles-open-directory)

         ;; Custom commands for this role
         (refactor-code
          :key ?r
          :description "Refactor code"
          :model "codellama:7b"
          :prompt "Refactor this code to improve readability and efficiency:"
          :system "You are an expert software engineer who improves code quality."
          :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))

         (explain-code
          :key ?e
          :description "Explain code"
          :model "deepseek-r1:7b"
          :prompt "Explain what this code does in detail:"
          :system "You are a programming teacher who explains code clearly."
          :action (lambda () (ollama-buddy--send-with-command 'explain-code)))

         (git-commit
          :key ?g
          :description "Git commit message"
          :prompt "Write a concise git commit message for these changes:"
          :system "You are a version control expert who creates clear commit messages."
          :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
         ))

13.5 Switching Roles
====================

To switch between roles:
     M-x ollama-buddy-roles-switch-role
   or press ‘C-c R’ in the chat buffer.

   You'll be presented with a list of available roles to choose from.

13.6 Managing Role Files
========================

Roles are stored as Elisp files in the ‘ollama-buddy-roles-directory’.

   To locate your roles directory:
     ;; Check where your roles are stored
     (message ollama-buddy-roles-directory)

   By default, this is set to ‘~/.emacs.d/ollama-buddy-presets/’, but
you can customize it:
     (setq ollama-buddy-roles-directory "/your/custom/path/to/presets")

   To open this directory:
     M-x ollama-buddy-roles-open-directory
   or press ‘C-c D’ in the chat buffer.

13.7 Advanced Role Customization
================================

13.7.1 Command-Specific Models
------------------------------

Assign specific models to commands for optimal performance:

     (ollama-buddy-add-model-to-menu-entry 'refactor-code "codellama:7b")

13.7.2 Command-Specific Parameters
----------------------------------

Optimize parameters for specific commands:

     (ollama-buddy-add-parameters-to-command 'refactor-code
       'temperature 0.2
       'top_p 0.7
       'repeat_penalty 1.3)

13.7.3 Creating New Commands
----------------------------

Add entirely new commands to your menu:

     (ollama-buddy-update-menu-entry 'my-new-command
       :key ?z
       :description "My new awesome command"
       :prompt "Here is what I want you to do:"
       :system "You are an expert system specialized in this task."
       :action (lambda () (ollama-buddy--send-with-command 'my-new-command)))

13.8 Role Examples
==================

13.8.1 Programming Role
-----------------------

A complete example of a programming-focused role:

     ;; ollama-buddy preset for role: programmer
     (require 'ollama-buddy)

     (setq ollama-buddy-command-definitions
       '(
         ;; Standard commands (abbreviated for clarity)
         (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
         (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
         (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)

         ;; Programming-specific commands
         (refactor-code
          :key ?r
          :description "Refactor code"
          :model "codellama:7b"
          :prompt "Refactor this code to improve readability and efficiency:"
          :system "You are an expert software engineer who improves code quality."
          :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))

         (explain-code
          :key ?e
          :description "Explain code"
          :model "deepseek-r1:7b"
          :prompt "Explain what this code does in detail:"
          :system "You are a programming teacher who explains code clearly."
          :action (lambda () (ollama-buddy--send-with-command 'explain-code)))

         (add-tests
          :key ?t
          :description "Generate tests"
          :model "qwen2.5-coder:7b"
          :prompt "Generate unit tests for this code:"
          :system "You are a test automation expert who creates comprehensive test cases."
          :action (lambda () (ollama-buddy--send-with-command 'add-tests)))

         (git-commit
          :key ?g
          :description "Git commit message"
          :prompt "Write a concise git commit message for these changes:"
          :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
         ))

13.8.2 Writing Role
-------------------

A complete example of a writing-focused role:

     ;; ollama-buddy preset for role: writer
     (require 'ollama-buddy)

     (setq ollama-buddy-command-definitions
       '(
         ;; Standard commands (abbreviated for clarity)
         (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
         (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
         (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)

         ;; Writing-focused commands
         (summarize
          :key ?s
          :description "Summarize text"
          :prompt "Summarize the following text in a concise manner:"
          :system "You are an expert at extracting the key points from any text."
          :action (lambda () (ollama-buddy--send-with-command 'summarize)))

         (proofread
          :key ?p
          :description "Proofread text"
          :model "deepseek-r1:7b"
          :prompt "Proofread the following text and correct any errors:"
          :system "You are a professional editor who identifies and corrects grammar and style errors."
          :action (lambda () (ollama-buddy--send-with-command 'proofread)))

         (rewrite
          :key ?r
          :description "Rewrite text"
          :prompt "Rewrite the following text to improve clarity and flow:"
          :system "You are a skilled writer who can improve any text while preserving its meaning."
          :action (lambda () (ollama-buddy--send-with-command 'rewrite)))

         (brainstorm
          :key ?b
          :description "Brainstorm ideas"
          :model "llama3.2:3b"
          :prompt "Generate creative ideas related to the following topic:"
          :parameters ((temperature . 1.0) (top_p . 0.95))
          :action (lambda () (ollama-buddy--send-with-command 'brainstorm)))
         ))

13.9 Tips for Effective Role Usage
==================================

  1. Group related commands: Create roles around specific workflows or
     tasks
  2. Match models to tasks: Use lightweight models for simple tasks and
     more powerful models for complex ones
  3. Customize system prompts: Craft specific system prompts to guide
     the model for each command
  4. Use the roles directory: Press ‘C-c D’ to quickly access and manage
     your role files
  5. Create specialized roles: Consider roles for programming, writing,
     translation, or domain-specific knowledge


File: ollama-buddy.info,  Node: Fabric Pattern Integration,  Next: Awesome ChatGPT Prompts,  Prev: Roles and Commands,  Up: Top

14 Fabric Pattern Integration
*****************************

14.1 What are Fabric Patterns?
==============================

Fabric patterns are pre-defined prompt templates from Daniel Miessler's
Fabric project (<https://github.com/danielmiessler/fabric>).  They
provide optimized prompts for various tasks, categorized as:

   • universal - General-purpose patterns
   • code - Programming and development
   • writing - Content creation and editing
   • analysis - Data and concept examination

14.2 Setting Up Fabric Integration
==================================

To set up Fabric integration:
     M-x ollama-buddy-fabric-setup

   This will:
  1. Clone the Fabric repository (or set up sparse checkout)
  2. Populate available patterns
  3. Make patterns available for use

14.3 Using Fabric Patterns
==========================

To use a Fabric pattern:
     M-x ollama-buddy-fabric-send
   or press ‘C-c f’ and then ‘s’.

   You'll be prompted to:
  1. Select a pattern
  2. Enter text to process (or use selected text)

   The pattern will be used as a system prompt for your request.

14.4 Browsing Available Patterns
================================

To see all available patterns:
     M-x ollama-buddy-fabric-list-patterns
   or press ‘C-c f’ and then ‘l’.

   This shows:
   • Pattern names
   • Categories
   • Descriptions

14.5 Viewing Pattern Details
============================

To see the full content of a specific pattern:
     M-x ollama-buddy-fabric-show-pattern
   or press ‘C-c f’ and then ‘v’.

   Select a pattern to see:
   • The system prompt content
   • Full description

14.6 Updating Patterns
======================

To sync with the latest patterns from GitHub:
     M-x ollama-buddy-fabric-sync-patterns
   or press ‘C-c f’ and then ‘S’.

14.7 Using Patterns by Category
===============================

You can quickly access patterns by category:
   • ‘C-c f u’ - Universal patterns
   • ‘C-c f c’ - Code patterns
   • ‘C-c f w’ - Writing patterns
   • ‘C-c f a’ - Analysis patterns


File: ollama-buddy.info,  Node: Awesome ChatGPT Prompts,  Next: ChatGPT and Claude Support,  Prev: Fabric Pattern Integration,  Up: Top

15 Awesome ChatGPT Prompts
**************************

15.1 What is Awesome ChatGPT Prompts?
=====================================

Awesome ChatGPT Prompts is a curated collection of prompt templates
created by the community and maintained in the GitHub repository at
<https://github.com/f/awesome-chatgpt-prompts>.  These prompts are
designed to make ChatGPT (and other LLMs) act as various specialized
personas or experts, such as:

   • Writing professionals (poets, storytellers, copywriters)
   • Technical experts (programmers, researchers, scientists)
   • Creative professionals (artists, designers, photographers)
   • Business experts (marketers, consultants, strategists)
   • And many more specialized roles

15.2 Setting Up Awesome ChatGPT Prompts
=======================================

To set up the Awesome ChatGPT Prompts integration:
     M-x ollama-buddy-awesome-setup

   This will:
  1. Create a sparse checkout of the Awesome ChatGPT Prompts repository
  2. Download only the necessary files (prompts.csv and README)
  3. Populate and categorize the available prompts

15.3 Using Awesome ChatGPT Prompts
==================================

To use an Awesome ChatGPT Prompt:
     M-x ollama-buddy-awesome-send
   or press ‘C-c w’ and then ‘s’.

   You'll be prompted to:
  1. Select a prompt from the categorized list
  2. Enter text to process (or use selected text)

   The selected prompt will be used as a system prompt for your request,
transforming how the AI responds to your text.

15.4 Browsing Available Prompts
===============================

To see all available prompts:
     M-x ollama-buddy-awesome-list-prompts
   or press ‘C-c w’ and then ‘l’.

   This shows:
   • Prompt titles
   • Categories
   • Preview of prompt content

15.5 Categorized Browsing
=========================

Ollama Buddy automatically categorizes the Awesome ChatGPT Prompts into
useful groups:
   • writing - For writing, poetry, and creative content
   • code - For programming and development
   • business - For marketing, entrepreneurship, and business strategy
   • academic - For educational and research content
   • creative - For artistic and design-related prompts
   • philosophy - For philosophical reasoning and ethics
   • health - For medical, fitness, and wellness
   • legal - For law-related prompts
   • finance - For financial advice and analysis
   • other - Miscellaneous prompts

   To browse by category:
     M-x ollama-buddy-awesome-show-prompts-menu
   or press ‘C-c w’ and then ‘c’.

15.6 Viewing Prompt Details
===========================

To see the full content of a specific prompt:
     M-x ollama-buddy-awesome-show-prompt
   or press ‘C-c w’ and then ‘v’.

   Select a prompt to see its complete template.

15.7 Updating Prompts
=====================

To sync with the latest prompts from GitHub:
     M-x ollama-buddy-awesome-sync-prompts
   or press ‘C-c w’ and then ‘S’.

15.8 Setting Without Sending
============================

To set a prompt as the system prompt without sending text:
     M-x ollama-buddy-awesome-set-system-prompt
   or press ‘C-c w’ and then ‘p’.

   This is useful when you want to set up a specific persona before
starting a conversation.

15.9 Example Usage
==================

Some popular prompts include:
   • "Act as a poet" - Transforms your text into poetry
   • "Act as a Linux terminal" - Simulates a Linux terminal interface
   • "Act as a gaslighter" - Responds in a deliberately confusing manner
   • "Act as a javascript console" - Simulates a JavaScript console
   • "Act as an English translator" - Translates text to proper English


File: ollama-buddy.info,  Node: ChatGPT and Claude Support,  Next: Advanced Usage,  Prev: Awesome ChatGPT Prompts,  Up: Top

16 ChatGPT and Claude Support
*****************************

16.1 Overview
=============

Ollama Buddy integrates with commercial AI services:
   • OpenAI's ChatGPT API
   • Anthropic's Claude API

   This allows you to:
   • Use the latest commercial models when needed
   • Compare responses between local and cloud models
   • Leverage the strengths of different model families

16.2 Setting Up API Access
==========================

Before using commercial APIs, you need to set up API keys:

16.2.1 Secure API Key Storage
-----------------------------

The recommended approach is to use Emacs' built-in auth-source:
     ;; Add to ~/.authinfo.gpg (encrypted)
     machine api.openai.com login apikey password YOUR_OPENAI_API_KEY
     machine api.anthropic.com login apikey password YOUR_CLAUDE_API_KEY

16.2.2 Direct Configuration
---------------------------

For testing or temporary use (less secure):
     (setq ollama-buddy-openai-api-key "your-openai-key")
     (setq ollama-buddy-claude-api-key "your-claude-key")

16.3 Selecting Commercial Models
================================

Both OpenAI and Claude models appear in the model selection list with
special prefixes:
   • OpenAI models are prefixed with "openai:"
   • Claude models are prefixed with "claude:"

   To select a commercial model:
     M-x ollama-buddy--swap-model
   or press ‘C-c m’.

   Choose the model from the completion list.

16.4 Configuring Commercial Models
==================================

16.4.1 OpenAI Configuration
---------------------------

‘ollama-buddy-openai-default-model’
     Default OpenAI model to use (e.g., "gpt-4").
          (setq ollama-buddy-openai-default-model "gpt-4")

‘ollama-buddy-openai-temperature’
     Default temperature for OpenAI requests (0.0-2.0).
          (setq ollama-buddy-openai-temperature 0.7)

‘ollama-buddy-openai-max-tokens’
     Maximum tokens to generate (nil for API default).
          (setq ollama-buddy-openai-max-tokens 2000)

‘ollama-buddy-openai-api-endpoint’
     Custom API endpoint (defaults to OpenAI's standard endpoint).
          (setq ollama-buddy-openai-api-endpoint "https://api.openai.com/v1/chat/completions")

16.4.2 Claude Configuration
---------------------------

‘ollama-buddy-claude-default-model’
     Default Claude model to use.
          (setq ollama-buddy-claude-default-model "claude-3-opus-20240229")

‘ollama-buddy-claude-temperature’
     Default temperature for Claude requests (0.0-1.0).
          (setq ollama-buddy-claude-temperature 0.7)

‘ollama-buddy-claude-max-tokens’
     Maximum tokens to generate (nil for API default).
          (setq ollama-buddy-claude-max-tokens 2000)

16.5 History Management
=======================

Each API service maintains its own conversation history:
   • Ollama history for local models
   • OpenAI history for ChatGPT models
   • Claude history for Claude models

   This ensures that context is maintained appropriately for each
service.

16.6 Improved Error Handling
============================

As of version 0.9.20, Ollama Buddy includes enhanced error handling for
ChatGPT and Claude:
   • Better Unicode character handling in JSON requests
   • More robust error recovery
   • Clearer error messages
   • Consistent handling of API responses


File: ollama-buddy.info,  Node: Advanced Usage,  Next: API Reference,  Prev: ChatGPT and Claude Support,  Up: Top

17 Advanced Usage
*****************

17.1 Managing Token Usage
=========================

Ollama Buddy can track token usage statistics:

   To toggle token statistics display after responses:
     M-x ollama-buddy-toggle-token-display
   or press ‘C-c T’ in the chat buffer.

   To view detailed token usage statistics:
     M-x ollama-buddy-display-token-stats
   or press ‘C-c u’ in the chat buffer.

   To display a visual graph of token usage:
     M-x ollama-buddy-display-token-graph
   or press ‘C-c U’ in the chat buffer.

17.2 Customizing the Interface
==============================

17.2.1 Interface Level
----------------------

Ollama Buddy has two interface levels:
   • basic - Simplified for beginners
   • advanced - Full feature set for power users

   To toggle between them:
     M-x ollama-buddy-toggle-interface-level
   or press ‘C-c A’ in the chat buffer.

17.2.2 Model Colors
-------------------

Each model has a distinctive color to help identify responses.

   To toggle model colors:
     M-x ollama-buddy-toggle-model-colors
   or press ‘C-c c’ in the chat buffer.

17.2.3 Debug Mode
-----------------

For advanced troubleshooting, you can enable debug mode:
     M-x ollama-buddy-toggle-debug-mode
   or press ‘C-c B’ in the chat buffer.

   This shows raw JSON messages in a debug buffer.

17.3 Editing Conversation History
=================================

To manually edit conversation history:
     M-x ollama-buddy-history-edit
   or press ‘C-c J’ in the chat buffer.

   This opens an editable buffer with the conversation history.  You can
modify it and press ‘C-c C-c’ to save or ‘C-c C-k’ to cancel.

   To edit history for a specific model, use ‘C-u C-c J’.

17.4 Advanced System Prompt Management
======================================

For more control over system prompts:

17.4.1 Setting a system prompt without sending
----------------------------------------------

     (ollama-buddy-set-system-prompt)
   Enter your system prompt, then press ‘C-c s’.

17.4.2 Using a system prompt from Fabric
----------------------------------------

     M-x ollama-buddy-fabric-set-system-prompt
   or press ‘C-c f p’.

17.5 Using Direct API Access
============================

For direct programmatic access to Ollama:

     (ollama-buddy--make-request "/api/tags" "GET")

   Or with a payload:
     (ollama-buddy--make-request "/api/chat" "POST"
                                (json-encode '((model . "llama3:latest")
                                              (prompt . "Hello"))))


File: ollama-buddy.info,  Node: API Reference,  Next: FAQ,  Prev: Advanced Usage,  Up: Top

18 API Reference
****************

18.1 Interactive Functions
==========================

‘ollama-buddy-menu’
     Display the main Ollama Buddy menu.

‘ollama-buddy-transient-menu’
     Display the transient-based menu.

‘ollama-buddy--open-chat’
     Open the chat buffer.

‘ollama-buddy--send-prompt’
     Send the current prompt to the AI.

‘ollama-buddy--swap-model’
     Switch to a different model.

‘ollama-buddy-manage-models’
     Display and manage available models.

‘ollama-buddy-pull-model’
     Pull a new model from Ollama Hub.

‘ollama-buddy-import-gguf-file’
     Import a GGUF file to create a custom model.

‘ollama-buddy-set-system-prompt’
     Set the current prompt as the system prompt.

‘ollama-buddy-reset-system-prompt’
     Reset the system prompt to default (none).

‘ollama-buddy-sessions-save’
     Save the current conversation as a session.

‘ollama-buddy-sessions-load’
     Load a previously saved session.

‘ollama-buddy-sessions-list’
     Display a list of saved sessions.

‘ollama-buddy-sessions-delete’
     Delete a saved session.

‘ollama-buddy-sessions-new’
     Start a new session.

‘ollama-buddy-toggle-history’
     Toggle conversation history on/off.

‘ollama-buddy-clear-history’
     Clear the conversation history.

‘ollama-buddy-history-edit’
     Display the conversation history.

‘ollama-buddy-roles-switch-role’
     Switch to a different role.

‘ollama-buddy-role-creator-create-new-role’
     Create a new role.

‘ollama-buddy-params-display’
     Display current parameter settings.

‘ollama-buddy-params-edit’
     Edit a specific parameter.

‘ollama-buddy-params-reset’
     Reset all parameters to defaults.

‘ollama-buddy-toggle-params-in-header’
     Toggle display of parameters in header.

‘ollama-buddy-toggle-token-display’
     Toggle display of token statistics.

‘ollama-buddy-display-token-stats’
     Display token usage statistics.

‘ollama-buddy-display-token-graph’
     Display a visual graph of token usage.

‘ollama-buddy-fabric-setup’
     Set up Fabric pattern integration.

‘ollama-buddy-fabric-sync-patterns’
     Sync with the latest Fabric patterns.

‘ollama-buddy-fabric-list-patterns’
     List available Fabric patterns.

‘ollama-buddy-fabric-send’
     Apply a Fabric pattern to selected text.

‘ollama-buddy-toggle-markdown-conversion’
     Toggle Markdown to Org conversion.

‘ollama-buddy-toggle-debug-mode’
     Toggle display of debug information.

‘ollama-buddy-set-model-context-size’
     Set the context size for a specific model.

‘ollama-buddy-toggle-context-percentage’
     Toggle context percentage display in the status bar.

‘ollama-buddy-show-context-info’
     Display detailed context usage information.

‘ollama-buddy-set-max-history-length’
     Set the maximum number of message pairs to keep in history.

‘ollama-buddy-user-prompts-save’
     Save the current system prompt.

‘ollama-buddy-user-prompts-load’
     Load a previously saved system prompt.

‘ollama-buddy-user-prompts-list’
     Display a list of all saved user system prompts.

‘ollama-buddy-user-prompts-edit’
     Edit a user system prompt.

‘ollama-buddy-user-prompts-delete’
     Delete a user system prompt.

‘ollama-buddy-user-prompts-create-new’
     Create a new system prompt from scratch.

‘ollama-buddy-transient-user-prompts-menu’
     Display the transient menu for user system prompts.

18.2 Core Functions
===================

‘ollama-buddy--send’
     Send a prompt to Ollama.

‘ollama-buddy--make-request’
     Make a generic request to the Ollama API.

‘ollama-buddy--get-models’
     Get a list of available models.

‘ollama-buddy--get-valid-model’
     Get a valid model with fallback handling.

‘ollama-buddy--add-to-history’
     Add a message to the conversation history.

‘ollama-buddy--get-history-for-request’
     Get history for the current request.

‘ollama-buddy--prepare-prompt-area’
     Prepare the prompt area in the buffer.

‘ollama-buddy--update-status’
     Update the status display.

18.3 Customization Functions
============================

‘ollama-buddy-update-command-with-params’
     Update a command definition with new properties and parameters.

‘ollama-buddy-update-menu-entry’
     Update a menu entry's properties.

‘ollama-buddy-add-model-to-menu-entry’
     Associate a specific model with a menu entry.

‘ollama-buddy-add-parameters-to-command’
     Add specific parameters to a command definition.


File: ollama-buddy.info,  Node: FAQ,  Next: Troubleshooting,  Prev: API Reference,  Up: Top

19 Frequently Asked Questions
*****************************

19.1 General Questions
======================

19.1.1 What is the difference between Ollama Buddy and other AI assistants?
---------------------------------------------------------------------------

Ollama Buddy integrates with Ollama to run LLMs locally, offering
privacy, customization, and seamless Emacs integration without relying
on external API services.

19.1.2 Does Ollama Buddy require an internet connection?
--------------------------------------------------------

Once you've installed Ollama and pulled your models, no internet
connection is required for normal operation.  Internet is only needed
when pulling new models or syncing Fabric patterns.

19.1.3 Which models work best with Ollama Buddy?
------------------------------------------------

Most models supported by Ollama work well.  Popular choices include:
   • llama3:latest - Good general purpose assistant
   • codellama:latest - Excellent for code-related tasks
   • mistral:latest - Good balance of performance and quality
   • phi:latest - Smaller model that works well on limited hardware

19.1.4 How much RAM do I need?
------------------------------

It depends on the model:
   • Small models (7B) - 8GB minimum, 16GB recommended
   • Medium models (13B) - 16GB minimum, 24GB+ recommended
   • Large models (34B+) - 32GB+ recommended

   Quantized models (e.g., Q4_K_M variants) require less RAM.

19.2 Usage Questions
====================

19.2.1 How do I cancel a request that's taking too long?
--------------------------------------------------------

Press ‘C-c k’ in the chat buffer or select "Kill Request" from the menu.

19.2.2 How can I save my conversations?
---------------------------------------

Use ‘C-c S’ to save the current session, giving it a name.  You can
restore it later with ‘C-c L’.

19.2.3 Can I use multiple models in the same conversation?
----------------------------------------------------------

Yes, you can switch models at any time with ‘C-c m’.  Each model
maintains its own conversation history.

19.2.4 How do I clear the conversation history?
-----------------------------------------------

Press ‘C-c X’ to clear history, or ‘C-c N’ to start a completely new
session.

19.2.5 How can I create a custom command?
-----------------------------------------

The easiest way is through the role creator: press ‘C-c E’ and follow
the prompts to create commands with specific prompts, models, and
parameters.

19.2.6 How can I manage context windows?
----------------------------------------

Ollama Buddy provides several options:
   • Enable context monitoring with ‘(setq
     ollama-buddy-show-context-percentage t)’
   • Use ‘C-c C’ to check current context usage
   • Limit history length with ‘C-c Y’
   • Set model-specific context sizes with ‘C-c $’

19.2.7 What happens when I exceed the context limit?
----------------------------------------------------

When context monitoring is enabled:
   • You'll get a warning when approaching the limit (85-100%)
   • You'll get an error dialog at or above 100%
   • You can choose to proceed anyway or modify your content

19.2.8 How do I create effective system prompts?
------------------------------------------------

Effective system prompts typically include:
   • Clear role definition for the AI
   • Specific guidelines for response format and style
   • Examples of desired output (when applicable)
   • Constraints or limitations to observe
   Start simple, test the response, and refine iteratively.

19.2.9 Where are my system prompts stored?
------------------------------------------

System prompts are stored as .org files in the directory specified by
'ollama-buddy-user-prompts-directory', which defaults to
'~/.emacs.d/ollama-buddy-user-prompts/'.

19.3 Troubleshooting
====================

19.3.1 Ollama Buddy shows "OFFLINE" status
------------------------------------------

Ensure that:
   • Ollama is installed and running
   • The hostname and port are correctly configured (‘ollama-buddy-host’
     and ‘ollama-buddy-port’)
   • Your firewall isn't blocking connections

19.3.2 Responses are slow or the model seems to hang
----------------------------------------------------

Try:
   • Using a smaller model
   • Adjusting the ‘num_ctx’ parameter to a smaller value
   • Setting ‘low_vram’ to ‘t’ if you have limited GPU memory
   • Checking CPU/RAM usage to ensure your system isn't overloaded

19.3.3 Getting "error parsing model" when pulling a model
---------------------------------------------------------

This usually means:
   • The model name is incorrect
   • The model is not available in the Ollama repository
   • You have network connectivity issues

19.3.4 Model responses are low quality or truncated
---------------------------------------------------

Try:
   • Increasing the ‘temperature’ parameter for more creative responses
   • Increasing ‘num_predict’ for longer responses
   • Using a more capable model
   • Providing clearer instructions in your prompt

19.3.5 How do system prompts differ from regular prompts?
---------------------------------------------------------

System prompts provide overall instructions to the AI about its role and
how to respond, while regular prompts are your specific questions or
requests.  System prompts persist across the conversation, affecting all
responses, while regular prompts are one-time interactions.

19.3.6 Can I share system prompts between different installations?
------------------------------------------------------------------

Yes, the user system prompts are stored as plain text .org files in your
'ollama-buddy-user-prompts-directory'.  You can copy these files to
share them with colleagues or between different machines.


File: ollama-buddy.info,  Node: Troubleshooting,  Next: Contributing,  Prev: FAQ,  Up: Top

20 Troubleshooting
******************

20.1 Common Issues
==================

20.1.1 Connection Problems
--------------------------

Symptom: Unable to connect to Ollama server
        • Check if Ollama is running with ‘ps aux | grep ollama’
        • Verify host and port settings (‘ollama-buddy-host’ and
          ‘ollama-buddy-port’)
        • Try connecting to Ollama directly: ‘curl
          http://localhost:11434/api/tags’

Symptom: Connection breaks during long responses
        • This can happen with very large responses
        • Try setting a lower ‘num_predict’ value
        • Check if your OS has any network timeout settings

20.1.2 Model Problems
---------------------

Symptom: Model loads but gives poor responses
        • Try a different model
        • Adjust parameters (increase temperature for more creativity)
        • Provide clearer or more detailed prompts
        • Check if the model is appropriate for your task

Symptom: Model fails to load or crashes
        • Check system memory usage
        • Try a smaller quantized model
        • Adjust ‘num_ctx’ to a smaller value
        • Set ‘low_vram’ to ‘t’ if using GPU

20.1.3 Interface Issues
-----------------------

Symptom: Chat buffer becomes unresponsive
        • Cancel any running requests with ‘C-c k’
        • Check if Emacs is using high CPU
        • Try disabling token statistics display
        • Close and reopen the chat buffer

Symptom: Markdown conversion issues
        • Toggle markdown conversion off with ‘C-c C-o’
        • Check if the response contains complex formatting
        • Try editing the history to fix formatting issues

20.2 Debugging
==============

20.2.1 Enable Debug Mode
------------------------

To get more information about what's happening:
     M-x ollama-buddy-toggle-debug-mode

   This opens a debug buffer showing raw JSON communication with Ollama.

20.2.2 Check Logs
-----------------

Ollama logs can be useful for troubleshooting:
     tail -f ~/.ollama/logs/ollama.log

20.2.3 Report Issues
--------------------

If you encounter a bug:
  1. Enable debug mode
  2. Reproduce the issue
  3. Copy the debug output
  4. Report the issue on GitHub with:
        • Emacs version
        • Ollama version
        • Model used
        • Debug output
        • Steps to reproduce


File: ollama-buddy.info,  Node: Contributing,  Next: Index,  Prev: Troubleshooting,  Up: Top

21 Contributing
***************

21.1 Getting Started
====================

Ollama Buddy is an open-source project, and contributions are welcome!

  1. Fork the repository:
     <https://github.com/captainflasmr/ollama-buddy>
  2. Clone your fork: ‘git clone
     https://github.com/YOUR-USERNAME/ollama-buddy.git’
  3. Create a branch: ‘git checkout -b my-feature-branch’
  4. Make your changes
  5. Test thoroughly
  6. Commit with a clear message
  7. Push to your fork
  8. Create a pull request

21.2 Development Setup
======================

21.2.1 Required Tools
---------------------

   • Emacs 28.1+
   • Ollama installed and running
   • Git

21.2.2 Recommended Packages
---------------------------

   • package-lint
   • flycheck
   • elisp-lint

21.3 Coding Guidelines
======================

   • Follow Emacs Lisp conventions
   • Use two spaces for indentation
   • Add documentation strings to functions
   • Keep line length under 80 characters
   • Use prefix ‘ollama-buddy--’ for internal functions
   • Use prefix ‘ollama-buddy-’ for public functions

21.4 Testing
============

21.4.1 Run Existing Tests
-------------------------

The package includes comprehensive tests:

     M-x ollama-buddy-run-tests
     M-x ollama-buddy-integration-run-tests
     M-x ollama-buddy-fabric-run-tests
     M-x ollama-buddy-parameter-run-tests

21.4.2 Adding New Tests
-----------------------

When adding features, please also add tests:
   • Unit tests for individual functions
   • Integration tests for API interactions
   • Parameter tests for parameter handling

21.5 Feature Requests and Bug Reports
=====================================

   • Use GitHub Issues for bug reports and feature requests
   • Provide clear steps to reproduce bugs
   • For feature requests, explain the use case

21.5.1 User System Prompts Issues
---------------------------------

Symptom: Cannot save system prompts
        • Check if 'ollama-buddy-user-prompts-directory' exists and is
          writable
        • Ensure you have set a system prompt before trying to save it
        • Check for any error messages in the minibuffer or *Messages*
          buffer

Symptom: Prompts not appearing in the load menu
        • Verify that prompts are saved in the correct format
          (category__title__system.org)
        • Check if the prompts directory contains files with proper
          formatting
        • Try refreshing the prompts cache with 'M-x
          ollama-buddy-user-prompts-refresh-cache'


File: ollama-buddy.info,  Node: Index,  Prev: Contributing,  Up: Top

Index
*****


Tag Table:
Node: Top550
Node: Introduction2239
Node: Installation4196
Node: Configuration6458
Node: Quick Start12042
Node: Core Features13791
Node: Chat Interface16652
Node: Working with Models19485
Node: Context Management22136
Node: File Attachments27745
Node: Parameter Control33674
Node: Session Management36077
Node: User System Prompts38086
Node: Roles and Commands46576
Node: Fabric Pattern Integration58154
Node: Awesome ChatGPT Prompts60381
Node: ChatGPT and Claude Support64241
Node: Advanced Usage67696
Node: API Reference70394
Node: FAQ75142
Node: Troubleshooting81153
Node: Contributing83658
Node: Index86326

End Tag Table


Local Variables:
coding: utf-8
End:
