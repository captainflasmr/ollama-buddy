\input texinfo
@c -*-texinfo-*-
@c %**start of header
@setfilename ollama-buddy.info
@documentencoding UTF-8
@settitle Ollama Buddy User Manual
@c %**end of header

@dircategory Emacs
@direntry
* Ollama Buddy: (ollama-buddy). AI assistant integration with Ollama.
@end direntry

@copying
Copyright @copyright{} 2024-2026 James Dyer

@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
@end quotation
@end copying

@titlepage
@title Ollama Buddy User Manual
@subtitle For Ollama Buddy version 2.9
@author James Dyer
@page
@vskip 0pt plus 1filll
@insertcopying
@end titlepage

@c Output the table of the contents at the beginning.
@contents

@ifnottex
@node Top
@top Ollama Buddy

Ollama Buddy is a comprehensive Emacs package that provides seamless integration with Ollama, 
allowing you to leverage powerful large language models (LLMs) directly within your Emacs workflow.

@end ifnottex

@menu
* Introduction::                What is Ollama Buddy?
* Installation::                How to install the package
* Configuration::               Basic and advanced configuration
* Quick Start::                 Getting started with basic commands
* Core Features::               Explanation of main capabilities
* Chat Interface::              Using the chat UI
* Working with Models::         Managing and using different models
* Context Management::          Managing context
* File Attachments::            Including files in conversations
* Web Search::                  Real-time web search integration
* Parameter Control::           Customizing AI behavior with parameters
* Session Management::          Saving and loading conversations
* User System Prompts::         Saving and managing system prompts
* Roles and Commands::          Creating custom commands and roles
* Fabric Pattern Integration::  Using predefined prompt patterns
* Awesome ChatGPT Prompts::     Using the Awesome ChatGPT Prompts collection
* Remote Providers::            Using commercial LLM APIs (OpenAI, Claude, Gemini, Grok, Codestral)
* Ollama Cloud Models::         Using Ollama's cloud-hosted models
* Global System Prompt::        Configuring a persistent global system prompt
* Tone Selector::               Quickly switch response style
* Thinking Blocks::             Collapsible reasoning output for thinking models
* In-Buffer Replace::           Stream rewrites directly into source buffers with inline diff
* Tool Calling::                Enabling LLMs to invoke Emacs functions
* RAG::                         Retrieval-Augmented Generation with local documents
* Advanced Usage::              Tips and techniques for power users
* API Reference::               Comprehensive function documentation
* FAQ::                         Frequently asked questions
* Troubleshooting::             Common problems and solutions
* Contributing::                How to contribute to Ollama Buddy
* Index::                       Complete index

@end menu

@node Introduction
@chapter Introduction

@section What is Ollama Buddy?

Ollama Buddy is an Emacs package that provides a friendly AI assistant interface to Ollama, 
a tool for running large language models (LLMs) locally on your computer. It allows you to 
interact with AI models directly from within Emacs for various tasks such as:

@itemize @bullet
@item Code refactoring and explanation
@item Writing assistance and proofreading
@item Generating Git commit messages
@item Dictionary lookups and language assistance
@item Custom AI-powered workflows via roles
@item Using pre-built prompt templates from Fabric
@item Utilizing Awesome ChatGPT Prompts
@item Integrating with commercial APIs (OpenAI, Claude, Gemini, Grok, Copilot, Codestral)
@item Using Ollama cloud models
@item Including files as context in conversations
@item Real-time web search to provide current information to LLMs
@item Tool calling --- enabling LLMs to read files, run commands, search buffers and more
@end itemize

Instead of context-switching to web interfaces or terminal applications, Ollama Buddy brings 
the power of local LLMs right into your Emacs workflow.

@section Why Use Ollama Buddy?

@itemize @bullet
@item @strong{Privacy}: All interactions happen locally with Ollama - no data sent to external services unless you use commercial APIs
@item @strong{Integration}: Seamlessly fits into your existing Emacs workflow
@item @strong{Flexibility}: Supports multiple models, parameter tuning, and custom commands
@item @strong{Efficiency}: Quick access to AI assistance without leaving your editor
@item @strong{Extensibility}: Create custom roles and commands for your specific needs
@item @strong{File Support}: Include text files, code, and documentation directly in conversations
@end itemize

@section Prerequisites

Before using Ollama Buddy, you need:

@itemize @bullet
@item Emacs 28.1 or later
@item Ollama installed and running on your system (see @url{https://ollama.ai})
@item At least one language model pulled into Ollama
@item (Optional) API keys for OpenAI or Claude if you want to use those services
@end itemize

@node Installation
@chapter Installation

@section Installing Ollama

Before installing Ollama Buddy, you need to install Ollama itself:

@enumerate
@item Visit @url{https://ollama.ai} and download the installer for your platform
@item Install and run Ollama according to the instructions
@item Pull at least one model using @code{ollama pull llama3:latest} (or another model of your choice)
@end enumerate

@section Package Installation

@subsection Using package.el

The recommended way to install Ollama Buddy is through MELPA:

@example
M-x package-install RET ollama-buddy RET
@end example

@subsection Using use-package

If you use @code{use-package}, add the following to your Emacs configuration:

@example
(use-package ollama-buddy
  :ensure t
  :bind ("C-c o" . ollama-buddy-role-transient-menu))
@end example

With a default model:

@example
(use-package ollama-buddy
  :ensure t
  :bind ("C-c o" . ollama-buddy-role-transient-menu)
  :custom (ollama-buddy-default-model "llama3:latest"))
@end example

@subsection Manual Installation

To install manually:

@enumerate
@item Clone the repository:
@example
git clone https://github.com/captainflasmr/ollama-buddy.git
@end example

@item Add to your configuration:
@example
(add-to-list 'load-path "/path/to/ollama-buddy")
(require 'ollama-buddy)
(global-set-key (kbd "C-c o") #'ollama-buddy-role-transient-menu)
@end example
@end enumerate

@section Dependencies

Ollama Buddy requires the following:

@itemize @bullet
@item Emacs 28.1 or later
@item (Optional) transient 0.4.0+ for advanced menu system
@end itemize

Built-in packages used: json, cl-lib, url, subr-x, dired, org, savehist.

@section Communication Backends

Ollama Buddy supports two communication backends:

@table @code
@item network-process (default)
Uses Emacs built-in network process.

@item curl
Uses external curl command for requests. Useful as a fallback when network process has issues.
@end table

To switch backends:
@example
M-x ollama-buddy-switch-communication-backend
@end example
or press @code{C-c e} in the chat buffer.

To configure curl backend:
@example
(require 'ollama-buddy-curl)
(setq ollama-buddy-communication-backend 'curl)
(setq ollama-buddy-curl-executable "curl")
(setq ollama-buddy-curl-timeout 300)
@end example

@section API Key Setup

If you want to use OpenAI or Claude integration, you'll need to set up API keys securely:

@enumerate
@item Use Emacs built-in auth-source for secure storage
@item Add to your auth sources (e.g., ~/.authinfo.gpg):
@example
machine api.openai.com login apikey password YOUR_OPENAI_API_KEY_HERE
machine api.anthropic.com login apikey password YOUR_CLAUDE_API_KEY_HERE
@end example
@item Alternatively, set the variables directly (less secure):
@example
(setq ollama-buddy-openai-api-key "your-openai-key")
(setq ollama-buddy-claude-api-key "your-claude-key")
@end example
@end enumerate

@node Configuration
@chapter Configuration

@section Basic Configuration

Here are the essential configuration options:

@table @code
@item ollama-buddy-default-model
Set your preferred default model.
@example
(setq ollama-buddy-default-model "llama3:latest")
@end example

@item ollama-buddy-host
Host where Ollama server is running (default: "localhost").
@example
(setq ollama-buddy-host "localhost")
@end example

@item ollama-buddy-port
Port where Ollama server is running (default: 11434).
@example
(setq ollama-buddy-port 11434)
@end example
@end table

@section Display Options

Customize the appearance and behavior of Ollama Buddy:

@table @code
@item ollama-buddy-convert-markdown-to-org
Whether to automatically convert markdown to org-mode format in responses (default: t).
@example
(setq ollama-buddy-convert-markdown-to-org t)
@end example

@item ollama-buddy-display-token-stats
Whether to display token usage statistics after responses (default: nil).
@example
(setq ollama-buddy-display-token-stats t)
@end example

@item ollama-buddy-streaming-enabled
Whether to use streaming mode for responses (default: t).
@example
(setq ollama-buddy-streaming-enabled t)
@end example

@item ollama-buddy-auto-scroll
Whether to auto-scroll the chat buffer during streaming output (default: nil).
@example
(setq ollama-buddy-auto-scroll t)
@end example

@item ollama-buddy-pulse-response
Whether to pulse/flash the response text when streaming completes (default: t).
@example
(setq ollama-buddy-pulse-response t)
@end example

@item ollama-buddy-full-welcome-enabled
Whether to show the full welcome screen with all keybinding hints (default: t).
When nil, only the essential keybindings are shown.
@example
(setq ollama-buddy-full-welcome-enabled nil)
@end example
@end table

@section File Attachment Configuration

Configure file attachment behavior:

@table @code
@item ollama-buddy-max-file-size
Maximum size for attached files in bytes (default: 10MB).
@example
(setq ollama-buddy-max-file-size (* 10 1024 1024))  ; 10MB
@end example

@item ollama-buddy-supported-file-types
List of regex patterns for supported file types (default includes text, code, and configuration files).
@example
(setq ollama-buddy-supported-file-types
      '("\\.txt$" "\\.md$" "\\.org$" "\\.py$" "\\.js$" "\\.el$"))
@end example
@end table

@section Directory Configuration

Customize where Ollama Buddy stores its files:

@table @code
@item ollama-buddy-sessions-directory
Directory for storing session files.
@example
(setq ollama-buddy-sessions-directory 
      (expand-file-name "ollama-buddy-sessions" user-emacs-directory))
@end example

@item ollama-buddy-roles-directory
Directory for storing role preset files.
@example
(setq ollama-buddy-roles-directory
      (expand-file-name "ollama-buddy-presets" user-emacs-directory))
@end example

@item ollama-buddy-modelfile-directory
Directory for storing temporary Modelfiles.
@example
(setq ollama-buddy-modelfile-directory
      (expand-file-name "ollama-buddy-modelfiles" user-emacs-directory))
@end example

@item ollama-buddy-awesome-local-dir
Directory for storing Awesome ChatGPT Prompts.
@example
(setq ollama-buddy-awesome-local-dir
      (expand-file-name "awesome-chatgpt-prompts" user-emacs-directory))
@end example
@end table

@section History and Session Configuration

Configure how conversation history is managed:

@table @code
@item ollama-buddy-history-enabled
Whether to use conversation history in Ollama requests (default: t).
@example
(setq ollama-buddy-history-enabled t)
@end example

@item ollama-buddy-max-history-length
Maximum number of message pairs to keep in conversation history (default: 10).
@example
(setq ollama-buddy-max-history-length 10)
@end example

@item ollama-buddy-show-history-indicator
Whether to show the history indicator in the header line (default: nil).
@example
(setq ollama-buddy-show-history-indicator t)
@end example
@end table

@section Context Management Configuration

Configure how Ollama Buddy handles context management:

@table @code
@item ollama-buddy-show-context-percentage
Whether to show context percentage in the status bar (default: nil).
@example
(setq ollama-buddy-show-context-percentage t)
@end example

@item ollama-buddy-fallback-context-sizes
Mapping of model names to their default context sizes.
@example
(setq ollama-buddy-fallback-context-sizes
  '(("llama3:8b" . 4096)
    ("codellama:7b" . 8192)))
@end example

@item ollama-buddy-max-history-length
Maximum number of message pairs to keep (affects context usage).
@example
(setq ollama-buddy-max-history-length 10)
@end example
@end table

@section External API Configuration

For remote provider integration, configure API keys using auth-source (recommended):

@example
;; In ~/.authinfo.gpg
machine ollama-buddy-openai login apikey password YOUR_KEY
machine ollama-buddy-claude login apikey password YOUR_KEY
machine ollama-buddy-gemini login apikey password YOUR_KEY
machine ollama-buddy-grok login apikey password YOUR_KEY
machine ollama-buddy-codestral login apikey password YOUR_KEY
@end example

Then in your Emacs config:
@example
(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password
       :host "ollama-buddy-openai" :user "apikey"))
@end example

@subsection Provider Prefixes

Prefixes are only used when external providers (OpenAI, Claude, etc.) are loaded:

@table @asis
@item When only Ollama is used
Models appear without prefix: @code{llama3:8b}, @code{qwen3-coder:480b-cloud}

@item When external providers are loaded
Prefixes disambiguate providers:
@itemize @bullet
@item @code{o:} - Ollama local (e.g., @code{o:llama3:8b})
@item @code{cl:} - Ollama cloud (e.g., @code{cl:qwen3-coder:480b-cloud})
@item @code{a:} - OpenAI (e.g., @code{a:gpt-4})
@item @code{c:} - Claude (e.g., @code{c:claude-3-opus})
@item @code{g:} - Gemini (e.g., @code{g:gemini-pro})
@item @code{k:} - Grok (e.g., @code{k:grok-1})
@item @code{p:} - GitHub Copilot (e.g., @code{p:gpt-4o})
@item @code{s:} - Codestral (e.g., @code{s:codestral-latest})
@end itemize
@end table

The @code{☁} cloud symbol provides visual indication for cloud models regardless of whether prefixes are used.

@section Global System Prompt Configuration

Configure a persistent system prompt for all requests:

@table @code
@item ollama-buddy-global-system-prompt
The global prompt prepended to all requests.
@example
(setq ollama-buddy-global-system-prompt
  "Format responses in plain prose. Avoid markdown tables.")
@end example

@item ollama-buddy-global-system-prompt-enabled
Whether to enable the global prompt (default: t).
@example
(setq ollama-buddy-global-system-prompt-enabled t)
@end example
@end table

@section Vision Configuration

Configure vision support for image analysis:

@table @code
@item ollama-buddy-vision-enabled
Whether to enable vision support (default: t).

@item ollama-buddy-vision-models
Models known to support vision capabilities.
@example
(setq ollama-buddy-vision-models
      '("gemma3:4b" "llama3.2:3b" "llama3.2:8b"))
@end example

@item ollama-buddy-image-formats
Supported image file formats.
@end table

@section Cloud Model Configuration

Configure Ollama cloud models:

@table @code
@item ollama-buddy-cloud-models
List of available cloud models.
@example
(setq ollama-buddy-cloud-models
  '("qwen3-coder:480b-cloud"
    "deepseek-v3.1:671b-cloud"))
@end example

@item ollama-buddy-ollama-executable
Path to the ollama CLI (default: "ollama").
@end table

@section Awesome ChatGPT Prompts Configuration

Configure the Awesome ChatGPT Prompts integration:

@table @code
@item ollama-buddy-awesome-repo-url
URL of the Awesome ChatGPT Prompts GitHub repository.
@example
(setq ollama-buddy-awesome-repo-url "https://github.com/f/awesome-chatgpt-prompts.git")
@end example

@item ollama-buddy-awesome-update-on-startup
Whether to automatically update prompts when Emacs starts.
@example
(setq ollama-buddy-awesome-update-on-startup nil)
@end example

@item ollama-buddy-awesome-categorize-prompts
Whether to categorize prompts based on common keywords.
@example
(setq ollama-buddy-awesome-categorize-prompts t)
@end example
@end table

@node Quick Start
@chapter Quick Start

@section Basic Usage

@enumerate
@item Launch Ollama Buddy:
@example
M-x ollama-buddy-role-transient-menu
@end example
or use your configured keybinding (e.g., @code{C-c o}).

@item The menu will show available options. Press the corresponding key for the action you want.

@item To open the chat interface, press @code{o} or select "Open Chat".

@item In the chat buffer, type your prompt and press @code{C-c C-c} to send it.

@item The AI will respond in the chat buffer.
@end enumerate

@section Common Operations

@table @asis
@item Sending text from a file
Select text in any buffer, then press @code{C-c o} and choose "Send Region" (or press @code{l}).

@item Refactoring code
Select code, press @code{C-c o}, then choose "Refactor Code" (or press @code{r}).

@item Generating a commit message
Select your changes, press @code{C-c o}, then choose "Git Commit Message" (or press @code{g}).

@item Changing models
Press @code{C-c m} to switch between available models. Cloud models appear in the list marked with @code{☁}.

@item Using the transient menu
Press @code{C-c O} for the main transient menu with all features organized by category.

@item Attaching files
Press @code{C-c C-a} to attach a file, or use the transient menu "Attachments" section.

@item Toggling reasoning visibility
Press @code{C-c V} to hide or show reasoning/thinking sections in responses.

@item Toggling Org/Markdown conversion
Press @code{C-c C-o} to switch between Org-mode and Markdown output. Also available from the transient menu under "Diagnostics".

@item Toggling Global System Prompt
Press @code{C-c <} to toggle the global system prompt on or off. The header line shows @code{<} when disabled.

@item Using Awesome ChatGPT Prompts
Select text, press @code{C-c o}, then @code{a} for the Awesome prompts menu, then @code{s} to send with a prompt.

@item Using Fabric patterns
Select text, press @code{C-c o}, then @code{f} for the Fabric menu, then @code{s} to send with a pattern.

@item Getting help
In the chat buffer, press @code{C-c h} to display the help screen with available commands and models.
@end table

@node Core Features
@chapter Core Features

@section Chat Interface

The chat interface is the main way to interact with Ollama Buddy:

@itemize @bullet
@item Persistent conversation with history
@item Markdown to Org-mode conversion
@item Model-specific colors
@item System prompt support
@item Parameter customization
@item Reasoning/thinking section visibility control
@item Context window management and monitoring
@item Real-time context usage display
@item Context size validation before sending prompts
@item Customizable context thresholds and warnings
@item File attachment support
@end itemize

@section Pre-built Commands

Ollama Buddy comes with several pre-built commands:

@table @asis
@item Code Refactoring
Improves code while maintaining functionality

@item Code Description
Explains what code does and how it works

@item Git Commit Messages
Generates meaningful commit messages from code changes

@item Dictionary Lookups
Provides comprehensive word definitions

@item Synonym Finder
Suggests alternative words with context

@item Proofreading
Corrects grammar, style, and spelling
@end table

@section Model Management

@itemize @bullet
@item Switch between any model available in Ollama
@item Use ChatGPT and Claude models with API keys
@item Pull new models directly from the interface
@item View model information and statistics
@item Delete models you no longer need
@item Import GGUF files to create new models
@end itemize

@section Parameter Control

@itemize @bullet
@item Fine-tune model behavior with customizable parameters
@item Save and use parameter profiles for different use cases
@item Command-specific parameter settings
@item Real-time parameter adjustment
@end itemize

@section Role Transient Menu

The role menu (@code{ollama-buddy-role-transient-menu}) provides a dynamic transient popup built from the current role's @code{ollama-buddy-command-definitions}. It rebuilds each time it opens, so switching roles immediately changes the menu.

@itemize @bullet
@item Full transient UX with persistent display, help, and discoverability
@item Commands grouped into named columns via the @code{:group} property
@item Commands without @code{:group} appear in a single ``Commands'' column
@item Commands with @code{:model} show the model name in the description
@item Duplicate keys are deduplicated (first definition wins)
@item @code{quit} entries are skipped (transient handles quit natively with @code{C-g})
@item The original minibuffer menu is still available via @code{M-x ollama-buddy-menu}
@end itemize

@section Roles and Custom Commands

@itemize @bullet
@item Create custom command sets for specific workflows
@item Design specialized AI assistants with custom system prompts
@item Save and switch between different roles
@item Share role configurations across your team
@end itemize

@section Prompt Template Collections

@itemize @bullet
@item Use pre-built prompt patterns from Fabric project
@item Utilize the Awesome ChatGPT Prompts collection
@item Apply specialized prompts to your content with one command
@item Browse prompts by category
@end itemize

@section External API Integration

@itemize @bullet
@item Connect to OpenAI's ChatGPT API (prefix: @code{a:})
@item Connect to Anthropic's Claude API (prefix: @code{c:})
@item Connect to Google Gemini API (prefix: @code{g:})
@item Connect to X Grok API (prefix: @code{k:})
@item Connect to GitHub Copilot Chat API (prefix: @code{p:})
@item Connect to Mistral Codestral API (prefix: @code{s:})
@item Use Ollama cloud models (indicated by @code{☁})
@item Seamlessly switch between local and cloud models
@item Secure API key management via auth-source
@end itemize

@section Vision Support

@itemize @bullet
@item Analyze images with vision-capable models
@item Automatic image detection in prompts
@item Support for PNG, JPG, JPEG, WebP, and GIF formats
@item Configurable list of vision-capable models
@end itemize

@section File Attachments

@itemize @bullet
@item Attach text files, code, and documentation to conversations
@item Automatic context inclusion with proper token counting
@item Session persistence for attachments
@item Support for various file types
@item Dired integration for bulk file attachment
@end itemize

@node Chat Interface
@chapter Chat Interface

@section Opening the Chat

To open the chat interface:

@enumerate
@item Use @code{M-x ollama-buddy-role-transient-menu} or your configured keybinding
@item Press @code{o} to select "Open Chat"
@item A new buffer will open with the Ollama Buddy chat interface
@end enumerate

@section Interface Overview

The chat interface consists of:

@itemize @bullet
@item A welcome message with available models and providers
@item Conversation history (previous prompts and responses)
@item A prompt area for entering your queries with model capability indicators
@item A header line with status information including:
  @itemize @minus
  @item Cloud model indicator (@code{☁}) - using an Ollama cloud model
  @item Tool calling enabled indicator (@code{⚒}) - tools enabled AND model supports them
  @item Vision indicator (@code{⊙}) - model supports vision
  @item Attachment indicators (@code{≡N})
  @item Web search indicators (@code{♁N})
  @item Context usage bar (when enabled)
  @item Global system prompt disabled indicator (@code{<})
  @item History count (@code{H5/10}, off by default)
  @item Current model name
  @item System prompt indicator
  @item Modified parameters
  @end itemize
@item A prompt line showing model capabilities:
  @itemize @minus
  @item @code{☁} when using a cloud model
  @item @code{⚒} when model supports tool calling (regardless of enabled state)
  @end itemize
@item Context warnings and validation
@end itemize

@section Sending Prompts

To send a prompt to the AI:

@enumerate
@item Type your message in the prompt area (after ">> PROMPT:")
@item Press @code{C-c C-c} to send
@item Wait for the AI to generate a response
@end enumerate

You can also:
@itemize @bullet
@item Use @code{M-p} and @code{M-n} to navigate through prompt history
@item Press @code{C-c k} to cancel a request if it's taking too long
@item Press @code{C-a} for smart beginning-of-line: first press jumps to the start of the prompt text (after ``>> PROMPT:''), second press goes to column 0
@end itemize

@section Inline @@ Commands

When typing in the prompt area, pressing @code{@@} triggers a @code{completing-read}
menu of available inline commands.  These insert syntax templates into the prompt.

Available @code{@@} commands:

@table @code
@item @@search(query)
Search the web and attach results.

@item @@rag(query)
Search a RAG index and attach results.

@item @@file(path)
Attach a file to the conversation context.
@end table

@itemize @bullet
@item Outside the prompt area, @code{@@} inserts a literal @samp{@@}.
@item Press @code{C-g} to cancel (nothing inserted); use @code{C-q @@} for a literal @samp{@@} in the prompt.
@item Customize available commands via @code{ollama-buddy-at-commands}.
@end itemize

@subsection Inline @@file(path) Syntax

Use @code{@@file(path)} in prompts to attach files directly:

@example
Review this code: @@file(/home/user/project/main.py)
@end example

Multiple files can be attached:

@example
Compare @@file(/path/to/old.py) with @@file(/path/to/new.py)
@end example

Paths are expanded when the prompt is sent.  Already-attached and non-existent
files are skipped with a message.  Works well with @code{dired-copy-filename-as-kill}
for pasting file paths.

@section Slash Commands

When typing in the prompt area, pressing @code{/} triggers a @code{completing-read}
menu of action commands.  Unlike @code{@@} commands which insert text, slash commands
execute immediately without sending anything to the LLM.

Available @code{/} commands (filtered by loaded modules):

@table @code
@item model
Swap the current model (@code{ollama-buddy--swap-model}).

@item system
Load a user prompt (@code{ollama-buddy-user-prompts-load}).
Requires @code{ollama-buddy-user-prompts}.

@item clear
Start a new session (@code{ollama-buddy-sessions-new}).

@item save
Save the current session (@code{ollama-buddy-sessions-save}).

@item load
Load a saved session (@code{ollama-buddy-sessions-load}).

@item tools
Toggle tool calling (@code{ollama-buddy-tools-toggle}).
Requires @code{ollama-buddy-tools}.

@item context
Show current attachments (@code{ollama-buddy-show-attachments}).

@item help
Open the help assistant (@code{ollama-buddy--menu-help-assistant}).

@item copy
Copy the last assistant response to the kill ring (@code{ollama-buddy-copy-last-response}).

@item retry
Resend the last user prompt to the current model (@code{ollama-buddy-retry-last-prompt}).

@item tone
Set the response tone/style (@code{ollama-buddy-set-tone}).

@item fabric
Load a Fabric pattern as system prompt (@code{ollama-buddy-fabric-set-system-prompt}).
Requires @code{ollama-buddy-fabric}.

@item awesome
Load an Awesome ChatGPT prompt as system prompt (@code{ollama-buddy-awesome-set-system-prompt}).
Requires @code{ollama-buddy-awesome}.

@item streaming
Toggle streaming mode on/off (@code{ollama-buddy-toggle-streaming}).

@item reset
Reset system prompt to default (@code{ollama-buddy-reset-system-prompt}).
@end table

@itemize @bullet
@item Outside the prompt area, @code{/} inserts a literal @samp{/}.
@item Press @code{C-g} to cancel (nothing inserted); use @code{C-q /} for a literal @samp{/} in the prompt.
@item Customize available commands via @code{ollama-buddy-slash-commands}.
@end itemize

@section System Prompts

System prompts allow you to define the AI's behavior:

@table @asis
@item Setting a system prompt
Type your system prompt, then press @code{C-c s}

@item Viewing the current system prompt
Press @code{C-c C-s}

@item Resetting the system prompt
Press @code{C-c C-r}, or use the main transient menu (@kbd{C-c O}) under System Prompts @code{C-r}

@item Using a pre-built prompt
Use Fabric patterns (@code{C-c f p}) or Awesome ChatGPT prompts (@code{C-c w p})
@end table

Example system prompt:
@example
You are a programming expert who specializes in Python. 
Provide concise, efficient solutions with explanations.
@end example

@section Markdown to Org Conversion

By default, Ollama Buddy converts markdown in responses to Org-mode syntax:

@itemize @bullet
@item Code blocks are converted to Org-mode source blocks
@item Headers are converted to Org-mode headings
@item Lists are properly formatted
@item Links are converted to Org-mode format
@end itemize

To toggle this feature:
@example
M-x ollama-buddy-toggle-markdown-conversion
@end example
or press @code{C-c C-o} in the chat buffer.

@section Reasoning Visibility Control

Ollama Buddy can hide reasoning/thinking sections in responses, making the output cleaner:

@itemize @bullet
@item Toggle visibility with @code{C-c V} or @code{M-x ollama-buddy-toggle-reasoning-visibility}
@item Configure markers with the @code{ollama-buddy-reasoning-markers} variable
@item When hidden, a status message shows the current reasoning section (e.g., "Think...")
@item Header line shows @code{V} indicator when reasoning is hidden
@end itemize

This feature helps focus on final answers while preserving the option to view the full reasoning process.

@node Working with Models
@chapter Working with Models

@section Available Models

Ollama Buddy displays available models in the chat interface. Each model is assigned a letter for quick selection.

To view detailed model information:
@example
M-x ollama-buddy-show-model-status
@end example
or press @code{C-c v} in the chat buffer.

@section Switching Models

To change the current model:

@enumerate
@item Press @code{C-c m} in the chat buffer
@item Select a model from the completion list
@item The new model will be used for future requests
@end enumerate

You can also switch models from the main menu with @code{m}.

@section Local vs. Cloud Models

Ollama Buddy supports both local Ollama models and cloud-based models:

@itemize @bullet
@item Local models (via Ollama): llama3, codellama, mistral, etc.
@item OpenAI models: gpt-3.5-turbo, gpt-4, etc.
@item Claude models: claude-3-opus, claude-3-sonnet, etc.
@end itemize

To use cloud models, you need to configure API keys as described in the Installation chapter.

@section Managing Models

Ollama Buddy provides a comprehensive model management interface. To access it:
@example
M-x ollama-buddy-manage-models
@end example
or press @code{C-c M} in the chat buffer.

From this interface, you can:
@itemize @bullet
@item See which local models are currently running (with option to unload all)
@item Pull new models from Ollama Hub or import GGUF files
@item Delete models you no longer need
@item View detailed model information
@item Select models for use
@item Browse cloud models (@code{cl:} prefix) with Select and Pull Manifest actions
@item Browse and install recommended models organized by category (General Chat, Reasoning, Efficient & Capable, Coding, General Alternatives)
@end itemize

Each recommended model category includes a description to help new users choose
the right type of model for their needs.  Categories are displayed as foldable
org subheadings so you can collapse sections you are not interested in.

@section Pulling New Models

To pull a new model:

@enumerate
@item Open the model management interface with @code{C-c M}
@item Click "[Custom Pull Model]" at the top of the Actions section
@item Enter the model name (e.g., "phi:latest", "codellama:7b")
@item Wait for the model to download
@end enumerate

You can also select from the ``Recommended Models'' list at the bottom, which shows curated popular models that can be pulled with a single click.

@section Importing GGUF Files

You can import custom GGUF model files:

@enumerate
@item Press @code{C-c M} to open the model management interface
@item Click "[Import GGUF File]" in the Actions section at the top
@item Select the GGUF file from your file system
@item Enter a name for the model
@item Optionally provide model parameters
@item Wait for Ollama to create the model
@end enumerate

@section Vision Models

Some models support vision capabilities for image analysis.

@subsection Configuring Vision Support

@table @code
@item ollama-buddy-vision-enabled
Whether to enable vision support (default: t).
@example
(setq ollama-buddy-vision-enabled t)
@end example

@item ollama-buddy-vision-models
List of models known to support vision:
@example
(setq ollama-buddy-vision-models
      '("gemma3:4b" "llama3.2:3b" "llama3.2:8b"))
@end example

@item ollama-buddy-image-formats
Supported image file formats:
@example
(setq ollama-buddy-image-formats
      '("\\.png$" "\\.jpg$" "\\.jpeg$" "\\.webp$" "\\.gif$"))
@end example
@end table

@subsection Using Vision Models

To analyze an image:
@enumerate
@item Select a vision-capable model
@item Include an image path in your prompt, or use the "Analyze image" command
@item The image will be automatically encoded and sent to the model
@end enumerate

Image paths can be:
@itemize @bullet
@item Quoted paths: @code{"path/to/image.png"}
@item Unquoted paths: @code{/home/user/image.jpg}
@end itemize

@section Multishot Mode

Multishot mode allows you to send the same prompt to multiple models simultaneously:

@enumerate
@item Type your prompt in the chat buffer
@item Press @code{C-c U}
@item Enter the sequence of model letters you want to use (e.g., "a,b,c" to use models a, b, and c)
@item Note that each item should be separated with a comma
@item Watch as Ollama Buddy processes your request with each model in sequence
@end enumerate

@node Context Management
@chapter Context Management

@section Understanding Context Windows

Context windows define how much text (measured in tokens) a model can process at once. This includes your current prompt, conversation history, any system prompts, and attached files. Understanding and managing context is crucial for:

@itemize @bullet
@item Preventing errors when context limits are exceeded
@item Optimizing model performance for different tasks
@item Managing longer conversations efficiently
@item Including files without exceeding context limits
@end itemize

@section Context Size Detection

Ollama Buddy uses multiple methods to determine a model's context size:

@enumerate
@item Built-in mappings for popular models (llama3, mistral, codellama, etc.)
@item Custom context sizes set via the @code{num_ctx} parameter
@item Manual configuration through interactive commands
@item Fallback to reasonable defaults (4096 tokens) for unknown models
@end enumerate

@section Enabling Context Monitoring

Context monitoring is disabled by default. To enable it:

@example
(setq ollama-buddy-show-context-percentage t)
@end example

With context monitoring enabled:
@itemize @bullet
@item The status bar shows current/max context usage (e.g., "2048/8192")
@item Text formatting indicates usage levels:
  @itemize @minus
  @item Normal font: Under 85% usage
  @item Bold and underlined: 85-100% usage
  @item Inverted: At or exceeding 100% usage
  @end itemize
@item Warnings appear before sending prompts that exceed limits
@end itemize

@section Context with File Attachments

File attachments are included in context calculations:

@itemize @bullet
@item Each attached file contributes to the total token count
@item The context breakdown shows attachment tokens separately
@item File content is included in the request context
@item Large files can significantly impact context usage
@end itemize

@section Context Management Commands

@table @asis
@item Show Context Information (@kbd{C-c C})
Displays a breakdown of current context usage, including:
@itemize @bullet
@item Conversation history token count
@item System prompt token count
@item Attachment token count
@item Current prompt token count
@item Total usage percentage
@end itemize

@item Set Model Context Size (@kbd{C-c $})
Manually configure the context size for a specific model.

@item Toggle Context Display (@kbd{C-c %})
Show or hide the context percentage in the status bar.
@end table

@section Token Estimation

Ollama Buddy estimates token counts using a heuristic approach:
@itemize @bullet
@item Each word is multiplied by 1.3 (following common approximations)
@item This provides a reasonable estimate for most use cases
@item Actual token counts may vary slightly between models
@end itemize

@section Managing Context in Practice

@subsection Workflow Strategies

@subsubsection Paste-and-Send Approach
@enumerate
@item Paste your content into the chat buffer
@item Press the send keybinding
@item If context is exceeded, you'll get a warning dialog
@item Choose whether to proceed or modify your content
@end enumerate

@subsubsection Preemptive Checking
@enumerate
@item Paste your content
@item Use @kbd{C-c C} to check context usage
@item If too high:
  @itemize @bullet
  @item Trim your current prompt
  @item Edit conversation history (@kbd{C-c J})
  @item Switch to a larger context model
  @item Adjust system prompt length
  @item Remove or reduce file attachments
  @end itemize
@end enumerate

@subsubsection History Length Management
Control context by limiting conversation history:
@example
(setq ollama-buddy-max-history-length 5)
@end example

This keeps only the last 5 message pairs, reducing context usage.

@subsection Using num_ctx Parameter

The @code{num_ctx} parameter allows you to set a specific context size:
@enumerate
@item Access the parameter menu with @kbd{C-c P}
@item Select @code{num_ctx}
@item Enter your desired context size
@item Ollama Buddy will respect this limit
@end enumerate

@section Context Display Configuration

Customize how context information is displayed:

@table @code
@item ollama-buddy-show-context-percentage
Whether to show context percentage in the status bar (default: nil).

@item ollama-buddy-context-warning-threshold
Percentage at which to warn about high context usage (default: 90).

@item ollama-buddy-context-error-threshold
Percentage at which to block sending (default: 100).
@end table

@section Fallback Context Sizes

Ollama Buddy includes predefined context sizes for popular models. You can customize these via:

@example
(setq ollama-buddy-fallback-context-sizes
  '(("llama3:8b" . 4096)
    ("codellama:7b" . 8192)
    ("mistral:7b" . 8192)))
@end example

@section Troubleshooting Context Issues

@table @asis
@item Context warnings appear unexpectedly
@itemize @bullet
@item Check if you have a long system prompt
@item Review conversation history length
@item Verify the model's actual context size
@item Check if files are attached and their sizes
@end itemize

@item Model responses are truncated
@itemize @bullet
@item Increase the @code{num_ctx} parameter
@item Reduce history length with @kbd{C-c Y}
@item Clear some conversation history
@item Remove large file attachments
@end itemize

@item Context calculations seem inaccurate
@itemize @bullet
@item Remember that token estimation is approximate
@item Different models may tokenize text differently
@item Use @kbd{C-c C} to see detailed breakdowns
@end itemize
@end table

@node File Attachments
@chapter File Attachments

@section Overview

File attachments allow you to include the contents of text files, code files, documentation, and configuration files directly in your conversations with AI models. This feature is particularly useful for:

@itemize @bullet
@item Code review and analysis
@item Documentation generation
@item Configuration file troubleshooting
@item Multi-file project discussions
@item Research with multiple text sources
@end itemize

@section Supported File Types

Ollama Buddy supports a wide range of file types by default:

@table @asis
@item Text and Documentation
@code{.txt}, @code{.md}, @code{.org}

@item Programming Languages
@code{.py}, @code{.js}, @code{.el}, @code{.cpp}, @code{.c}, @code{.java}

@item Web Technologies
@code{.html}, @code{.css}, @code{.json}, @code{.xml}

@item Configuration Files
@code{.yaml}, @code{.yml}, @code{.toml}, @code{.ini}, @code{.cfg}

@item Scripts
@code{.sh}, @code{.sql}
@end table

You can customize supported file types by modifying @code{ollama-buddy-supported-file-types}.

@section Attaching Files

@subsection Basic File Attachment

To attach a single file:

@enumerate
@item Press @code{C-c a} to open the attachment menu
@item Press @code{j} for "Attach file"
@item Select the file from the file browser
@item The file will be attached and its contents included in future prompts
@end enumerate

Alternatively, you can use @code{C-c C-a} directly.

@subsection Dired Integration

When working in Dired, you can attach files directly:

@table @asis
@item Attach file at point
Position the cursor on a file and press @code{C-c C-a}

@item Attach multiple marked files
Mark files with @code{m}, then run @code{M-x ollama-buddy-dired-attach-marked-files}
@end table

@section Managing Attachments

@subsection Viewing Attachments

To see currently attached files:
@example
M-x ollama-buddy-show-attachments
@end example
or press @code{C-c C-w}.

This opens a dedicated buffer showing:
@itemize @bullet
@item File names and paths
@item File sizes
@item File content preview
@end itemize

@subsection Detaching Files

To remove a specific file:
@example
M-x ollama-buddy-detach-file
@end example
or press @code{C-c C-d}.

You'll be prompted to select which file to detach from the list of currently attached files.

@subsection Clearing All Attachments

To remove all attached files at once:
@example
M-x ollama-buddy-clear-attachments
@end example
or press @code{C-c 0}.

@section File Size Limits

Ollama Buddy enforces file size limits to prevent overwhelming the context window:

@itemize @bullet
@item Default maximum file size: 10MB
@item Configurable via @code{ollama-buddy-max-file-size}
@item Files exceeding the limit will trigger an error
@end itemize

Example configuration:
@example
;; Set maximum file size to 5MB
(setq ollama-buddy-max-file-size (* 5 1024 1024))
@end example

@section How Attachments Work

@subsection Context Integration

When files are attached:

@enumerate
@item File contents are read and stored in memory
@item Content is included in the prompt context when sending requests
@item Token counting includes attachment content
@item Files are formatted with clear delimiters showing filename and type
@end enumerate

@subsection Session Persistence

File attachments are preserved across sessions:

@itemize @bullet
@item Saving a session (@code{C-c S}) includes all attached files
@item Loading a session (@code{C-c L}) restores attachments
@item Session files store both file paths and content
@item Attachment metadata is preserved (size, type, attachment time)
@end itemize

@section File Attachment Workflow Examples

@subsection Code Review Workflow

@enumerate
@item Attach source files using @code{C-c C-a}
@item Set a system prompt for code review: "You are an expert code reviewer"
@item Ask questions about the code: "What potential issues do you see in this code?"
@item The AI can reference all attached files in its analysis
@end enumerate

@subsection Multi-File Analysis

@enumerate
@item Use Dired to mark multiple related files
@item Attach them all with @code{M-x ollama-buddy-dired-attach-marked-files}
@item Ask for analysis: "Compare the approaches used in these files"
@item The AI can cross-reference content between files
@end enumerate

@subsection Configuration Troubleshooting

@enumerate
@item Attach configuration files (.yaml, .json, .ini)
@item Describe the issue: "This configuration isn't working as expected"
@item The AI can analyze the config and suggest fixes
@end enumerate

@section Context Considerations

File attachments impact context usage:

@itemize @bullet
@item Each attached file counts toward the total token limit
@item Large files can quickly fill available context
@item Monitor context usage with @code{C-c C} when using attachments
@item Consider detaching unnecessary files to free up context
@end itemize

@section Best Practices

@enumerate
@item Start with smaller files to avoid context issues
@item Use descriptive filenames for clarity
@item Remove attachments when no longer needed
@item Monitor context usage with large files
@item Use attachment history to avoid re-attaching the same files
@end enumerate

@section Troubleshooting Attachments

@table @asis
@item File won't attach
@itemize @bullet
@item Check if file type is supported (or override with "y")
@item Verify file size is under the limit
@item Ensure file exists and is readable
@end itemize

@item Context errors with attachments
@itemize @bullet
@item Remove some attachments with @code{C-c C-d}
@item Switch to a model with larger context
@item Reduce conversation history length
@end itemize

@item Attachments not showing in session
@itemize @bullet
@item Ensure you saved the session after attaching files
@item Check that the session file includes attachment data
@item Verify file paths are still valid when loading
@end itemize
@end table

@node Web Search
@chapter Web Search

@section Overview

Web search integration allows Ollama Buddy to fetch real-time information from the web and include it as context for your LLM conversations. This is particularly useful for:

@itemize @bullet
@item Getting current information beyond the model's training cutoff
@item Researching topics and summarizing findings
@item Fact-checking with up-to-date sources
@item Comparing information from multiple web sources
@end itemize

@section How It Works

The web search feature implements a multi-stage pipeline. Content can be retrieved using two methods, controlled by @code{ollama-buddy-web-search-content-source}:

@subsection eww Mode (Default, Recommended)

When @code{ollama-buddy-web-search-content-source} is set to @code{'eww} (the default):

@enumerate
@item @strong{Query to Ollama API}: Search queries are sent to Ollama's Web Search API (@code{https://ollama.com/api/web_search}) via REST with Bearer token authentication.

@item @strong{URL Extraction}: The API returns search results containing URLs.

@item @strong{eww/shr Processing}: Each URL is fetched and processed through Emacs' built-in eww/shr HTML renderer:
@itemize @minus
@item HTML is parsed using @code{libxml-parse-html-region}
@item Main content is extracted (looking for @code{<article>}, @code{<main>}, or content divs)
@item Content is rendered to clean plain text using @code{shr-insert-document}
@item Org-mode special characters are escaped (@code{*} and @code{#} at line starts become @code{,*} and @code{,#})
@end itemize

@item @strong{Context Attachment}: The cleaned text is formatted with org headings and attached to the conversation context.

@item @strong{LLM Submission}: The search results are included when sending prompts to any configured LLM provider.
@end enumerate

This mode produces cleaner, more complete content but requires additional HTTP requests to fetch each URL.

@subsection API Mode (Experimental)

When @code{ollama-buddy-web-search-content-source} is set to @code{'api}:

@enumerate
@item @strong{Query to Ollama API}: Search queries are sent to the Web Search API.

@item @strong{Direct Content}: Content snippets returned directly by the Ollama API are used without fetching individual URLs.

@item @strong{Context Attachment}: The API-provided content is formatted and attached to the conversation context.
@end enumerate

This mode is faster (no additional HTTP requests) but the content quality depends on what the Ollama API returns, which may be less refined than eww-rendered pages. Use this mode when speed is more important than content completeness.

@section Configuration

To use web search, you need an API key from Ollama:

@enumerate
@item Visit @url{https://ollama.com/settings/keys}
@item Create a new API key
@item Configure in Emacs:
@end enumerate

@lisp
;; Required: Set your API key
(setq ollama-buddy-web-search-api-key "your-api-key-here")

;; Optional customizations
(setq ollama-buddy-web-search-max-results 5)        ; Number of URLs to fetch (default: 5)
(setq ollama-buddy-web-search-snippet-length 2000)  ; Max chars per result (default: 500)
(setq ollama-buddy-web-search-include-urls nil)     ; Include URLs in context (default: nil)
(setq ollama-buddy-web-search-content-source 'eww)  ; Content source: 'eww (default) or 'api
@end lisp

Note: The web search module is now automatically loaded with Ollama Buddy.
You no longer need to explicitly @code{(require 'ollama-buddy-web-search)}.

@section Usage

@subsection Inline Search Syntax

The most convenient way to use web search is with inline syntax directly in your prompts:

@example
Tell me about @@search(latest emacs 30 features) and summarize the key points.
@end example

Multiple searches can be combined:

@example
Compare @@search(rust async programming) with @@search(go concurrency model)
@end example

When you send the prompt, Ollama Buddy will:
@enumerate
@item Extract all @code{@@search(query)} patterns
@item Perform each search and fetch URL contents
@item Attach results to the conversation context
@item Replace the search syntax with just the query text
@item Send the prompt with the attached web context
@end enumerate

@subsection Manual Search Commands

@table @kbd
@item C-c / s
Search and display results in a buffer (@code{ollama-buddy-web-search})

@item C-c / a
Search and attach results to conversation context (@code{ollama-buddy-web-search-attach})
@end table

@subsection Transient Menu

Access the web search menu via @code{C-c O /} which provides:

@itemize @bullet
@item Search & Display - perform search and show results
@item Search & Attach - perform search and attach to context
@item Show Attachments - view all attachments including web searches
@item Clear All - remove all attachments and web searches
@end itemize

@section Viewing Web Search Results

Web search results are integrated into the attachment system. Use @code{C-c C-w} to view all attachments, which displays both file attachments and web searches in an org-mode buffer:

@example
* Web Searches (1)

** what is the latest manchester united result
:PROPERTIES:
:RESULTS: 5
:TOKENS: ~1234
:SIZE: 5678 bytes
:TIME: 2024-01-15 10:30:00
:END:

*** 1. First Result Title
:PROPERTIES:
:URL: https://example.com/page1
:END:
#+begin_example
Full content from this URL...
#+end_example

*** 2. Second Result Title
...
@end example

The org-mode structure allows you to collapse/expand individual results.

@section Managing Web Searches

Web searches are treated as attachments:

@itemize @bullet
@item Status line shows @code{♁N} when N web searches are attached
@item @code{C-c 0} clears all attachments including web searches
@item Web search context is automatically included in prompts to all LLM providers
@end itemize

@section Provider Support

Web search works with all configured LLM providers:

@table @asis
@item Local Ollama
Models with @code{o:} prefix or no prefix

@item OpenAI
Models with @code{a:} prefix

@item Anthropic Claude
Models with @code{c:} prefix

@item Google Gemini
Models with @code{g:} prefix

@item X Grok
Models with @code{k:} prefix

@item GitHub Copilot
Models with @code{p:} prefix

@item Mistral Codestral
Models with @code{s:} prefix
@end table

@section Troubleshooting

@table @asis
@item 401 Unauthorized error
Ensure you're using an API key from @url{https://ollama.com/settings/keys}, not the token from @code{ollama signin} (cloud authentication).

@item Empty search results
Some websites may block automated requests or have content that doesn't extract well. Try different search queries.

@item Slow searches
URL fetching is asynchronous for display and attach commands. Inline searches in prompts use synchronous fetching to ensure results are ready before sending.
@end table

@node Parameter Control
@chapter Parameter Control

@section Understanding Parameters

Ollama's models support various parameters that control their behavior:

@table @asis
@item temperature
Controls randomness (0.0-1.0+), higher values produce more creative outputs

@item top_k
Limits token selection to top K most probable tokens

@item top_p
Nucleus sampling threshold (0.0-1.0)

@item repeat_penalty
Penalty for repeating tokens (higher values reduce repetition)
@end table

@section Viewing Current Parameters

To view all current parameters:
@example
M-x ollama-buddy-params-display
@end example
or press @code{C-c G} in the chat buffer.

Parameters that have been modified from default values are marked with an asterisk (*).

@section Editing Parameters

To edit parameters:

@enumerate
@item Press @code{C-c P} to open the parameter menu
@item Select the parameter you want to modify
@item Enter the new value
@end enumerate

You can also use @code{M-x ollama-buddy-params-edit} and select from a completion list.

@section Parameter Profiles

Ollama Buddy comes with predefined parameter profiles for different use cases:

@table @asis
@item Default
Standard balanced settings

@item Creative
Higher temperature, lower penalties for more creative responses

@item Precise
Lower temperature, higher penalties for more deterministic responses
@end table

To apply a profile:
@example
M-x ollama-buddy-transient-profile-menu
@end example
or press @code{C-c p} and select a profile.

@section Command-Specific Parameters

Some commands have pre-configured parameters. For example:
@itemize @bullet
@item The "Refactor Code" command uses lower temperature for more deterministic results
@item The "Creative Writing" command uses higher temperature for more varied outputs
@end itemize

These parameters are automatically applied when you use these commands and restored afterward.

@section Reset Parameters

To reset all parameters to default values:
@example
M-x ollama-buddy-params-reset
@end example
or press @code{C-c K} in the chat buffer.

@section Displaying Parameters in Header

To toggle whether modified parameters are shown in the header:
@example
M-x ollama-buddy-toggle-params-in-header
@end example
or press @code{C-c F} in the chat buffer.

@node Session Management
@chapter Session Management

@section Understanding Sessions

Sessions in Ollama Buddy allow you to:
@itemize @bullet
@item Save the entire conversation history
@item Save the current model selection
@item Restore previous conversations later
@item Switch between different conversation contexts
@end itemize

@section Creating a New Session

To start a fresh session:
@example
M-x ollama-buddy-sessions-new
@end example
or press @code{C-c N} in the chat buffer.

This will clear the current conversation history and let you start fresh.

@section Saving a Session

To save the current session:
@example
M-x ollama-buddy-sessions-save
@end example
or press @code{C-c S} in the chat buffer.

You'll be prompted to enter a name for the session.

@section Loading a Session

To load a previously saved session:
@example
M-x ollama-buddy-sessions-load
@end example
or press @code{C-c L} in the chat buffer.

You'll be presented with a list of saved sessions to choose from.

@section Managing Sessions

To open the sessions directory in Dired for direct file management:
@example
M-x ollama-buddy-sessions-directory
@end example
or press @code{C-c Z} in the chat buffer.

From Dired, you can browse, rename, delete, or otherwise manage your session files using standard Emacs file operations.

@section Session Naming

When saving a session, Ollama Buddy generates a default name from the first user message. The name is created by filtering out common English stop words (``the'', ``a'', ``is'', etc.) and short words (less than 3 characters), then joining up to 5 remaining key terms with hyphens.

For example, a conversation starting with ``Tell me about the history of space exploration'' would generate the session name @code{tell-history-space-exploration}.

@section Autosave Transcript

Ollama Buddy automatically saves the chat buffer to a recovery file after each completed response. The file is saved as @file{~autosave.org} in the sessions directory (@code{ollama-buddy-sessions-directory}).

This provides crash protection --- if Emacs exits unexpectedly, you can recover the conversation from the autosave file.

The autosave file is automatically deleted when you perform a proper session save with @code{C-c S}.

@section Conversation History

Sessions save the conversation history for each model separately.

To view the current conversation history:
@example
M-x ollama-buddy-history-edit
@end example
or press @code{C-c J} in the chat buffer.

To clear the history:
@example
M-x ollama-buddy-clear-history
@end example
or press @code{C-c X} in the chat buffer.

To toggle whether history is used in requests:
@example
M-x ollama-buddy-toggle-history
@end example
or press @code{C-c H} in the chat buffer.

@node User System Prompts
@chapter User System Prompts

@section Overview

The User System Prompts feature allows you to save, organize, and reuse effective system prompts for your conversations with AI models. This feature is particularly valuable for:

@itemize @bullet
@item Building a personal library of effective prompts
@item Maintaining context continuity across sessions
@item Sharing prompt templates with teammates
@item Refining your prompts over time
@item Categorizing prompts by domain or purpose
@end itemize

System prompts play a crucial role in guiding AI behavior and response quality. A well-crafted system prompt can dramatically improve the relevance, accuracy, and style of AI responses.

@section Accessing the System Prompts Menu

To access the system prompts menu:
@example
M-x ollama-buddy-transient-user-prompts-menu
@end example
or press @code{C-c s} in the chat buffer.

This opens a transient menu with the following options:

@table @asis
@item Save current (S)
Save your active system prompt for future reuse

@item Load prompt (L)
Select a previously saved prompt to apply

@item Create new (N)
Start fresh with a new prompt

@item List all Prompts (l)
View your entire prompt collection

@item Edit prompt (e)
Modify an existing prompt

@item Set with current prompt (s)
Set the current text as a system prompt

@item Delete prompt (d)
Remove prompts you no longer need

@item Reset prompt (r)
Clear the system prompt setting
@end table

@section Saving System Prompts

To save a system prompt:

@enumerate
@item Set a system prompt by typing it and pressing @code{C-c s s}
@item Open the system prompts menu with @code{C-c s}
@item Press @code{S} to save the current system prompt
@item Enter a category (from predefined options or create your own)
@item Enter a descriptive title for your prompt
@item The prompt will be saved to your prompts directory
@end enumerate

@section Loading Saved Prompts

To load a previously saved prompt:

@enumerate
@item Press @code{C-c s} to open the system prompts menu
@item Press @code{j} to set as system prompt
@item Select a prompt from the completion interface
@item The prompt will be loaded and set as your current system prompt
@end enumerate

Prompts are displayed in the format "@code{category: title}" for easy selection.

@section Managing Your Prompt Library

@subsection Viewing All Prompts

To view your entire prompt collection:
@example
M-x ollama-buddy-user-prompts-list
@end example
or press @code{C-c s l}.

This displays a buffer showing:
@itemize @bullet
@item Prompts organized by category
@item Prompt titles
@item Preview of prompt content
@end itemize

@subsection Editing Prompts

To edit an existing prompt:
@example
M-x ollama-buddy-user-prompts-edit
@end example
or press @code{C-c s e}.

This opens the prompt file in an Org mode buffer where you can make changes and save.

@subsection Creating New Prompts

To create a new prompt from scratch:
@example
M-x ollama-buddy-user-prompts-create-new
@end example
or press @code{C-c s N}.

This opens a template with Org headers where you can enter your prompt content.

@subsection Deleting Prompts

To delete a prompt:
@example
M-x ollama-buddy-user-prompts-delete
@end example
or press @code{C-c s d}.

You'll be asked to confirm before the prompt is deleted.

@section Categories and Organization

Prompts are organized into categories for easier management. Default categories include:

@itemize @bullet
@item general - General-purpose system prompts
@item coding - Programming-specific prompts
@item writing - Content creation and editing prompts
@item analysis - Data and research analysis prompts
@item creative - Prompts for creative tasks
@item technical - Technical documentation and explanation prompts
@item documentation - Documentation-focused prompts
@end itemize

You can customize the default categories:
@example
(setq ollama-buddy-user-prompts-default-categories
      '("general" "coding" "writing" "analysis" "creative" "custom"))
@end example

@section Prompt Storage Format

System prompts are stored as Org mode files with a specific naming convention:

@example
category__title__system.org
@end example

Each file contains:
@itemize @bullet
@item Org properties with metadata (title, category, date)
@item The full prompt content
@end itemize

Example prompt file content:
@example
#+TITLE: Python Expert
#+CATEGORY: coding
#+DATE: 2025-05-19 14:32:45

You are a Python programming expert with deep knowledge of both modern and 
legacy Python code. When analyzing or writing code:

1. Prioritize readability and maintainability over clever tricks
2. Follow PEP 8 conventions
3. Include docstrings and comments for non-obvious operations
4. Explain your thinking step-by-step
5. Provide examples when helpful

When asked to debug, first identify the likely cause before suggesting fixes.
@end example

@section Best Practices for System Prompts

@subsection Components of Effective Prompts

Well-designed system prompts typically include:

@itemize @bullet
@item Clear role definition (who/what the AI is supposed to be)
@item Guidelines for response style and format
@item Constraints or limitations to observe
@item Specific instructions for handling certain types of queries
@item Examples of desired responses (optional)
@end itemize

@subsection Example Patterns

@table @asis
@item Expert Role
"You are a [domain] expert with [X years] of experience in [specific areas]..."

@item Response Format
"Format your responses with a brief summary first, followed by detailed analysis..."

@item Specific Guidelines
"When responding to code queries, always include sample code and explain line-by-line..."

@item Thinking Process
"Think step-by-step, breaking down complex problems into smaller components..."
@end table

@section Example System Prompts

@subsection Technical Writing Assistant

@example
You are a technical writing expert who specializes in creating clear, concise, 
and accessible documentation. Your writing should:

1. Use plain language and avoid jargon where possible
2. Include appropriate headings and structural elements
3. Provide concrete examples that illustrate complex concepts
4. Use active voice and direct instructions for procedures
5. Anticipate common user questions and address them proactively

When presented with technical content, focus on making it understandable to 
the target audience while preserving technical accuracy.
@end example

@subsection Code Reviewer

@example
You are an experienced code reviewer with expertise in software engineering 
best practices. When reviewing code:

1. Identify potential bugs, edge cases, and performance issues
2. Suggest improvements to readability and maintainability
3. Highlight security vulnerabilities or potential risks
4. Reference design patterns or library functions that could improve the implementation
5. Provide specific, actionable feedback with examples

Balance constructive criticism with acknowledgment of well-written code.
@end example

@section Workflow Examples

@subsection Python Code Assistance

@enumerate
@item Load your "Python Expert" system prompt with @code{C-c s L}
@item Ask coding questions or paste code for analysis
@item The AI responds with Python-specific expertise
@item Save the conversation as a session for future reference
@end enumerate

@subsection Technical Writing Help

@enumerate
@item Create a new system prompt for technical writing (@code{C-c s N})
@item Define the AI's role as a technical writing assistant
@item Save the prompt in the "writing" category
@item Load this prompt whenever you need help with documentation
@item The AI consistently provides responses optimized for technical writing
@end enumerate

@section Integration with Roles

System prompts can be integrated with Ollama Buddy roles for more specialized workflows:

@enumerate
@item Create a system prompt for a specific purpose
@item Test and refine it through direct interaction
@item Once effective, save it to your prompt library
@item Reference this prompt in a custom role definition
@end enumerate

This creates a reusable AI assistant configuration that can be shared and improved over time.

@node Roles and Commands
@chapter Roles and Commands

@section Understanding Roles

Roles in Ollama Buddy are collections of commands with specific configurations:
@itemize @bullet
@item Each role has its own set of commands
@item Commands can use specific models
@item Commands can have specialized system prompts
@item Commands can have specialized parameters
@end itemize

This allows you to create specialized assistants for different workflows.

@section Role File Naming Convention

The file naming convention is critical to understand how roles, preset files, and menu configurations work together:

@table @asis
@item Required filename format
@code{ollama-buddy--preset__ROLE-NAME.el}
@itemize @bullet
@item The double underscore @code{__} separates the prefix from your role name
@item The role name portion becomes the identifier shown when switching roles
@item Example: @code{ollama-buddy--preset__programmer.el} creates a role named "programmer"
@end itemize
@end table

This naming convention is how Ollama Buddy discovers and identifies role files. When you run @code{ollama-buddy-roles-switch-role}, the system:

@enumerate
@item Scans the @code{ollama-buddy-roles-directory} for files matching the pattern
@item Extracts the role name from each filename (the part after @code{__})
@item Presents these names in the role selection interface
@item When selected, loads the corresponding file which redefines @code{ollama-buddy-command-definitions}
@item This redefinition immediately changes the available commands in your Ollama Buddy menu
@end enumerate

The relationship chain works like this:
@example
ollama-buddy--preset__ROLE-NAME.el → Defines ollama-buddy-command-definitions → Controls menu content
@end example

When creating roles using the interactive role creator (@code{C-c E}), this naming convention is automatically handled for you. When creating roles manually, you must follow this pattern for Ollama Buddy to recognize your role files correctly.

@section Built-in Commands

Ollama Buddy comes with several built-in commands:

@table @asis
@item refactor-code
Improves code while maintaining functionality

@item describe-code
Explains what code does and how it works

@item git-commit
Generates meaningful commit messages

@item dictionary-lookup
Provides comprehensive word definitions

@item synonym
Suggests alternative words with context

@item proofread
Corrects grammar, style, and spelling
@end table

@section Creating Custom Roles

There are two ways to create custom roles:

@subsection Interactive Role Creator

The most user-friendly approach:

@enumerate
@item Press @code{C-c E} or run @code{M-x ollama-buddy-role-creator-create-new-role}
@item Enter a name for your role (e.g., "programmer")
@item Enter the number of menu columns (default: 2)
@item For each command you want to add:
  @itemize @bullet
  @item Specify a command name (e.g., "refactor-code")
  @item Choose a key shortcut for the menu
  @item Add a description
  @item Optionally specify a model
  @item Optionally add prompt prefixes and system messages
  @end itemize
@end enumerate

The interactive creator automatically handles file naming and placement.

@subsection Manual Role Creation

For more advanced customization, create role files manually:

@enumerate
@item Create a file named @code{ollama-buddy--preset__your-role-name.el} in your @code{ollama-buddy-roles-directory}
@item Structure your file like this:
@end enumerate

@example
;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands
    (open-chat
     :key ?o
     :description "Open chat buffer"
     :group "General"
     :action ollama-buddy--open-chat)

    (switch-role
     :key ?R
     :description "Switch roles"
     :group "General"
     :action ollama-buddy-roles-switch-role)

    ;; Custom commands for this role
    (refactor-code
     :key ?r
     :description "Refactor code"
     :group "Code"
     :model "codellama:7b"
     :prompt "Refactor this code to improve readability and efficiency:"
     :system "You are an expert software engineer who improves code quality."
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))

    (explain-code
     :key ?e
     :description "Explain code"
     :group "Code"
     :model "deepseek-r1:7b"
     :prompt "Explain what this code does in detail:"
     :system "You are a programming teacher who explains code clearly."
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))

    (git-commit
     :key ?g
     :description "Git commit message"
     :group "Utility"
     :prompt "Write a concise git commit message for these changes:"
     :system "You are a version control expert who creates clear commit messages."
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))

    ;; System commands
    (custom-prompt
     :key ?e
     :description "Custom prompt"
     :group "System"
     :action ollama-buddy--menu-custom-prompt)
    ))
@end example

The @code{:group} property is optional. Commands sharing the same @code{:group} value appear together in a named column in the transient role menu (@code{ollama-buddy-role-transient-menu}). Presets without any @code{:group} properties still work --- all commands appear in a single ``Commands'' column for full backwards compatibility.

@section Command Grouping

The @code{:group} property on command definitions controls how commands are organized into columns in the transient role menu. This provides a clean, discoverable layout:

@example
;; Commands with :group "Code" appear in a "Code" column
(refactor-code
 :key ?r
 :description "Refactor code"
 :group "Code"
 :action ...)

(explain-code
 :key ?e
 :description "Explain code"
 :group "Code"
 :action ...)
@end example

Built-in presets use groups like ``General'', ``Code Analysis'', ``Code Generation'', ``Custom'', ``System'', etc. You can use any group names you like in your own presets.

@section Switching Roles

To switch between roles:
@example
M-x ollama-buddy-roles-switch-role
@end example
or press @code{C-c R} in the chat buffer.

You'll be presented with a list of available roles to choose from.

@section Managing Role Files

Roles are stored as Elisp files in the @code{ollama-buddy-roles-directory}.

To locate your roles directory:
@example
;; Check where your roles are stored
(message ollama-buddy-roles-directory)
@end example

By default, this is set to @code{~/.emacs.d/ollama-buddy-presets/}, but you can customize it:
@example
(setq ollama-buddy-roles-directory "/your/custom/path/to/presets")
@end example

To open this directory:
@example
M-x ollama-buddy-roles-open-directory
@end example
or press @code{C-c D} in the chat buffer.

@section Advanced Role Customization

@subsection Per-Role Menu Columns

Each role can define its own menu column layout by setting @code{ollama-buddy-menu-columns} in the role file. This allows roles with many commands to use more columns, while simpler roles can use fewer:

@example
;; In your role preset file:
(setq ollama-buddy-menu-columns 3)  ; Use 3 columns for this role
@end example

When you switch roles, the menu column setting changes accordingly. The menu display automatically adjusts its height to ensure all items are visible, regardless of the number of rows.

@subsection Command-Specific Models

Assign specific models to commands for optimal performance:

@example
(ollama-buddy-add-model-to-menu-entry 'refactor-code "codellama:7b")
@end example

@subsection Command-Specific Parameters

Optimize parameters for specific commands:

@example
(ollama-buddy-add-parameters-to-command 'refactor-code
  'temperature 0.2
  'top_p 0.7
  'repeat_penalty 1.3)
@end example

@subsection Creating New Commands

Add entirely new commands to your menu:

@example
(ollama-buddy-update-menu-entry 'my-new-command
  :key ?z
  :description "My new awesome command"
  :prompt "Here is what I want you to do:"
  :system "You are an expert system specialized in this task."
  :action (lambda () (ollama-buddy--send-with-command 'my-new-command)))
@end example

@section Role Examples

@subsection Programming Role

A complete example of a programming-focused role:

@example
;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
    (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)
    
    ;; Programming-specific commands
    (refactor-code
     :key ?r
     :description "Refactor code"
     :model "codellama:7b"
     :prompt "Refactor this code to improve readability and efficiency:"
     :system "You are an expert software engineer who improves code quality."
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description "Explain code"
     :model "deepseek-r1:7b"
     :prompt "Explain what this code does in detail:"
     :system "You are a programming teacher who explains code clearly."
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (add-tests
     :key ?t
     :description "Generate tests"
     :model "qwen2.5-coder:7b"
     :prompt "Generate unit tests for this code:"
     :system "You are a test automation expert who creates comprehensive test cases."
     :action (lambda () (ollama-buddy--send-with-command 'add-tests)))
    
    (git-commit
     :key ?g
     :description "Git commit message"
     :prompt "Write a concise git commit message for these changes:"
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
@end example

@subsection Writing Role

A complete example of a writing-focused role:

@example
;; ollama-buddy preset for role: writer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
    (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)
    
    ;; Writing-focused commands
    (summarize
     :key ?s
     :description "Summarize text"
     :prompt "Summarize the following text in a concise manner:"
     :system "You are an expert at extracting the key points from any text."
     :action (lambda () (ollama-buddy--send-with-command 'summarize)))
    
    (proofread
     :key ?p
     :description "Proofread text"
     :model "deepseek-r1:7b"
     :prompt "Proofread the following text and correct any errors:"
     :system "You are a professional editor who identifies and corrects grammar and style errors."
     :action (lambda () (ollama-buddy--send-with-command 'proofread)))
    
    (rewrite
     :key ?r
     :description "Rewrite text"
     :prompt "Rewrite the following text to improve clarity and flow:"
     :system "You are a skilled writer who can improve any text while preserving its meaning."
     :action (lambda () (ollama-buddy--send-with-command 'rewrite)))
    
    (brainstorm
     :key ?b
     :description "Brainstorm ideas"
     :model "llama3.2:3b"
     :prompt "Generate creative ideas related to the following topic:"
     :parameters ((temperature . 1.0) (top_p . 0.95))
     :action (lambda () (ollama-buddy--send-with-command 'brainstorm)))
    ))
@end example

@section Tips for Effective Role Usage

@enumerate
@item Group related commands: Create roles around specific workflows or tasks
@item Match models to tasks: Use lightweight models for simple tasks and more powerful models for complex ones
@item Customize system prompts: Craft specific system prompts to guide the model for each command
@item Use the roles directory: Press @code{C-c D} to quickly access and manage your role files
@item Create specialized roles: Consider roles for programming, writing, translation, or domain-specific knowledge
@end enumerate

@section Built-in Role Presets

Ollama Buddy comes with several pre-configured role presets that you can use immediately or customize:

@table @strong
@item default
The standard preset with general-purpose commands for chat, code operations, and text manipulation.

@item developer
Focused on programming tasks including code explanation, refactoring, bug fixing, and code review.

@item emacs
Specialized for Emacs Lisp development with commands for Elisp-specific help, package development, and Emacs configuration.

@item writer
Creative writing focused with commands for storytelling, poetry, copywriting, and content creation.

@item translator
Multi-language translation support with formal/informal tone options and localization assistance.

@item tutor
Educational assistant for learning and understanding concepts. Features include:
@itemize @bullet
@item Multiple explanation levels (simple, in-depth, ELI5)
@item Quiz and practice problem generation
@item Prerequisite and related topic exploration
@item Common mistakes identification
@item Emacs Info documentation integration with completing-read for browsing info nodes
@end itemize

@item documenter
Technical documentation assistant for creating professional documentation:
@itemize @bullet
@item README and quick start guide generation
@item Architecture documentation
@item API documentation and docstring generation
@item Changelog and release notes writing
@item FAQ and troubleshooting guide creation
@end itemize

@item bard
Shakespearean-style responses with dramatic flair and poetic language.

@item buffy
Witty, pop-culture-savvy responses inspired by Buffy Summers.

@item janeway
Diplomatic and thoughtful responses inspired by Captain Janeway.
@end table

To switch between these presets, use @code{M-x ollama-buddy-roles-switch-role} or press @code{R} in the Ollama Buddy menu.

@node Fabric Pattern Integration
@chapter Fabric Pattern Integration

@section What are Fabric Patterns?

Fabric patterns are pre-defined prompt templates from Daniel Miessler's Fabric project (@url{https://github.com/danielmiessler/fabric}). They provide optimized prompts for various tasks, categorized as:

@itemize @bullet
@item universal - General-purpose patterns
@item code - Programming and development
@item writing - Content creation and editing
@item analysis - Data and concept examination
@end itemize

@section Setting Up Fabric Integration

To set up Fabric integration:
@example
M-x ollama-buddy-fabric-setup
@end example

This will:
@enumerate
@item Clone the Fabric repository (or set up sparse checkout)
@item Populate available patterns
@item Make patterns available for use
@end enumerate

@section Using Fabric Patterns

To use a Fabric pattern:
@example
M-x ollama-buddy-fabric-send
@end example
or press @code{C-c f} and then @code{s}.

You'll be prompted to:
@enumerate
@item Select a pattern
@item Enter text to process (or use selected text)
@end enumerate

The pattern will be used as a system prompt for your request.

@section Browsing Available Patterns

To see all available patterns:
@example
M-x ollama-buddy-fabric-list-patterns
@end example
or press @code{C-c f} and then @code{l}.

This shows:
@itemize @bullet
@item Pattern names
@item Categories
@item Descriptions
@end itemize

@section Viewing Pattern Details

To see the full content of a specific pattern:
@example
M-x ollama-buddy-fabric-show-pattern
@end example
or press @code{C-c f} and then @code{v}.

Select a pattern to see:
@itemize @bullet
@item The system prompt content
@item Full description
@end itemize

@section Updating Patterns

To sync with the latest patterns from GitHub:
@example
M-x ollama-buddy-fabric-sync-patterns
@end example
or press @code{C-c f} and then @code{S}.

@section Using Patterns by Category

You can quickly access patterns by category:
@itemize @bullet
@item @code{C-c f u} - Universal patterns
@item @code{C-c f c} - Code patterns
@item @code{C-c f w} - Writing patterns
@item @code{C-c f a} - Analysis patterns
@end itemize

@node Awesome ChatGPT Prompts
@chapter Awesome ChatGPT Prompts

@section What is Awesome ChatGPT Prompts?

Awesome ChatGPT Prompts is a curated collection of prompt templates created by the community and maintained in the GitHub repository at @url{https://github.com/f/awesome-chatgpt-prompts}. These prompts are designed to make ChatGPT (and other LLMs) act as various specialized personas or experts, such as:

@itemize @bullet
@item Writing professionals (poets, storytellers, copywriters)
@item Technical experts (programmers, researchers, scientists)
@item Creative professionals (artists, designers, photographers)
@item Business experts (marketers, consultants, strategists)
@item And many more specialized roles
@end itemize

@section Setting Up Awesome ChatGPT Prompts

To set up the Awesome ChatGPT Prompts integration:
@example
M-x ollama-buddy-awesome-setup
@end example

This will:
@enumerate
@item Create a sparse checkout of the Awesome ChatGPT Prompts repository
@item Download only the necessary files (prompts.csv and README)
@item Populate and categorize the available prompts
@end enumerate

@section Using Awesome ChatGPT Prompts

To use an Awesome ChatGPT Prompt:
@example
M-x ollama-buddy-awesome-send
@end example
or press @code{C-c w} and then @code{s}.

You'll be prompted to:
@enumerate
@item Select a prompt from the categorized list
@item Enter text to process (or use selected text)
@end enumerate

The selected prompt will be used as a system prompt for your request, transforming how the AI responds to your text.

@section Browsing Available Prompts

To see all available prompts:
@example
M-x ollama-buddy-awesome-list-prompts
@end example
or press @code{C-c w} and then @code{l}.

This shows:
@itemize @bullet
@item Prompt titles
@item Categories
@item Preview of prompt content
@end itemize

@section Categorized Browsing

Ollama Buddy automatically categorizes the Awesome ChatGPT Prompts into useful groups:
@itemize @bullet
@item writing - For writing, poetry, and creative content
@item code - For programming and development
@item business - For marketing, entrepreneurship, and business strategy
@item academic - For educational and research content
@item creative - For artistic and design-related prompts
@item philosophy - For philosophical reasoning and ethics
@item health - For medical, fitness, and wellness
@item legal - For law-related prompts
@item finance - For financial advice and analysis
@item other - Miscellaneous prompts
@end itemize

To browse by category:
@example
M-x ollama-buddy-awesome-show-prompts-menu
@end example
or press @code{C-c w} and then @code{c}.

@section Viewing Prompt Details

To see the full content of a specific prompt:
@example
M-x ollama-buddy-awesome-show-prompt
@end example
or press @code{C-c w} and then @code{v}.

Select a prompt to see its complete template.

@section Updating Prompts

To sync with the latest prompts from GitHub:
@example
M-x ollama-buddy-awesome-sync-prompts
@end example
or press @code{C-c w} and then @code{S}.

@section Setting Without Sending

To set a prompt as the system prompt without sending text:
@example
M-x ollama-buddy-awesome-set-system-prompt
@end example
or press @code{C-c w} and then @code{p}.

This is useful when you want to set up a specific persona before starting a conversation.

@section Example Usage

Some popular prompts include:
@itemize @bullet
@item "Act as a poet" - Transforms your text into poetry
@item "Act as a Linux terminal" - Simulates a Linux terminal interface
@item "Act as a gaslighter" - Responds in a deliberately confusing manner
@item "Act as a javascript console" - Simulates a JavaScript console
@item "Act as an English translator" - Translates text to proper English
@end itemize

@node Remote Providers
@chapter Remote Providers

@section Overview

Ollama Buddy integrates with multiple commercial AI services, each loaded on-demand:

@table @asis
@item OpenAI (prefix: @code{a:})
Load with @code{(require 'ollama-buddy-openai nil t)}

@item Claude/Anthropic (prefix: @code{c:})
Load with @code{(require 'ollama-buddy-claude nil t)}

@item Google Gemini (prefix: @code{g:})
Load with @code{(require 'ollama-buddy-gemini nil t)}

@item Grok/X (prefix: @code{k:})
Load with @code{(require 'ollama-buddy-grok nil t)}

@item GitHub Copilot (prefix: @code{p:})
Load with @code{(require 'ollama-buddy-copilot nil t)}

@item Codestral/Mistral (prefix: @code{s:})
Load with @code{(require 'ollama-buddy-codestral nil t)}
@end table

Local Ollama models use the @code{o:} prefix when remote models are available, or no prefix when only local models are present.

@section Status Line Indicators

The header line shows compact status indicators:

@itemize @bullet
@item @code{x} - Streaming disabled
@item Context usage bar (when enabled)
@item @code{<} - Global system prompt disabled
@item @code{H5/10} - History count (when enabled via @code{ollama-buddy-show-history-indicator})
@item @code{☁} - Currently using an Ollama cloud model
@item @code{⚒} - Current model supports tool calling
@item @code{≡N} - Number of file attachments
@item @code{♁N} - Number of web search results attached
@item @code{V} - Reasoning/thinking sections hidden
@item @code{T} - Token statistics display enabled
@end itemize

Example: @code{☁⚒≡2♁1} means cloud model with tool support, 2 file attachments and 1 web search attached.

@section Setting Up API Access

Before using commercial APIs, you need to set up API keys. The recommended approach is to use Emacs' built-in auth-source:

@example
;; Add to ~/.authinfo.gpg (encrypted)
machine ollama-buddy-openai login apikey password YOUR_OPENAI_API_KEY
machine ollama-buddy-claude login apikey password YOUR_CLAUDE_API_KEY
machine ollama-buddy-gemini login apikey password YOUR_GEMINI_API_KEY
machine ollama-buddy-grok login apikey password YOUR_GROK_API_KEY
machine ollama-buddy-copilot login apikey password YOUR_GITHUB_TOKEN
machine ollama-buddy-codestral login apikey password YOUR_CODESTRAL_API_KEY
@end example

Then configure with auth-source:
@example
(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
@end example

@section Selecting Remote Models

All remote models appear in the model selection list with their provider prefix:
@example
M-x ollama-buddy--swap-model
@end example
or press @code{C-c m}.

Models will appear like: @code{a:gpt-4}, @code{c:claude-3-opus}, @code{g:gemini-pro}, etc.

@section Provider Configuration

@subsection OpenAI Configuration
@table @code
@item ollama-buddy-openai-api-key
Your OpenAI API key.

@item ollama-buddy-openai-default-model
Default OpenAI model to use (e.g., "gpt-4").

@item ollama-buddy-openai-temperature
Default temperature for OpenAI requests (0.0-2.0).

@item ollama-buddy-openai-max-tokens
Maximum tokens to generate (nil for API default).
@end table

@subsection Claude Configuration
@table @code
@item ollama-buddy-claude-api-key
Your Anthropic Claude API key.

@item ollama-buddy-claude-default-model
Default Claude model to use.

@item ollama-buddy-claude-temperature
Default temperature for Claude requests (0.0-1.0).

@item ollama-buddy-claude-max-tokens
Maximum tokens to generate.
@end table

@subsection Gemini Configuration
@table @code
@item ollama-buddy-gemini-api-key
Your Google Gemini API key.

@item ollama-buddy-gemini-default-model
Default Gemini model to use.
@end table

@subsection Grok Configuration
@table @code
@item ollama-buddy-grok-api-key
Your X/Grok API key.

@item ollama-buddy-grok-default-model
Default Grok model to use.
@end table

@subsection GitHub Copilot Configuration
@table @code
@item ollama-buddy-copilot-api-key
Your GitHub personal access token with Copilot scope.
Get your token from @url{https://github.com/settings/tokens}.

@item ollama-buddy-copilot-default-model
Default Copilot model to use (e.g., "gpt-4o").

@item ollama-buddy-copilot-available-models
List of models available through GitHub Copilot.
@end table

Note: GitHub Copilot requires an active Copilot subscription.

@subsection Codestral Configuration
@table @code
@item ollama-buddy-codestral-api-key
Your Mistral Codestral API key.

@item ollama-buddy-codestral-default-model
Default Codestral model to use.
@end table

@section History Management

Each provider maintains its own conversation history, ensuring context is preserved appropriately per service.

@node Ollama Cloud Models
@chapter Ollama Cloud Models

@section Overview

Ollama cloud models run on ollama.com infrastructure and provide access to large models without local hardware requirements. These models require authentication via the Ollama CLI.

@section Available Cloud Models

Cloud models are configured via @code{ollama-buddy-cloud-models}:

@example
(setq ollama-buddy-cloud-models
  '("qwen3-coder:480b-cloud"
    "kimi-k2.5:cloud"
    "deepseek-v3.1:671b-cloud"
    "gpt-oss:120b-cloud"
    "gpt-oss:20b-cloud"
    "glm-4.7:cloud"
    "minimax-m2.1:cloud"))
@end example

Cloud models have a @code{-cloud} suffix in their names.

@section Selecting Cloud Models

To select a cloud model:
@itemize @bullet
@item Use @code{C-c m} and search for the cloud model name (marked with @code{☁})
@item Select from the @code{☁ Cloud Models} section in Model Management (@code{C-c M})
@end itemize

Cloud models are displayed with a @code{cl:} prefix (e.g.@: @code{cl:deepseek-v3.1:671b-cloud})
to distinguish them from local models.  The Model Management page also shows the
current cloud authentication status next to the section heading.

@section Cloud Authentication

Authentication is handled via the Ollama CLI:

@table @asis
@item Sign In (@code{M-x ollama-buddy-cloud-signin})
Opens your browser for Ollama cloud login. Accessible from transient menu "Cloud Auth > Sign In".

@item Sign Out (@code{M-x ollama-buddy-cloud-signout})
Signs out from cloud services. Accessible from transient menu "Cloud Auth > Sign Out".

@item Check Status (@code{M-x ollama-buddy-cloud-status})
Verifies your authentication status. Accessible from transient menu "Cloud Auth > Auth Status".
@end table

@section Automatic Manifest Pulling

Cloud models require a small manifest to be pulled locally before they can be used.
Ollama Buddy handles this automatically: when you select a cloud model for the first
time, the manifest is fetched via @code{ollama pull} before your prompt is sent.
You will see a brief ``Pulling cloud model manifest...'' message during this process.

This works with both the network-process and curl communication backends.
If the pull fails (e.g.@: due to authentication issues), an error is displayed
and the request is not sent.

@section Cloud Model Indicator

When using a cloud model, the header line displays a @code{☁} symbol to indicate cloud model usage.

@section Configuration

@table @code
@item ollama-buddy-cloud-models
List of available Ollama cloud models.

@item ollama-buddy-ollama-executable
Path to the ollama CLI executable (default: "ollama"). Used for signin/signout commands.
@end table

@node Global System Prompt
@chapter Global System Prompt

@section Overview

The global system prompt provides baseline formatting instructions that are automatically prepended to all requests, regardless of the session-specific system prompt. This ensures consistent response formatting across all interactions.

@section Configuration

@table @code
@item ollama-buddy-global-system-prompt
The global prompt content prepended to all requests.
@example
(setq ollama-buddy-global-system-prompt
  "Format responses in plain prose. Avoid markdown tables unless
specifically requested. Use clear paragraphs and bullet points
for structured information.")
@end example

@item ollama-buddy-global-system-prompt-enabled
Whether to enable the global system prompt (default: t).
@example
(setq ollama-buddy-global-system-prompt-enabled t)
@end example
@end table

@section Toggling Global Prompt

To toggle the global system prompt on or off:
@example
M-x ollama-buddy-toggle-global-system-prompt
@end example
or press @code{C-c <} in the chat buffer. Also available from the transient menu under "Display Toggle".

@section How It Works

When enabled, the global system prompt is combined with any session-specific system prompt:

@itemize @bullet
@item If both global and session prompts exist, they are combined with two newlines between them
@item If only the global prompt exists, it is used alone
@item If only a session prompt exists, it is used alone
@item If neither exists, no system prompt is sent
@end itemize

This allows you to maintain consistent formatting while still using specialized prompts for specific tasks.

@node Tone Selector
@chapter Tone Selector

@section Overview

The tone selector lets you quickly switch response style without manually editing system prompts. It provides a set of predefined tones (Normal, Concise, Learning, Explanatory, Formal) that modify how the LLM responds.

@section Usage

To select a tone:
@example
M-x ollama-buddy-set-tone
@end example
or press @code{C-c ~} in the chat buffer. Also available from the transient menu under ``Display Toggle'' with the @code{~} key.

@section Built-in Tones

@table @strong
@item Normal
No modification (default). The LLM responds with its standard style.
@item Concise
Short, focused answers without unnecessary elaboration.
@item Learning
Thorough explanations with context, examples and analogies.
@item Explanatory
Detailed step-by-step reasoning for complex topics.
@item Formal
Professional, precise and structured responses.
@end table

@section Configuration

@table @code
@item ollama-buddy-tone-alist
Alist mapping tone names to prompt modifier strings. Add custom tones:
@example
(add-to-list 'ollama-buddy-tone-alist
  '("Creative" . "Be creative and imaginative in your responses."))
@end example

@item ollama-buddy--current-tone
The currently active tone name (default: @code{"Normal"}).
@end table

@section How It Works

The selected tone text is prepended to the effective system prompt. The combination order is: tone + global prompt + session prompt. When the tone is ``Normal'' (empty string), no tone text is added.

The header line shows a @code{~X} indicator when a non-Normal tone is active, where @code{X} is the first letter of the tone name (e.g.@: @code{~C} for Concise, @code{~L} for Learning).

@node Thinking Blocks
@chapter Thinking Blocks

@section Overview

Reasoning models (such as DeepSeek-R1, QwQ, and others) produce an internal chain-of-thought before giving their final answer. Ollama Buddy renders this reasoning content as a native org-mode heading that folds automatically, so the thinking process is available for inspection but does not clutter the conversation.

@section How It Works

When a model emits @code{<think>} tags or uses the DeepSeek @code{message.thinking} API field, Ollama Buddy:

@enumerate
@item Inserts a @code{*** ✦ Think} org heading in the chat buffer.
@item Streams the reasoning content under that heading in real time.
@item When reasoning ends, folds the heading with @code{outline-hide-subtree} and inserts a @code{*** ✦ Response} heading to mark the start of the actual answer.
@end enumerate

The @code{✦} symbol appears in the header line and prompt line whenever the active model supports thinking/reasoning.

@section Key Bindings

@table @kbd
@item C-c V
Toggle all thinking blocks in the buffer --- expands any that are folded, or folds all if they are all expanded.

@item TAB
On a @code{*** ✦ Think} heading line, cycle visibility of that individual block (standard org-mode behavior).
@end table

@section Configuration

@table @asis
@item @code{ollama-buddy-collapse-thinking}
When non-nil (default @code{t}), thinking content is automatically collapsed to the @code{*** ✦ Think} heading after the reasoning block ends.

@item @code{ollama-buddy-hide-reasoning}
When non-nil and @code{ollama-buddy-collapse-thinking} is nil, the thinking block is hidden entirely (replaced by ``Thinking@dots{}'') rather than kept in the buffer.

@item @code{ollama-buddy-thinking-models}
Static list of model name substrings that are known reasoning models. Auto-detection from the Ollama @code{/api/show} capabilities array is also performed.
@end table

@node In-Buffer Replace
@chapter In-Buffer Replace

@section Overview

In-buffer replace streams LLM output directly into the source buffer rather than the
chat buffer.  When active, commands that operate on a selected region (Refactor, Proofread,
Git Commit, Dictionary Lookup, Synonym, Describe Code, or any custom role command) replace
the selection in-place with the model's response, shown with a green highlight pending
your decision.

The existing chat buffer workflow is completely unchanged when the toggle is off (the
default).  All existing commands, prompts, system prompts, and model selection logic are
unaffected --- only the output destination changes.

@section Enabling In-Buffer Replace

Toggle via the transient Actions menu (@kbd{C-c O}, key @kbd{W}), the chat buffer
keybinding @kbd{C-c W}, or:

@example
M-x ollama-buddy-toggle-in-buffer-replace
@end example

The @code{✎} symbol appears in the header line and prompt line while the mode is active.
The custom role menu footer shows a live @samp{In-Buffer Replace [ON] / [OFF]} toggle.

@section Basic Workflow

@enumerate
@item Toggle in-buffer replace on (@kbd{W} or @kbd{C-c W}).
@item In any source buffer, select a region of text (e.g.@: a function body, a paragraph).
@item Invoke a role command (e.g.@: Refactor from the Developer role).
@item A dimmed @samp{[Rewriting...]} placeholder replaces the selection while connecting.
@item The model's response streams in with a green highlight.
@item When streaming finishes, the mode line shows @samp{ [Rewrite?]}.
@item Press @kbd{C-c C-c} to @strong{accept} (keep new text, remove highlight).
@item Press @kbd{C-c C-k} to @strong{reject} (discard new text and restore the original).
@end enumerate

Cancelling mid-stream with @kbd{C-c C-k} stops the network process and restores the
original selection immediately.

@section Inline Diff View

After streaming completes, press @kbd{C-c d} to open an inline diff view inside the
same buffer:

@example
[new text — green background]
  [changed words — brighter green overlay]

── Original ────────────────────────────
[original text — yellow background]
  [removed words — red strikethrough overlay]
────────────────────────────────────────
@end example

Word-level differences are highlighted by @code{smerge-refine-regions}: green overlays
mark added or changed words in the new text; red strikethrough overlays mark words present
in the original but absent from the rewrite.

Press @kbd{C-c d} again to hide the diff block without accepting or rejecting.
Accepting (@kbd{C-c C-c}) or rejecting (@kbd{C-c C-k}) automatically removes the diff
block.

@section Key Bindings (while rewrite is pending)

@table @kbd
@item C-c C-c
Accept the rewrite --- keep the streamed text and remove the highlight.

@item C-c C-k
Reject the rewrite --- stop streaming (if still in progress) and restore the original
selected text.

@item C-c d
Toggle the inline diff view.  Shows the original text below the new text with
word-level smerge highlighting.  Press again to hide.
@end table

@section Automatic Output Polishing

Two automatic adjustments help keep in-buffer results clean:

@itemize @bullet
@item @strong{Clean-output tone} --- when in-buffer replace is active a
``Return only the requested content'' instruction is automatically prepended to the
effective system prompt, suppressing introductory phrases and commentary.

@item @strong{Code-fence stripping} --- if the model wraps its response in markdown code
fences (@samp{```lang @dots{} ```}), the fences are stripped before insertion, leaving
only the inner content.
@end itemize

@section Implementation Notes

The feature is implemented in @file{ollama-buddy-rewrite.el}, loaded optionally:

@example
(require 'ollama-buddy-rewrite nil t)
@end example

The module creates a direct @code{make-network-process} connection to Ollama
(@code{/api/chat}) and streams JSON responses using the same line-delimited format as the
main chat buffer filter.  Two Emacs markers track the rewrite region:

@itemize @bullet
@item A @emph{non-advancing} start marker (type @code{nil}) --- stays before inserted text.
@item An @emph{advancing} end marker (type @code{t} during streaming, switched to
@code{nil} on completion) --- moves past each newly inserted chunk, then fixed so the
diff block insertion does not displace it.
@end itemize

The inline diff block is inserted as real buffer text at the fixed end marker position.
@code{smerge-refine-regions} is called with both the original block and the new text
residing in the same buffer, which is the standard within-buffer usage of that function.
Original text is preserved in the rewrite state plist for rejection; no undo history
manipulation is required.

@section Limitations

@itemize @bullet
@item Only works with local Ollama models (not remote providers).
@item Only one rewrite can be pending at a time.
@item The streaming response is inserted verbatim; no markdown-to-org conversion
is performed (unlike the chat buffer) --- the fence stripper handles the common
code-block case.
@end itemize

@node Tool Calling
@chapter Tool Calling

@section Overview

Tool calling (also known as function calling) enables LLMs to invoke Emacs functions during conversations. Instead of just generating text, a tool-capable model can read files, list directories, search buffers, perform calculations, and execute shell commands --- all orchestrated automatically within the conversation flow.

The tool calling framework is provided by @code{ollama-buddy-tools.el}.

@section Setup

@example
;; Load the tools module
(require 'ollama-buddy-tools)

;; Enable tool calling
(setq ollama-buddy-tools-enabled t)
@end example

You must use a model that supports tool calling. Tool-capable models are indicated with a @code{⚒} symbol in the header line, model selection, and model management buffer.

@section Built-in Tools

@subsection Safe Tools (available in safe mode)

@table @code
@item read_file
Read the contents of a file from the filesystem.

@item list_directory
List the contents of a directory.

@item get_buffer_content
Get the content of an Emacs buffer by name.

@item list_buffers
List all open Emacs buffers, with optional regex filtering.

@item calculate
Evaluate a mathematical expression using Emacs @code{calc-eval}.

@item search_buffer
Search for a regex pattern in a buffer and return matching lines.
@end table

@subsection Unsafe Tools (require safe mode off)

@table @code
@item write_file
Write content to a file on the filesystem.

@item execute_shell
Execute a shell command and return its output.
@end table

@section Example Prompts

With a tool-capable model selected and tools enabled, try prompts like:

@example
What files are in my ~/Documents directory?

Read the file ~/.emacs.d/init.el and summarize what packages I'm using.

What is 2^64 - 1?

What buffers do I have open?

Search for all defun definitions in the buffer ollama-buddy-tools.el
@end example

For unsafe tools (with safe mode disabled):

@example
Run df -h and tell me how much disk space I have.

Create a file at /tmp/hello.txt with Hello World.
@end example

@section How It Works

When tool calling is enabled and a tool-capable model is selected, the conversation flow works as follows:

@enumerate
@item You send a prompt (e.g., ``What files are in my home directory?'').
@item Ollama Buddy sends the prompt along with the tool schemas to the model.
@item The model decides whether to call a tool. If so, it returns a tool call request instead of text.
@item Ollama Buddy executes the requested tool (e.g., @code{list_directory} with path @code{~}).
@item The tool result is sent back to the model as a @code{tool} role message.
@item The model generates a natural language response incorporating the tool output.
@item Steps 3--6 may repeat if the model needs multiple tools, up to @code{ollama-buddy-tools-max-iterations}.
@end enumerate

This loop is transparent to the user --- you see the final response in the chat buffer.

@subsection Model Compatibility

Tools are only sent to models that support them. If you have tools enabled but switch to a model that doesn't support tool calling:

@itemize @bullet
@item The request proceeds normally without tools (no error)
@item The header line @code{⚒} indicator disappears (tools enabled but not applicable)
@item The prompt line @code{⚒} indicator also disappears (model doesn't support tools)
@item When you switch back to a tool-capable model, tools become active again
@end itemize

This allows you to leave tools enabled globally while freely switching between models.

@section Configuration

@table @code
@item ollama-buddy-tools-enabled
Whether tool calling is active (default: @code{nil}).

@item ollama-buddy-tools-safe-mode
When non-nil, only safe (read-only) tools can execute (default: @code{t}).

@item ollama-buddy-tools-auto-execute
When non-nil, tools execute without confirmation prompts (default: @code{t}).
When nil, each tool invocation shows a @code{yes-or-no-p} prompt before executing.

@item ollama-buddy-tools-max-iterations
Maximum tool-call round-trips per user prompt to prevent infinite loops (default: @code{10}).

@item ollama-buddy-tools-builtin-enabled
Whether to load the 8 built-in tools on startup (default: @code{t}).
When nil, only custom user-registered tools are available.

@item ollama-buddy-tools-models
List of model names known to support tool calling. Used for the @code{⚒} indicator.
@example
(setq ollama-buddy-tools-models
  '("qwen3" "qwen3-coder" "llama3.1" "llama3.3"
    "mistral" "gpt-oss" "command-r"))
@end example
@end table

@section Registering Custom Tools

Use @code{ollama-buddy-tools-register} to add your own tools:

@example
(ollama-buddy-tools-register
 'my-tool                          ; NAME (symbol or string)
 "Description of what the tool does" ; DESCRIPTION
 '((type . "object")               ; PARAMETERS (JSON schema)
   (required . ["param1"])
   (properties . ((param1 . ((type . "string")
                              (description . "Parameter description"))))))
 (lambda (args)                    ; FUNCTION (receives alist, returns string)
   (let ((param1 (alist-get 'param1 args)))
     (format "Result: %s" param1)))
 t)  ; SAFE flag: t = safe (read-only), nil = unsafe
@end example

To remove a tool:
@example
(ollama-buddy-tools-unregister 'my-tool)
@end example

The tool function receives an alist of arguments parsed from the model's JSON response and must return a string result. If the function returns a non-string value, it is formatted with @code{%S}.

@section Interactive Commands

@table @code
@item ollama-buddy-tools-toggle (@kbd{C-c SPC})
Toggle tool calling on or off. When toggled, the header line updates to show/hide the @code{⚒} indicator.

@item ollama-buddy-tools-toggle-safe-mode
Toggle safe mode for tool execution.

@item ollama-buddy-tools-info (@kbd{C-c Q})
Display information about all registered tools in a dedicated org-mode buffer, showing status table, safe mode settings, and each tool's description with parameters.

@item ollama-buddy-tools-clear
Remove all registered tools.

@item ollama-buddy-tools-setup
Re-initialize built-in tools.
@end table

These commands are also available from the transient menu (@kbd{C-c O}) under the ``Actions'' section:
@itemize @bullet
@item @kbd{SPC} - Toggle Tools
@item @kbd{Q} - List Tools
@end itemize

@section Tool Indicators

The @code{⚒} (hammer and wrench) indicator appears in two places with different meanings:

@table @asis
@item Header line
Shows @code{⚒} when tool calling is currently @emph{enabled} (toggled on with @kbd{C-c SPC}). This indicates that tool calls will be sent to the model.

@item Prompt line
Shows @code{⚒} when the current model @emph{supports} tool calling. This appears regardless of whether tools are enabled, helping you identify which models can use tools.
@end table

For example, with a tool-capable model like @code{qwen3:8b}:
@itemize @bullet
@item The prompt always shows: @code{* *qwen3:8b* ⚒ >> PROMPT:}
@item The header line shows @code{⚒} only when you've enabled tools with @kbd{C-c SPC}
@end itemize

Additional places showing tool capability:
@itemize @bullet
@item The @code{C-c m} completing-read model selector (alongside @code{⊙} for vision models)
@item The model management buffer (@code{C-c M}) next to model names
@end itemize

The list of tool-capable models is configured via @code{ollama-buddy-tools-models}. Cloud model variants (e.g.@: @code{cl:qwen3-coder:480b-cloud}) are automatically matched against their base names.

@node RAG
@chapter RAG (Retrieval-Augmented Generation)

@section Overview

RAG allows you to index local documents and source code, then retrieve relevant
context for conversations via semantic search.  Instead of pasting entire files
into prompts, RAG automatically finds the most relevant chunks and attaches them
as context, keeping token usage efficient and responses grounded in your actual
documents.

The RAG module (@code{ollama-buddy-rag.el}) is loaded by default.

@section Requirements

By default, RAG uses Ollama for embeddings and requires an embedding model to be
pulled:

@example
ollama pull nomic-embed-text
@end example

Other Ollama embedding models include @code{mxbai-embed-large} and
@code{all-minilm}.  You can pull these from Model Management (@kbd{C-c M})
under the ``Embedding (RAG)'' category.

Alternatively, you can point RAG at any OpenAI-compatible embedding server
instance) by setting @code{ollama-buddy-rag-embedding-base-url} and
@code{ollama-buddy-rag-embedding-api-style}.  In that case no Ollama embedding
model is needed.

@section Quick Start

@enumerate
@item Pull an embedding model: @code{ollama pull nomic-embed-text}
@item Open the RAG menu: @kbd{C-c O} then @kbd{r}
@item Index a directory: select @kbd{i} and choose a directory
@item Wait for indexing to complete (progress shown in status line)
@item Search your index: select @kbd{s} and enter a query
@item Or attach context to a conversation: select @kbd{a}, enter a query, then chat normally
@end enumerate

@section How It Works

@enumerate
@item @strong{Indexing}: Files in a directory are read, split into chunks (by
paragraph/section), and each chunk is sent to the embedding model to produce a
vector representation.  The resulting index is saved to disk.
@item @strong{Searching}: Your query is embedded using the same model, then
compared against all chunk vectors using cosine similarity.  The most relevant
chunks are returned.
@item @strong{Attaching}: Retrieved chunks are injected into the conversation
context as a system-level prefix, so the LLM can reference them when
generating a response.
@end enumerate

@section Configuration

@table @code
@item ollama-buddy-rag-embedding-model
The embedding model to use (default: @code{"nomic-embed-text"}).

@item ollama-buddy-rag-chunk-size
Target number of tokens per chunk (default: @code{300}).

@item ollama-buddy-rag-chunk-overlap
Number of overlapping tokens between consecutive chunks (default: @code{50}).

@item ollama-buddy-rag-top-k
Number of top results to return from a search (default: @code{5}).

@item ollama-buddy-rag-similarity-threshold
Minimum cosine similarity score for a chunk to be included in results
(default: @code{0.3}).

@item ollama-buddy-rag-index-directory
Directory where RAG index files are stored
(default: @code{~/.emacs.d/ollama-buddy-rag/}).

@item ollama-buddy-rag-file-extensions
List of file extensions to include when indexing a directory
(default: common text and code extensions).

@item ollama-buddy-rag-exclude-patterns
List of glob patterns to exclude from indexing
(default: @code{("node_modules" ".git" "__pycache__" ".elc")}).

@item ollama-buddy-rag-batch-size
Number of chunks to embed per API call (default: @code{20}).

@item ollama-buddy-rag-embedding-base-url
Base URL for the embedding service.  When @code{nil} (the default), the main
Ollama host and port are used.  Set this to point at a separate
OpenAI-compatible embedding server, for example:

@example
(setq ollama-buddy-rag-embedding-base-url "http://localhost:9000")
@end example

This is useful when running a dedicated embedding service independently
from your Ollama instance.

@item ollama-buddy-rag-embedding-api-style
The API style used by the embedding endpoint.  Two values are supported:

@table @code
@item ollama
(default) POST to @code{/api/embed} --- the Ollama native endpoint.

@item openai
POST to @code{/v1/embeddings} --- compatible with any OpenAI-style embedding
service.  Use this together with @code{ollama-buddy-rag-embedding-base-url}.
@end table

@item ollama-buddy-rag-embedding-api-key
Optional Bearer token sent to the embedding service.  Only needed when
@code{ollama-buddy-rag-embedding-api-style} is @code{openai} and the service
requires authentication.  Leave @code{nil} for local servers.
@end table

@section PDF Support

RAG can index PDF files if @code{pdftotext} (from @code{poppler-utils}) is
installed on your system.  PDF support is auto-detected at runtime:

@itemize @bullet
@item If @code{pdftotext} is available, @code{.pdf} files are included when
indexing directories.
@item If @code{pdftotext} is not installed, PDF files are silently skipped.
@item PDFs bypass the file size limit (@code{ollama-buddy-rag-max-file-size})
since binary PDF size does not reflect extracted text size.
@end itemize

To install @code{pdftotext}:

@example
# Debian/Ubuntu
sudo apt install poppler-utils

# Arch Linux
sudo pacman -S poppler

# macOS
brew install poppler
@end example

@section Interactive Commands

These commands are available from the RAG transient menu (@kbd{C-c O} then @kbd{r}):

@table @code
@item ollama-buddy-rag-index-directory-interactive (@kbd{i})
Index a directory.  Prompts for the directory path, reads and chunks all
matching files, then asynchronously generates embeddings.  Progress is shown in
the status line.

@item ollama-buddy-rag-search-interactive (@kbd{s})
Search the index with a text query.  Results are displayed in an org-mode
buffer showing similarity scores, file paths, and content excerpts.

@item ollama-buddy-rag-attach-context (@kbd{a})
Search and attach the top results as context for the next conversation.
Attached RAG context appears when you use Show Attachments.

@item ollama-buddy-rag-list-indices (@kbd{l})
List all saved RAG indices with their chunk counts and file counts.

@item ollama-buddy-rag-delete-index (@kbd{d})
Delete a saved RAG index.
@end table

@section Inline @@rag() Syntax

You can use @code{@@rag(query)} directly in prompts to automatically search your
RAG index and attach results as context --- no separate attach step needed.
This mirrors the @code{@@search(query)} syntax for web search.

@example
Tell me about @@rag(giant squid attack) from the book.
@end example

Multiple queries are supported:

@example
Compare @@rag(captain nemo) with @@rag(ned land) in terms of character development.
@end example

How it works:

@enumerate
@item The @code{@@rag()} patterns are extracted from the prompt.
@item Each query is embedded synchronously and searched against the index.
@item Matching chunks are attached as RAG context.
@item The @code{@@rag()} wrappers are removed, preserving the query text in the
prompt sent to the model.
@end enumerate

If you have multiple indexes, you will be prompted to select one.  If only one
index exists, it is used automatically.

@section Example Workflow

Suppose you have the text of ``Twenty Thousand Leagues Under the Sea'' in
@file{~/books/leagues.txt}:

@example
;; 1. Index the book
C-c O r i  ->  ~/books/

;; 2a. Attach via transient menu
C-c O r a  ->  "giant squid attack"

;; 2b. Or use inline syntax -- no separate step needed
"Describe the @@rag(giant squid encounter) in detail, referencing the text."

;; 3. Ask a question in chat -- the LLM now has relevant passages
"Describe the giant squid encounter in detail, referencing the text."
@end example

Without RAG, the model would give a generic answer from training data.  With
RAG, it references specific passages from your indexed document.

@node Advanced Usage
@chapter Advanced Usage

@section Managing Token Usage

Ollama Buddy can track token usage statistics:

To toggle token statistics display after responses:
@example
M-x ollama-buddy-toggle-token-display
@end example
or press @code{C-c T} in the chat buffer.

To view detailed token usage statistics:
@example
M-x ollama-buddy-display-token-stats
@end example
or press @code{C-c #} in the chat buffer.

This displays a combined view with token usage graphs by model, average token rates, and recent interactions.

@section Customizing the Interface

@subsection Streaming Options

Control how responses are displayed:

@table @code
@item ollama-buddy-streaming-enabled
Toggle streaming mode where responses appear token by token.
@example
M-x ollama-buddy-toggle-streaming
@end example

@item ollama-buddy-auto-scroll
Enable auto-scrolling during streaming output.

@item ollama-buddy-pulse-response
Flash the response when streaming completes.
@end table

@subsection Debug Mode

For advanced troubleshooting, you can enable debug mode:
@example
M-x ollama-buddy-toggle-debug-mode
@end example
or press @code{C-c B} in the chat buffer.

This shows raw JSON messages in a debug buffer.

@section Editing Conversation History

To manually edit conversation history:
@example
M-x ollama-buddy-history-edit
@end example
or press @code{C-c J} in the chat buffer.

This opens an editable buffer with the conversation history. You can modify it and press @code{C-c C-c} to save or @code{C-c C-k} to cancel.

To edit history for a specific model, use @code{C-u C-c J}.

@section Advanced System Prompt Management

For more control over system prompts:

@subsection Setting a system prompt without sending
@example
(ollama-buddy-set-system-prompt)
@end example
Enter your system prompt, then press @code{C-c s}.

@subsection Using a system prompt from Fabric
@example
M-x ollama-buddy-fabric-set-system-prompt
@end example
or press @code{C-c f p}.

@section Using Direct API Access

For direct programmatic access to Ollama:

@example
(ollama-buddy--make-request "/api/tags" "GET")
@end example

Or with a payload:
@example
(ollama-buddy--make-request "/api/chat" "POST" 
                           (json-encode '((model . "llama3:latest")
                                         (prompt . "Hello"))))
@end example

@node API Reference
@chapter API Reference

@section Interactive Functions

@table @code
@item ollama-buddy-role-transient-menu
Display the dynamic role-specific transient menu, built from @code{ollama-buddy-command-definitions}.

@item ollama-buddy-menu
Display the minibuffer-based Ollama Buddy menu (legacy fallback).

@item ollama-buddy-transient-menu
Display the main transient-based menu.

@item ollama-buddy--open-chat
Open the chat buffer.

@item ollama-buddy--send-prompt
Send the current prompt to the AI.

@item ollama-buddy--swap-model
Switch to a different model. All models (including cloud models marked with @code{☁}) are listed together.

@item ollama-buddy-cloud-signin
Sign in to Ollama cloud services.

@item ollama-buddy-cloud-signout
Sign out from Ollama cloud services.

@item ollama-buddy-cloud-status
Check Ollama cloud authentication status.

@item ollama-buddy-toggle-global-system-prompt
Toggle the global system prompt on or off.

@item ollama-buddy-unload-model
Unload a specific model to free up resources.

@item ollama-buddy-unload-all-models
Unload all currently running models.

@item ollama-buddy-manage-models
Display and manage available models.

@item ollama-buddy-pull-model
Pull a new model from Ollama Hub.

@item ollama-buddy-import-gguf-file
Import a GGUF file to create a custom model.

@item ollama-buddy-set-system-prompt
Set the current prompt as the system prompt.

@item ollama-buddy-reset-system-prompt
Reset the system prompt to default (none).

@item ollama-buddy-sessions-save
Save the current conversation as a session.

@item ollama-buddy-sessions-load
Load a previously saved session.

@item ollama-buddy-sessions-directory
Open the sessions directory in Dired for file management.

@item ollama-buddy-sessions-new
Start a new session.

@item ollama-buddy-toggle-history
Toggle conversation history on/off.

@item ollama-buddy-clear-history
Clear the conversation history.

@item ollama-buddy-history-edit
Display the conversation history.

@item ollama-buddy-roles-switch-role
Switch to a different role.

@item ollama-buddy-role-creator-create-new-role
Create a new role.

@item ollama-buddy-params-display
Display current parameter settings.

@item ollama-buddy-params-edit
Edit a specific parameter.

@item ollama-buddy-params-reset
Reset all parameters to defaults.

@item ollama-buddy-toggle-params-in-header
Toggle display of parameters in header.

@item ollama-buddy-toggle-token-display
Toggle display of token statistics.

@item ollama-buddy-display-token-stats
Display token usage statistics with graphs and recent interactions.

@item ollama-buddy-fabric-setup
Set up Fabric pattern integration.

@item ollama-buddy-fabric-sync-patterns
Sync with the latest Fabric patterns.

@item ollama-buddy-fabric-list-patterns
List available Fabric patterns.

@item ollama-buddy-fabric-send
Apply a Fabric pattern to selected text.

@item ollama-buddy-toggle-markdown-conversion
Toggle Markdown to Org conversion.

@item ollama-buddy-toggle-streaming
Toggle streaming mode for responses.

@item ollama-buddy-toggle-reasoning-visibility
Toggle visibility of reasoning/thinking sections in responses.

@item ollama-buddy-toggle-show-history-indicator
Toggle display of history indicator in the header line.

@item ollama-buddy-switch-communication-backend
Interactively switch between network-process and curl backends.

@item ollama-buddy-test-communication-backend
Test the current communication backend.

@item ollama-buddy-toggle-debug-mode
Toggle display of debug information.

@item ollama-buddy-set-model-context-size
Set the context size for a specific model.

@item ollama-buddy-toggle-context-percentage
Toggle context percentage display in the status bar.

@item ollama-buddy-show-context-info
Display detailed context usage information.

@item ollama-buddy-set-max-history-length
Set the maximum number of message pairs to keep in history.

@item ollama-buddy-user-prompts-save
Save the current system prompt.

@item ollama-buddy-user-prompts-load
Load a previously saved system prompt.

@item ollama-buddy-user-prompts-list
Display a list of all saved user system prompts.

@item ollama-buddy-user-prompts-edit
Edit a user system prompt.

@item ollama-buddy-user-prompts-delete
Delete a user system prompt.

@item ollama-buddy-user-prompts-create-new
Create a new system prompt from scratch.

@item ollama-buddy-transient-user-prompts-menu
Display the transient menu for user system prompts.

@end table

@section Core Functions

@table @code
@item ollama-buddy--send
Send a prompt to Ollama.

@item ollama-buddy--make-request
Make a generic request to the Ollama API.

@item ollama-buddy--get-models
Get a list of available models.

@item ollama-buddy--get-valid-model
Get a valid model with fallback handling.

@item ollama-buddy--add-to-history
Add a message to the conversation history.

@item ollama-buddy--get-history-for-request
Get history for the current request.

@item ollama-buddy--prepare-prompt-area
Prepare the prompt area in the buffer.

@item ollama-buddy--update-status
Update the status display.
@end table

@section Customization Functions

@table @code
@item ollama-buddy-update-command-with-params
Update a command definition with new properties and parameters.

@item ollama-buddy-update-menu-entry
Update a menu entry's properties.

@item ollama-buddy-add-model-to-menu-entry
Associate a specific model with a menu entry.

@item ollama-buddy-add-parameters-to-command
Add specific parameters to a command definition.
@end table

@node FAQ
@chapter Frequently Asked Questions

@section General Questions

@subsection What is the difference between Ollama Buddy and other AI assistants?
Ollama Buddy integrates with Ollama to run LLMs locally, offering privacy, customization, and seamless Emacs integration without relying on external API services.

@subsection Does Ollama Buddy require an internet connection?
For local Ollama models, no internet connection is required after pulling them. Internet is needed for:
@itemize @bullet
@item Pulling new models from Ollama
@item Using Ollama cloud models
@item Using remote providers (OpenAI, Claude, Gemini, Grok, Codestral)
@item Syncing Fabric patterns or Awesome ChatGPT Prompts
@end itemize

@subsection How do I use Ollama cloud models?
Cloud models require authentication:
@enumerate
@item Run @code{M-x ollama-buddy-cloud-signin} to sign in
@item Select cloud models with @code{C-c m} (they appear marked with @code{☁})
@item The @code{☁} symbol in the header indicates cloud model usage
@end enumerate

@subsection Which models work best with Ollama Buddy?
Most models supported by Ollama work well. Popular choices include:
@itemize @bullet
@item llama3:latest - Good general purpose assistant
@item codellama:latest - Excellent for code-related tasks
@item mistral:latest - Good balance of performance and quality
@item phi:latest - Smaller model that works well on limited hardware
@end itemize

@subsection How much RAM do I need?
It depends on the model:
@itemize @bullet
@item Small models (7B) - 8GB minimum, 16GB recommended
@item Medium models (13B) - 16GB minimum, 24GB+ recommended
@item Large models (34B+) - 32GB+ recommended
@end itemize

Quantized models (e.g., Q4_K_M variants) require less RAM.

@section Usage Questions

@subsection How do I cancel a request that's taking too long?
Press @code{C-c k} in the chat buffer or select "Kill Request" from the menu.

@subsection How can I save my conversations?
Use @code{C-c S} to save the current session, giving it a name. You can restore it later with @code{C-c L}.

@subsection Can I use multiple models in the same conversation?
Yes, you can switch models at any time with @code{C-c m}. Each model maintains its own conversation history.

@subsection How do I clear the conversation history?
Press @code{C-c X} to clear history, or @code{C-c N} to start a completely new session.

@subsection How can I create a custom command?
The easiest way is through the role creator: press @code{C-c E} and follow the prompts to create commands with specific prompts, models, and parameters.

@subsection How can I manage context windows?
Ollama Buddy provides several options:
@itemize @bullet
@item Enable context monitoring with @code{(setq ollama-buddy-show-context-percentage t)}
@item Use @kbd{C-c C} to check current context usage
@item Limit history length with @kbd{C-c Y}
@item Set model-specific context sizes with @kbd{C-c $}
@end itemize

@subsection What happens when I exceed the context limit?
When context monitoring is enabled:
@itemize @bullet
@item You'll get a warning when approaching the limit (85-100%)
@item You'll get an error dialog at or above 100%
@item You can choose to proceed anyway or modify your content
@end itemize

@subsection How do I create effective system prompts?
Effective system prompts typically include:
@itemize @bullet
@item Clear role definition for the AI
@item Specific guidelines for response format and style
@item Examples of desired output (when applicable)
@item Constraints or limitations to observe
@end itemize
Start simple, test the response, and refine iteratively.

@subsection Where are my system prompts stored?
System prompts are stored as .org files in the directory specified by `ollama-buddy-user-prompts-directory`, which defaults to `~/.emacs.d/ollama-buddy-user-prompts/`.

@section Troubleshooting

@subsection Ollama Buddy shows "OFFLINE" status
Ensure that:
@itemize @bullet
@item Ollama is installed and running
@item The hostname and port are correctly configured (@code{ollama-buddy-host} and @code{ollama-buddy-port})
@item Your firewall isn't blocking connections
@end itemize

@subsection Responses are slow or the model seems to hang
Try:
@itemize @bullet
@item Using a smaller model
@item Adjusting the @code{num_ctx} parameter to a smaller value
@item Setting @code{low_vram} to @code{t} if you have limited GPU memory
@item Checking CPU/RAM usage to ensure your system isn't overloaded
@end itemize

@subsection Getting "error parsing model" when pulling a model
This usually means:
@itemize @bullet
@item The model name is incorrect
@item The model is not available in the Ollama repository
@item You have network connectivity issues
@end itemize

@subsection Model responses are low quality or truncated
Try:
@itemize @bullet
@item Increasing the @code{temperature} parameter for more creative responses
@item Increasing @code{num_predict} for longer responses
@item Using a more capable model
@item Providing clearer instructions in your prompt
@end itemize

@subsection How do system prompts differ from regular prompts?
System prompts provide overall instructions to the AI about its role and how to respond, while regular prompts are your specific questions or requests. System prompts persist across the conversation, affecting all responses, while regular prompts are one-time interactions.

@subsection What is the global system prompt?
The global system prompt is a baseline prompt automatically prepended to all requests. It's useful for consistent formatting instructions across all models. Toggle it with @code{M-x ollama-buddy-toggle-global-system-prompt}.

@subsection How do I use image analysis/vision features?
@enumerate
@item Select a vision-capable model (e.g., gemma3:4b, llama3.2:3b)
@item Include an image path in your prompt
@item The image will be automatically detected and encoded
@end enumerate

Supported formats: PNG, JPG, JPEG, WebP, GIF.

@subsection Can I share system prompts between different installations?
Yes, the user system prompts are stored as plain text .org files in your `ollama-buddy-user-prompts-directory`. You can copy these files to share them with colleagues or between different machines.

@node Troubleshooting
@chapter Troubleshooting

@section Common Issues

@subsection Connection Problems

@table @asis
@item Symptom: Unable to connect to Ollama server
@itemize @bullet
@item Check if Ollama is running with @code{ps aux | grep ollama}
@item Verify host and port settings (@code{ollama-buddy-host} and @code{ollama-buddy-port})
@item Try connecting to Ollama directly: @code{curl http://localhost:11434/api/tags}
@end itemize

@item Symptom: Connection breaks during long responses
@itemize @bullet
@item This can happen with very large responses
@item Try setting a lower @code{num_predict} value
@item Check if your OS has any network timeout settings
@end itemize
@end table

@subsection Model Problems

@table @asis
@item Symptom: Model loads but gives poor responses
@itemize @bullet
@item Try a different model
@item Adjust parameters (increase temperature for more creativity)
@item Provide clearer or more detailed prompts
@item Check if the model is appropriate for your task
@end itemize

@item Symptom: Model fails to load or crashes
@itemize @bullet
@item Check system memory usage
@item Try a smaller quantized model
@item Adjust @code{num_ctx} to a smaller value
@item Set @code{low_vram} to @code{t} if using GPU
@end itemize
@end table

@subsection Interface Issues

@table @asis
@item Symptom: Chat buffer becomes unresponsive
@itemize @bullet
@item Cancel any running requests with @code{C-c k}
@item Check if Emacs is using high CPU
@item Try disabling token statistics display
@item Close and reopen the chat buffer
@end itemize

@item Symptom: Markdown conversion issues
@itemize @bullet
@item Toggle markdown conversion off with @code{C-c C-o}
@item Check if the response contains complex formatting
@item Try editing the history to fix formatting issues
@end itemize
@end table

@section Debugging

@subsection Enable Debug Mode

To get more information about what's happening:
@example
M-x ollama-buddy-toggle-debug-mode
@end example

This opens a debug buffer showing raw JSON communication with Ollama.

@subsection Check Logs

Ollama logs can be useful for troubleshooting:
@example
tail -f ~/.ollama/logs/ollama.log
@end example

@subsection Report Issues

If you encounter a bug:
@enumerate
@item Enable debug mode
@item Reproduce the issue
@item Copy the debug output
@item Report the issue on GitHub with:
  @itemize @bullet
  @item Emacs version
  @item Ollama version
  @item Model used
  @item Debug output
  @item Steps to reproduce
  @end itemize
@end enumerate

@node Contributing
@chapter Contributing

@section Getting Started

Ollama Buddy is an open-source project, and contributions are welcome!

@enumerate
@item Fork the repository: @url{https://github.com/captainflasmr/ollama-buddy}
@item Clone your fork: @code{git clone https://github.com/YOUR-USERNAME/ollama-buddy.git}
@item Create a branch: @code{git checkout -b my-feature-branch}
@item Make your changes
@item Test thoroughly
@item Commit with a clear message
@item Push to your fork
@item Create a pull request
@end enumerate

@section Development Setup

@subsection Required Tools

@itemize @bullet
@item Emacs 28.1+
@item Ollama installed and running
@item Git
@end itemize

@subsection Recommended Packages

@itemize @bullet
@item package-lint
@item flycheck
@item elisp-lint
@end itemize

@section Coding Guidelines

@itemize @bullet
@item Follow Emacs Lisp conventions
@item Use two spaces for indentation
@item Add documentation strings to functions
@item Keep line length under 80 characters
@item Use prefix @code{ollama-buddy--} for internal functions
@item Use prefix @code{ollama-buddy-} for public functions
@end itemize

@section Testing

@subsection Run Existing Tests

The package includes comprehensive tests:

@example
M-x ollama-buddy-run-tests
M-x ollama-buddy-integration-run-tests
M-x ollama-buddy-fabric-run-tests
M-x ollama-buddy-parameter-run-tests
@end example

@subsection Adding New Tests

When adding features, please also add tests:
@itemize @bullet
@item Unit tests for individual functions
@item Integration tests for API interactions
@item Parameter tests for parameter handling
@end itemize

@section Feature Requests and Bug Reports

@itemize @bullet
@item Use GitHub Issues for bug reports and feature requests
@item Provide clear steps to reproduce bugs
@item For feature requests, explain the use case
@end itemize

@subsection User System Prompts Issues

@table @asis
@item Symptom: Cannot save system prompts
@itemize @bullet
@item Check if `ollama-buddy-user-prompts-directory` exists and is writable
@item Ensure you have set a system prompt before trying to save it
@item Check for any error messages in the minibuffer or *Messages* buffer
@end itemize

@item Symptom: Prompts not appearing in the load menu
@itemize @bullet
@item Verify that prompts are saved in the correct format (category__title__system.org)
@item Check if the prompts directory contains files with proper formatting
@item Try refreshing the prompts cache with `M-x ollama-buddy-user-prompts--refresh-cache`
@end itemize
@end table

@node Index
@unnumbered Index

@printindex cp

@bye
