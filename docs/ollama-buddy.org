<<Top>>

Next: [[#Introduction][Introduction]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

* Ollama Buddy [[#Ollama-Buddy][¶]]
:PROPERTIES:
:CUSTOM_ID: Ollama-Buddy
:CLASS: top
:END:
Ollama Buddy is a comprehensive Emacs package that provides seamless
integration with Ollama, allowing you to leverage powerful large
language models (LLMs) directly within your Emacs workflow.

<<SEC_Contents>>
** Table of Contents
:PROPERTIES:
:CUSTOM_ID: table-of-contents
:CLASS: contents-heading
:END:

- [[#Introduction][1 Introduction]]
  - [[#What-is-Ollama-Buddy_003f][1.1 What is Ollama Buddy?]]
  - [[#Why-Use-Ollama-Buddy_003f][1.2 Why Use Ollama Buddy?]]
  - [[#Prerequisites][1.3 Prerequisites]]
- [[#Installation][2 Installation]]
  - [[#Installing-Ollama][2.1 Installing Ollama]]
  - [[#Package-Installation][2.2 Package Installation]]
    - [[#Using-package_002eel][2.2.1 Using package.el]]
    - [[#Using-use_002dpackage][2.2.2 Using use-package]]
    - [[#Manual-Installation][2.2.3 Manual Installation]]
  - [[#Dependencies][2.3 Dependencies]]
  - [[#Communication-Backends][2.4 Communication Backends]]
  - [[#API-Key-Setup][2.5 API Key Setup]]
- [[#Configuration][3 Configuration]]
  - [[#Basic-Configuration][3.1 Basic Configuration]]
  - [[#Display-Options][3.2 Display Options]]
  - [[#File-Attachment-Configuration][3.3 File Attachment
    Configuration]]
  - [[#Directory-Configuration][3.4 Directory Configuration]]
  - [[#History-and-Session-Configuration][3.5 History and Session
    Configuration]]
  - [[#Context-Management-Configuration][3.6 Context Management
    Configuration]]
  - [[#External-API-Configuration][3.7 External API Configuration]]
    - [[#Provider-Prefixes][3.7.1 Provider Prefixes]]
  - [[#Global-System-Prompt-Configuration][3.8 Global System Prompt
    Configuration]]
  - [[#Vision-Configuration][3.9 Vision Configuration]]
  - [[#Cloud-Model-Configuration][3.10 Cloud Model Configuration]]
  - [[#Awesome-ChatGPT-Prompts-Configuration][3.11 Awesome ChatGPT
    Prompts Configuration]]
- [[#Quick-Start][4 Quick Start]]
  - [[#Basic-Usage][4.1 Basic Usage]]
  - [[#Common-Operations][4.2 Common Operations]]
- [[#Core-Features][5 Core Features]]
  - [[#Chat-Interface-1][5.1 Chat Interface]]
  - [[#Pre_002dbuilt-Commands][5.2 Pre-built Commands]]
  - [[#Model-Management][5.3 Model Management]]
  - [[#Parameter-Control-1][5.4 Parameter Control]]
  - [[#Roles-and-Custom-Commands][5.5 Roles and Custom Commands]]
  - [[#Prompt-Template-Collections][5.6 Prompt Template Collections]]
  - [[#External-API-Integration][5.7 External API Integration]]
  - [[#Vision-Support][5.8 Vision Support]]
  - [[#File-Attachments-1][5.9 File Attachments]]
- [[#Chat-Interface][6 Chat Interface]]
  - [[#Opening-the-Chat][6.1 Opening the Chat]]
  - [[#Interface-Overview][6.2 Interface Overview]]
  - [[#Sending-Prompts][6.3 Sending Prompts]]
  - [[#System-Prompts][6.4 System Prompts]]
  - [[#Markdown-to-Org-Conversion][6.5 Markdown to Org Conversion]]
  - [[#Reasoning-Visibility-Control][6.6 Reasoning Visibility Control]]
- [[#Working-with-Models][7 Working with Models]]
  - [[#Available-Models][7.1 Available Models]]
  - [[#Switching-Models][7.2 Switching Models]]
  - [[#Local-vs_002e-Cloud-Models][7.3 Local vs. Cloud Models]]
  - [[#Managing-Models][7.4 Managing Models]]
  - [[#Pulling-New-Models][7.5 Pulling New Models]]
  - [[#Importing-GGUF-Files][7.6 Importing GGUF Files]]
  - [[#Vision-Models][7.7 Vision Models]]
    - [[#Configuring-Vision-Support][7.7.1 Configuring Vision Support]]
    - [[#Using-Vision-Models][7.7.2 Using Vision Models]]
  - [[#Multishot-Mode][7.8 Multishot Mode]]
- [[#Context-Management][8 Context Management]]
  - [[#Understanding-Context-Windows][8.1 Understanding Context
    Windows]]
  - [[#Context-Size-Detection][8.2 Context Size Detection]]
  - [[#Enabling-Context-Monitoring][8.3 Enabling Context Monitoring]]
  - [[#Context-with-File-Attachments][8.4 Context with File
    Attachments]]
  - [[#Context-Management-Commands][8.5 Context Management Commands]]
  - [[#Token-Estimation][8.6 Token Estimation]]
  - [[#Managing-Context-in-Practice][8.7 Managing Context in Practice]]
    - [[#Workflow-Strategies][8.7.1 Workflow Strategies]]
      - [[#Paste_002dand_002dSend-Approach][8.7.1.1 Paste-and-Send
        Approach]]
      - [[#Preemptive-Checking][8.7.1.2 Preemptive Checking]]
      - [[#History-Length-Management][8.7.1.3 History Length
        Management]]
    - [[#Using-num_005fctx-Parameter][8.7.2 Using num_ctx Parameter]]
  - [[#Context-Display-Configuration][8.8 Context Display
    Configuration]]
  - [[#Fallback-Context-Sizes][8.9 Fallback Context Sizes]]
  - [[#Troubleshooting-Context-Issues][8.10 Troubleshooting Context
    Issues]]
- [[#File-Attachments][9 File Attachments]]
  - [[#Overview][9.1 Overview]]
  - [[#Supported-File-Types][9.2 Supported File Types]]
  - [[#Attaching-Files][9.3 Attaching Files]]
    - [[#Basic-File-Attachment][9.3.1 Basic File Attachment]]
    - [[#Dired-Integration][9.3.2 Dired Integration]]
  - [[#Managing-Attachments][9.4 Managing Attachments]]
    - [[#Viewing-Attachments][9.4.1 Viewing Attachments]]
    - [[#Detaching-Files][9.4.2 Detaching Files]]
    - [[#Clearing-All-Attachments][9.4.3 Clearing All Attachments]]
  - [[#File-Size-Limits][9.5 File Size Limits]]
  - [[#How-Attachments-Work][9.6 How Attachments Work]]
    - [[#Context-Integration][9.6.1 Context Integration]]
    - [[#Session-Persistence][9.6.2 Session Persistence]]
  - [[#File-Attachment-Workflow-Examples][9.7 File Attachment Workflow
    Examples]]
    - [[#Code-Review-Workflow][9.7.1 Code Review Workflow]]
    - [[#Multi_002dFile-Analysis][9.7.2 Multi-File Analysis]]
    - [[#Configuration-Troubleshooting][9.7.3 Configuration
      Troubleshooting]]
  - [[#Context-Considerations][9.8 Context Considerations]]
  - [[#Best-Practices][9.9 Best Practices]]
  - [[#Troubleshooting-Attachments][9.10 Troubleshooting Attachments]]
- [[#Web-Search][10 Web Search]]
  - [[#Overview-1][10.1 Overview]]
  - [[#How-It-Works][10.2 How It Works]]
    - [[#eww-Mode-_0028Default_002c-Recommended_0029][10.2.1 eww Mode
      (Default, Recommended)]]
    - [[#API-Mode-_0028Experimental_0029][10.2.2 API Mode
      (Experimental)]]
  - [[#Configuration-2][10.3 Configuration]]
  - [[#Usage][10.4 Usage]]
    - [[#Inline-Search-Syntax][10.4.1 Inline Search Syntax]]
    - [[#Manual-Search-Commands][10.4.2 Manual Search Commands]]
    - [[#Transient-Menu][10.4.3 Transient Menu]]
  - [[#Viewing-Web-Search-Results][10.5 Viewing Web Search Results]]
  - [[#Managing-Web-Searches][10.6 Managing Web Searches]]
  - [[#Provider-Support][10.7 Provider Support]]
  - [[#Troubleshooting-1][10.8 Troubleshooting]]
- [[#Parameter-Control][11 Parameter Control]]
  - [[#Understanding-Parameters][11.1 Understanding Parameters]]
  - [[#Viewing-Current-Parameters][11.2 Viewing Current Parameters]]
  - [[#Editing-Parameters][11.3 Editing Parameters]]
  - [[#Parameter-Profiles][11.4 Parameter Profiles]]
  - [[#Command_002dSpecific-Parameters][11.5 Command-Specific
    Parameters]]
  - [[#Reset-Parameters][11.6 Reset Parameters]]
  - [[#Displaying-Parameters-in-Header][11.7 Displaying Parameters in
    Header]]
- [[#Session-Management][12 Session Management]]
  - [[#Understanding-Sessions][12.1 Understanding Sessions]]
  - [[#Creating-a-New-Session][12.2 Creating a New Session]]
  - [[#Saving-a-Session][12.3 Saving a Session]]
  - [[#Loading-a-Session][12.4 Loading a Session]]
  - [[#Managing-Sessions][12.5 Managing Sessions]]
  - [[#Conversation-History][12.6 Conversation History]]
- [[#User-System-Prompts][13 User System Prompts]]
  - [[#Overview-2][13.1 Overview]]
  - [[#Accessing-the-System-Prompts-Menu][13.2 Accessing the System
    Prompts Menu]]
  - [[#Saving-System-Prompts][13.3 Saving System Prompts]]
  - [[#Loading-Saved-Prompts][13.4 Loading Saved Prompts]]
  - [[#Managing-Your-Prompt-Library][13.5 Managing Your Prompt Library]]
    - [[#Viewing-All-Prompts][13.5.1 Viewing All Prompts]]
    - [[#Editing-Prompts][13.5.2 Editing Prompts]]
    - [[#Creating-New-Prompts][13.5.3 Creating New Prompts]]
    - [[#Deleting-Prompts][13.5.4 Deleting Prompts]]
  - [[#Categories-and-Organization][13.6 Categories and Organization]]
  - [[#Prompt-Storage-Format][13.7 Prompt Storage Format]]
  - [[#Best-Practices-for-System-Prompts][13.8 Best Practices for System
    Prompts]]
    - [[#Components-of-Effective-Prompts][13.8.1 Components of Effective
      Prompts]]
    - [[#Example-Patterns][13.8.2 Example Patterns]]
  - [[#Example-System-Prompts][13.9 Example System Prompts]]
    - [[#Technical-Writing-Assistant][13.9.1 Technical Writing
      Assistant]]
    - [[#Code-Reviewer][13.9.2 Code Reviewer]]
  - [[#Workflow-Examples][13.10 Workflow Examples]]
    - [[#Python-Code-Assistance][13.10.1 Python Code Assistance]]
    - [[#Technical-Writing-Help][13.10.2 Technical Writing Help]]
  - [[#Integration-with-Roles][13.11 Integration with Roles]]
- [[#Roles-and-Commands][14 Roles and Commands]]
  - [[#Understanding-Roles][14.1 Understanding Roles]]
  - [[#Role-File-Naming-Convention][14.2 Role File Naming Convention]]
  - [[#Built_002din-Commands][14.3 Built-in Commands]]
  - [[#Creating-Custom-Roles][14.4 Creating Custom Roles]]
    - [[#Interactive-Role-Creator][14.4.1 Interactive Role Creator]]
    - [[#Manual-Role-Creation][14.4.2 Manual Role Creation]]
  - [[#Switching-Roles][14.5 Switching Roles]]
  - [[#Managing-Role-Files][14.6 Managing Role Files]]
  - [[#Advanced-Role-Customization][14.7 Advanced Role Customization]]
    - [[#Per_002dRole-Menu-Columns][14.7.1 Per-Role Menu Columns]]
    - [[#Command_002dSpecific-Models][14.7.2 Command-Specific Models]]
    - [[#Command_002dSpecific-Parameters-1][14.7.3 Command-Specific
      Parameters]]
    - [[#Creating-New-Commands][14.7.4 Creating New Commands]]
  - [[#Role-Examples][14.8 Role Examples]]
    - [[#Programming-Role][14.8.1 Programming Role]]
    - [[#Writing-Role][14.8.2 Writing Role]]
  - [[#Tips-for-Effective-Role-Usage][14.9 Tips for Effective Role
    Usage]]
- [[#Fabric-Pattern-Integration][15 Fabric Pattern Integration]]
  - [[#What-are-Fabric-Patterns_003f][15.1 What are Fabric Patterns?]]
  - [[#Setting-Up-Fabric-Integration][15.2 Setting Up Fabric
    Integration]]
  - [[#Using-Fabric-Patterns][15.3 Using Fabric Patterns]]
  - [[#Browsing-Available-Patterns][15.4 Browsing Available Patterns]]
  - [[#Viewing-Pattern-Details][15.5 Viewing Pattern Details]]
  - [[#Updating-Patterns][15.6 Updating Patterns]]
  - [[#Using-Patterns-by-Category][15.7 Using Patterns by Category]]
- [[#Awesome-ChatGPT-Prompts][16 Awesome ChatGPT Prompts]]
  - [[#What-is-Awesome-ChatGPT-Prompts_003f][16.1 What is Awesome
    ChatGPT Prompts?]]
  - [[#Setting-Up-Awesome-ChatGPT-Prompts][16.2 Setting Up Awesome
    ChatGPT Prompts]]
  - [[#Using-Awesome-ChatGPT-Prompts][16.3 Using Awesome ChatGPT
    Prompts]]
  - [[#Browsing-Available-Prompts][16.4 Browsing Available Prompts]]
  - [[#Categorized-Browsing][16.5 Categorized Browsing]]
  - [[#Viewing-Prompt-Details][16.6 Viewing Prompt Details]]
  - [[#Updating-Prompts][16.7 Updating Prompts]]
  - [[#Setting-Without-Sending][16.8 Setting Without Sending]]
  - [[#Example-Usage][16.9 Example Usage]]
- [[#Remote-Providers][17 Remote Providers]]
  - [[#Overview-3][17.1 Overview]]
  - [[#Status-Line-Indicators][17.2 Status Line Indicators]]
  - [[#Setting-Up-API-Access][17.3 Setting Up API Access]]
  - [[#Selecting-Remote-Models][17.4 Selecting Remote Models]]
  - [[#Provider-Configuration][17.5 Provider Configuration]]
    - [[#OpenAI-Configuration][17.5.1 OpenAI Configuration]]
    - [[#Claude-Configuration][17.5.2 Claude Configuration]]
    - [[#Gemini-Configuration][17.5.3 Gemini Configuration]]
    - [[#Grok-Configuration][17.5.4 Grok Configuration]]
    - [[#GitHub-Copilot-Configuration][17.5.5 GitHub Copilot
      Configuration]]
    - [[#Codestral-Configuration][17.5.6 Codestral Configuration]]
  - [[#History-Management][17.6 History Management]]
- [[#Ollama-Cloud-Models][18 Ollama Cloud Models]]
  - [[#Overview-4][18.1 Overview]]
  - [[#Available-Cloud-Models][18.2 Available Cloud Models]]
  - [[#Selecting-Cloud-Models][18.3 Selecting Cloud Models]]
  - [[#Cloud-Authentication][18.4 Cloud Authentication]]
  - [[#Cloud-Model-Indicator][18.5 Cloud Model Indicator]]
  - [[#Configuration-3][18.6 Configuration]]
- [[#Global-System-Prompt][19 Global System Prompt]]
  - [[#Overview-5][19.1 Overview]]
  - [[#Configuration-4][19.2 Configuration]]
  - [[#Toggling-Global-Prompt][19.3 Toggling Global Prompt]]
  - [[#How-It-Works-1][19.4 How It Works]]
- [[#Advanced-Usage][20 Advanced Usage]]
  - [[#Managing-Token-Usage][20.1 Managing Token Usage]]
  - [[#Customizing-the-Interface][20.2 Customizing the Interface]]
    - [[#Streaming-Options][20.2.1 Streaming Options]]
    - [[#Debug-Mode][20.2.2 Debug Mode]]
  - [[#Editing-Conversation-History][20.3 Editing Conversation History]]
  - [[#Advanced-System-Prompt-Management][20.4 Advanced System Prompt
    Management]]
    - [[#Setting-a-system-prompt-without-sending][20.4.1 Setting a
      system prompt without sending]]
    - [[#Using-a-system-prompt-from-Fabric][20.4.2 Using a system prompt
      from Fabric]]
  - [[#Using-Direct-API-Access][20.5 Using Direct API Access]]
- [[#API-Reference][21 API Reference]]
  - [[#Interactive-Functions][21.1 Interactive Functions]]
  - [[#Core-Functions][21.2 Core Functions]]
  - [[#Customization-Functions][21.3 Customization Functions]]
- [[#FAQ][22 Frequently Asked Questions]]
  - [[#General-Questions][22.1 General Questions]]
    - [[#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f][22.1.1
      What is the difference between Ollama Buddy and other AI
      assistants?]]
    - [[#Does-Ollama-Buddy-require-an-internet-connection_003f][22.1.2
      Does Ollama Buddy require an internet connection?]]
    - [[#How-do-I-use-Ollama-cloud-models_003f][22.1.3 How do I use
      Ollama cloud models?]]
    - [[#Which-models-work-best-with-Ollama-Buddy_003f][22.1.4 Which
      models work best with Ollama Buddy?]]
    - [[#How-much-RAM-do-I-need_003f][22.1.5 How much RAM do I need?]]
  - [[#Usage-Questions][22.2 Usage Questions]]
    - [[#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f][22.2.1
      How do I cancel a request that's taking too long?]]
    - [[#How-can-I-save-my-conversations_003f][22.2.2 How can I save my
      conversations?]]
    - [[#Can-I-use-multiple-models-in-the-same-conversation_003f][22.2.3
      Can I use multiple models in the same conversation?]]
    - [[#How-do-I-clear-the-conversation-history_003f][22.2.4 How do I
      clear the conversation history?]]
    - [[#How-can-I-create-a-custom-command_003f][22.2.5 How can I create
      a custom command?]]
    - [[#How-can-I-manage-context-windows_003f][22.2.6 How can I manage
      context windows?]]
    - [[#What-happens-when-I-exceed-the-context-limit_003f][22.2.7 What
      happens when I exceed the context limit?]]
    - [[#How-do-I-create-effective-system-prompts_003f][22.2.8 How do I
      create effective system prompts?]]
    - [[#Where-are-my-system-prompts-stored_003f][22.2.9 Where are my
      system prompts stored?]]
  - [[#Troubleshooting-2][22.3 Troubleshooting]]
    - [[#Ollama-Buddy-shows-_0022OFFLINE_0022-status][22.3.1 Ollama
      Buddy shows "OFFLINE" status]]
    - [[#Responses-are-slow-or-the-model-seems-to-hang][22.3.2 Responses
      are slow or the model seems to hang]]
    - [[#Getting-_0022error-parsing-model_0022-when-pulling-a-model][22.3.3
      Getting "error parsing model" when pulling a model]]
    - [[#Model-responses-are-low-quality-or-truncated][22.3.4 Model
      responses are low quality or truncated]]
    - [[#How-do-system-prompts-differ-from-regular-prompts_003f][22.3.5
      How do system prompts differ from regular prompts?]]
    - [[#What-is-the-global-system-prompt_003f][22.3.6 What is the
      global system prompt?]]
    - [[#How-do-I-use-image-analysis_002fvision-features_003f][22.3.7
      How do I use image analysis/vision features?]]
    - [[#Can-I-share-system-prompts-between-different-installations_003f][22.3.8
      Can I share system prompts between different installations?]]
- [[#Troubleshooting][23 Troubleshooting]]
  - [[#Common-Issues][23.1 Common Issues]]
    - [[#Connection-Problems][23.1.1 Connection Problems]]
    - [[#Model-Problems][23.1.2 Model Problems]]
    - [[#Interface-Issues][23.1.3 Interface Issues]]
  - [[#Debugging][23.2 Debugging]]
    - [[#Enable-Debug-Mode][23.2.1 Enable Debug Mode]]
    - [[#Check-Logs][23.2.2 Check Logs]]
    - [[#Report-Issues][23.2.3 Report Issues]]
- [[#Contributing][24 Contributing]]
  - [[#Getting-Started][24.1 Getting Started]]
  - [[#Development-Setup][24.2 Development Setup]]
    - [[#Required-Tools][24.2.1 Required Tools]]
    - [[#Recommended-Packages][24.2.2 Recommended Packages]]
  - [[#Coding-Guidelines][24.3 Coding Guidelines]]
  - [[#Testing][24.4 Testing]]
    - [[#Run-Existing-Tests][24.4.1 Run Existing Tests]]
    - [[#Adding-New-Tests][24.4.2 Adding New Tests]]
  - [[#Feature-Requests-and-Bug-Reports][24.5 Feature Requests and Bug
    Reports]]
    - [[#User-System-Prompts-Issues][24.5.1 User System Prompts Issues]]
- [[#Index][Index]]

--------------

<<Introduction>>

Next: [[#Installation][Installation]], Previous: [[#Top][Ollama Buddy]],
Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 1 Introduction [[#Introduction-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Introduction-1
:CLASS: chapter
:END:
- [[#What-is-Ollama-Buddy_003f][What is Ollama Buddy?]]
- [[#Why-Use-Ollama-Buddy_003f][Why Use Ollama Buddy?]]
- [[#Prerequisites][Prerequisites]]

<<What-is-Ollama-Buddy_003f>>
*** 1.1 What is Ollama Buddy? [[#What-is-Ollama-Buddy_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-is-ollama-buddy
:CLASS: section
:END:
Ollama Buddy is an Emacs package that provides a friendly AI assistant
interface to Ollama, a tool for running large language models (LLMs)
locally on your computer. It allows you to interact with AI models
directly from within Emacs for various tasks such as:

- Code refactoring and explanation
- Writing assistance and proofreading
- Generating Git commit messages
- Dictionary lookups and language assistance
- Custom AI-powered workflows via roles
- Using pre-built prompt templates from Fabric
- Utilizing Awesome ChatGPT Prompts
- Integrating with commercial APIs (OpenAI, Claude, Gemini, Grok,
  Copilot, Codestral)
- Using Ollama cloud models
- Including files as context in conversations
- Real-time web search to provide current information to LLMs

Instead of context-switching to web interfaces or terminal applications,
Ollama Buddy brings the power of local LLMs right into your Emacs
workflow.

<<Why-Use-Ollama-Buddy_003f>>
*** 1.2 Why Use Ollama Buddy? [[#Why-Use-Ollama-Buddy_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: why-use-ollama-buddy
:CLASS: section
:END:
- *Privacy*: All interactions happen locally with Ollama - no data sent
  to external services unless you use commercial APIs
- *Integration*: Seamlessly fits into your existing Emacs workflow
- *Flexibility*: Supports multiple models, parameter tuning, and custom
  commands
- *Efficiency*: Quick access to AI assistance without leaving your
  editor
- *Extensibility*: Create custom roles and commands for your specific
  needs
- *File Support*: Include text files, code, and documentation directly
  in conversations

<<Prerequisites>>
*** 1.3 Prerequisites [[#Prerequisites][¶]]
:PROPERTIES:
:CUSTOM_ID: prerequisites
:CLASS: section
:END:
Before using Ollama Buddy, you need:

- Emacs 28.1 or later
- Ollama installed and running on your system (see
  [[https://ollama.ai]])
- At least one language model pulled into Ollama
- (Optional) API keys for OpenAI or Claude if you want to use those
  services

--------------

<<Installation>>

Next: [[#Configuration][Configuration]], Previous:
[[#Introduction][Introduction]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 2 Installation [[#Installation-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Installation-1
:CLASS: chapter
:END:
- [[#Installing-Ollama][Installing Ollama]]
- [[#Package-Installation][Package Installation]]
- [[#Dependencies][Dependencies]]
- [[#Communication-Backends][Communication Backends]]
- [[#API-Key-Setup][API Key Setup]]

<<Installing-Ollama>>
*** 2.1 Installing Ollama [[#Installing-Ollama][¶]]
:PROPERTIES:
:CUSTOM_ID: installing-ollama
:CLASS: section
:END:
Before installing Ollama Buddy, you need to install Ollama itself:

1. Visit [[https://ollama.ai]] and download the installer for your
   platform
2. Install and run Ollama according to the instructions
3. Pull at least one model using =ollama pull llama3:latest= (or another
   model of your choice)

<<Package-Installation>>
*** 2.2 Package Installation [[#Package-Installation][¶]]
:PROPERTIES:
:CUSTOM_ID: package-installation
:CLASS: section
:END:
- [[#Using-package_002eel][Using package.el]]
- [[#Using-use_002dpackage][Using use-package]]
- [[#Manual-Installation][Manual Installation]]

<<Using-package_002eel>>
**** 2.2.1 Using package.el [[#Using-package_002eel][¶]]
:PROPERTIES:
:CUSTOM_ID: using-package.el
:CLASS: subsection
:END:
The recommended way to install Ollama Buddy is through MELPA:

#+begin_src example-preformatted
M-x package-install RET ollama-buddy RET
#+end_src

<<Using-use_002dpackage>>
**** 2.2.2 Using use-package [[#Using-use_002dpackage][¶]]
:PROPERTIES:
:CUSTOM_ID: using-use-package
:CLASS: subsection
:END:
If you use =use-package=, add the following to your Emacs configuration:

#+begin_src example-preformatted
(use-package ollama-buddy
  :ensure t
  :bind ("C-c o" . ollama-buddy-menu))
#+end_src

With a default model:

#+begin_src example-preformatted
(use-package ollama-buddy
  :ensure t
  :bind ("C-c o" . ollama-buddy-menu)
  :custom (ollama-buddy-default-model "llama3:latest"))
#+end_src

<<Manual-Installation>>
**** 2.2.3 Manual Installation [[#Manual-Installation][¶]]
:PROPERTIES:
:CUSTOM_ID: manual-installation
:CLASS: subsection
:END:
To install manually:

1. Clone the repository:

   #+begin_src example-preformatted
   git clone https://github.com/captainflasmr/ollama-buddy.git
   #+end_src

2. Add to your configuration:

   #+begin_src example-preformatted
   (add-to-list 'load-path "/path/to/ollama-buddy")
   (require 'ollama-buddy)
   (global-set-key (kbd "C-c o") #'ollama-buddy-menu)
   #+end_src

<<Dependencies>>
*** 2.3 Dependencies [[#Dependencies][¶]]
:PROPERTIES:
:CUSTOM_ID: dependencies
:CLASS: section
:END:
Ollama Buddy requires the following:

- Emacs 28.1 or later
- (Optional) transient 0.4.0+ for advanced menu system

Built-in packages used: json, cl-lib, url, subr-x, dired, org, savehist.

<<Communication-Backends>>
*** 2.4 Communication Backends [[#Communication-Backends][¶]]
:PROPERTIES:
:CUSTOM_ID: communication-backends
:CLASS: section
:END:
Ollama Buddy supports two communication backends:

- =network-process (default)= :: Uses Emacs built-in network process.

- =curl= :: Uses external curl command for requests. Useful as a
  fallback when network process has issues.

To switch backends:

#+begin_src example-preformatted
M-x ollama-buddy-switch-communication-backend
#+end_src

or press =C-c e= in the chat buffer.

To configure curl backend:

#+begin_src example-preformatted
(require 'ollama-buddy-curl)
(setq ollama-buddy-communication-backend 'curl)
(setq ollama-buddy-curl-executable "curl")
(setq ollama-buddy-curl-timeout 300)
#+end_src

<<API-Key-Setup>>
*** 2.5 API Key Setup [[#API-Key-Setup][¶]]
:PROPERTIES:
:CUSTOM_ID: api-key-setup
:CLASS: section
:END:
If you want to use OpenAI or Claude integration, you'll need to set up
API keys securely:

1. Use Emacs built-in auth-source for secure storage
2. Add to your auth sources (e.g., ~/.authinfo.gpg):

   #+begin_src example-preformatted
   machine api.openai.com login apikey password YOUR_OPENAI_API_KEY_HERE
   machine api.anthropic.com login apikey password YOUR_CLAUDE_API_KEY_HERE
   #+end_src

3. Alternatively, set the variables directly (less secure):

   #+begin_src example-preformatted
   (setq ollama-buddy-openai-api-key "your-openai-key")
   (setq ollama-buddy-claude-api-key "your-claude-key")
   #+end_src

--------------

<<Configuration>>

Next: [[#Quick-Start][Quick Start]], Previous:
[[#Installation][Installation]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 3 Configuration [[#Configuration-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Configuration-1
:CLASS: chapter
:END:
- [[#Basic-Configuration][Basic Configuration]]
- [[#Display-Options][Display Options]]
- [[#File-Attachment-Configuration][File Attachment Configuration]]
- [[#Directory-Configuration][Directory Configuration]]
- [[#History-and-Session-Configuration][History and Session
  Configuration]]
- [[#Context-Management-Configuration][Context Management
  Configuration]]
- [[#External-API-Configuration][External API Configuration]]
- [[#Global-System-Prompt-Configuration][Global System Prompt
  Configuration]]
- [[#Vision-Configuration][Vision Configuration]]
- [[#Cloud-Model-Configuration][Cloud Model Configuration]]
- [[#Awesome-ChatGPT-Prompts-Configuration][Awesome ChatGPT Prompts
  Configuration]]

<<Basic-Configuration>>
*** 3.1 Basic Configuration [[#Basic-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: basic-configuration
:CLASS: section
:END:
Here are the essential configuration options:

- =ollama-buddy-default-model= :: Set your preferred default model.

  #+begin_src example-preformatted
  (setq ollama-buddy-default-model "llama3:latest")
  #+end_src

- =ollama-buddy-host= :: Host where Ollama server is running (default:
  "localhost").

  #+begin_src example-preformatted
  (setq ollama-buddy-host "localhost")
  #+end_src

- =ollama-buddy-port= :: Port where Ollama server is running (default:
  11434).

  #+begin_src example-preformatted
  (setq ollama-buddy-port 11434)
  #+end_src

<<Display-Options>>
*** 3.2 Display Options [[#Display-Options][¶]]
:PROPERTIES:
:CUSTOM_ID: display-options
:CLASS: section
:END:
Customize the appearance and behavior of Ollama Buddy:

- =ollama-buddy-convert-markdown-to-org= :: Whether to automatically
  convert markdown to org-mode format in responses (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-convert-markdown-to-org t)
  #+end_src

- =ollama-buddy-display-token-stats= :: Whether to display token usage
  statistics after responses (default: nil).

  #+begin_src example-preformatted
  (setq ollama-buddy-display-token-stats t)
  #+end_src

- =ollama-buddy-streaming-enabled= :: Whether to use streaming mode for
  responses (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-streaming-enabled t)
  #+end_src

- =ollama-buddy-auto-scroll= :: Whether to auto-scroll the chat buffer
  during streaming output (default: nil).

  #+begin_src example-preformatted
  (setq ollama-buddy-auto-scroll t)
  #+end_src

- =ollama-buddy-pulse-response= :: Whether to pulse/flash the response
  text when streaming completes (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-pulse-response t)
  #+end_src

<<File-Attachment-Configuration>>
*** 3.3 File Attachment Configuration [[#File-Attachment-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: file-attachment-configuration
:CLASS: section
:END:
Configure file attachment behavior:

- =ollama-buddy-max-file-size= :: Maximum size for attached files in
  bytes (default: 10MB).

  #+begin_src example-preformatted
  (setq ollama-buddy-max-file-size (* 10 1024 1024))  ; 10MB
  #+end_src

- =ollama-buddy-supported-file-types= :: List of regex patterns for
  supported file types (default includes text, code, and configuration
  files).

  #+begin_src example-preformatted
  (setq ollama-buddy-supported-file-types
        '("\\.txt$" "\\.md$" "\\.org$" "\\.py$" "\\.js$" "\\.el$"))
  #+end_src

<<Directory-Configuration>>
*** 3.4 Directory Configuration [[#Directory-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: directory-configuration
:CLASS: section
:END:
Customize where Ollama Buddy stores its files:

- =ollama-buddy-sessions-directory= :: Directory for storing session
  files.

  #+begin_src example-preformatted
  (setq ollama-buddy-sessions-directory 
        (expand-file-name "ollama-buddy-sessions" user-emacs-directory))
  #+end_src

- =ollama-buddy-roles-directory= :: Directory for storing role preset
  files.

  #+begin_src example-preformatted
  (setq ollama-buddy-roles-directory
        (expand-file-name "ollama-buddy-presets" user-emacs-directory))
  #+end_src

- =ollama-buddy-modelfile-directory= :: Directory for storing temporary
  Modelfiles.

  #+begin_src example-preformatted
  (setq ollama-buddy-modelfile-directory
        (expand-file-name "ollama-buddy-modelfiles" user-emacs-directory))
  #+end_src

- =ollama-buddy-awesome-local-dir= :: Directory for storing Awesome
  ChatGPT Prompts.

  #+begin_src example-preformatted
  (setq ollama-buddy-awesome-local-dir
        (expand-file-name "awesome-chatgpt-prompts" user-emacs-directory))
  #+end_src

<<History-and-Session-Configuration>>
*** 3.5 History and Session Configuration [[#History-and-Session-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: history-and-session-configuration
:CLASS: section
:END:
Configure how conversation history is managed:

- =ollama-buddy-history-enabled= :: Whether to use conversation history
  in Ollama requests (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-history-enabled t)
  #+end_src

- =ollama-buddy-max-history-length= :: Maximum number of message pairs
  to keep in conversation history (default: 10).

  #+begin_src example-preformatted
  (setq ollama-buddy-max-history-length 10)
  #+end_src

- =ollama-buddy-show-history-indicator= :: Whether to show the history
  indicator in the header line (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-show-history-indicator t)
  #+end_src

<<Context-Management-Configuration>>
*** 3.6 Context Management Configuration [[#Context-Management-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: context-management-configuration
:CLASS: section
:END:
Configure how Ollama Buddy handles context management:

- =ollama-buddy-show-context-percentage= :: Whether to show context
  percentage in the status bar (default: nil).

  #+begin_src example-preformatted
  (setq ollama-buddy-show-context-percentage t)
  #+end_src

- =ollama-buddy-fallback-context-sizes= :: Mapping of model names to
  their default context sizes.

  #+begin_src example-preformatted
  (setq ollama-buddy-fallback-context-sizes
    '(("llama3:8b" . 4096)
      ("codellama:7b" . 8192)))
  #+end_src

- =ollama-buddy-max-history-length= :: Maximum number of message pairs
  to keep (affects context usage).

  #+begin_src example-preformatted
  (setq ollama-buddy-max-history-length 10)
  #+end_src

<<External-API-Configuration>>
*** 3.7 External API Configuration [[#External-API-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: external-api-configuration
:CLASS: section
:END:
For remote provider integration, configure API keys using auth-source
(recommended):

#+begin_src example-preformatted
;; In ~/.authinfo.gpg
machine ollama-buddy-openai login apikey password YOUR_KEY
machine ollama-buddy-claude login apikey password YOUR_KEY
machine ollama-buddy-gemini login apikey password YOUR_KEY
machine ollama-buddy-grok login apikey password YOUR_KEY
machine ollama-buddy-codestral login apikey password YOUR_KEY
#+end_src

Then in your Emacs config:

#+begin_src example-preformatted
(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password
       :host "ollama-buddy-openai" :user "apikey"))
#+end_src

- [[#Provider-Prefixes][Provider Prefixes]]

<<Provider-Prefixes>>
**** 3.7.1 Provider Prefixes [[#Provider-Prefixes][¶]]
:PROPERTIES:
:CUSTOM_ID: provider-prefixes
:CLASS: subsection
:END:
Each provider uses a prefix to identify its models:

- =a:= - OpenAI (e.g., =a:gpt-4=)
- =c:= - Claude (e.g., =c:claude-3-opus=)
- =g:= - Gemini (e.g., =g:gemini-pro=)
- =k:= - Grok (e.g., =k:grok-1=)
- =p:= - GitHub Copilot (e.g., =p:gpt-4o=)
- =s:= - Codestral (e.g., =s:codestral-latest=)
- =o:= - Ollama local (when remote providers are loaded)

<<Global-System-Prompt-Configuration>>
*** 3.8 Global System Prompt Configuration [[#Global-System-Prompt-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: global-system-prompt-configuration
:CLASS: section
:END:
Configure a persistent system prompt for all requests:

- =ollama-buddy-global-system-prompt= :: The global prompt prepended to
  all requests.

  #+begin_src example-preformatted
  (setq ollama-buddy-global-system-prompt
    "Format responses in plain prose. Avoid markdown tables.")
  #+end_src

- =ollama-buddy-global-system-prompt-enabled= :: Whether to enable the
  global prompt (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-global-system-prompt-enabled t)
  #+end_src

<<Vision-Configuration>>
*** 3.9 Vision Configuration [[#Vision-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: vision-configuration
:CLASS: section
:END:
Configure vision support for image analysis:

- =ollama-buddy-vision-enabled= :: Whether to enable vision support
  (default: t).

- =ollama-buddy-vision-models= :: Models known to support vision
  capabilities.

  #+begin_src example-preformatted
  (setq ollama-buddy-vision-models
        '("gemma3:4b" "llama3.2:3b" "llama3.2:8b"))
  #+end_src

- =ollama-buddy-image-formats= :: Supported image file formats.

<<Cloud-Model-Configuration>>
*** 3.10 Cloud Model Configuration [[#Cloud-Model-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: cloud-model-configuration
:CLASS: section
:END:
Configure Ollama cloud models:

- =ollama-buddy-cloud-models= :: List of available cloud models.

  #+begin_src example-preformatted
  (setq ollama-buddy-cloud-models
    '("qwen3-coder:480b-cloud"
      "deepseek-v3.1:671b-cloud"))
  #+end_src

- =ollama-buddy-ollama-executable= :: Path to the ollama CLI (default:
  "ollama").

<<Awesome-ChatGPT-Prompts-Configuration>>
*** 3.11 Awesome ChatGPT Prompts Configuration [[#Awesome-ChatGPT-Prompts-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: awesome-chatgpt-prompts-configuration
:CLASS: section
:END:
Configure the Awesome ChatGPT Prompts integration:

- =ollama-buddy-awesome-repo-url= :: URL of the Awesome ChatGPT Prompts
  GitHub repository.

  #+begin_src example-preformatted
  (setq ollama-buddy-awesome-repo-url "https://github.com/f/awesome-chatgpt-prompts.git")
  #+end_src

- =ollama-buddy-awesome-update-on-startup= :: Whether to automatically
  update prompts when Emacs starts.

  #+begin_src example-preformatted
  (setq ollama-buddy-awesome-update-on-startup nil)
  #+end_src

- =ollama-buddy-awesome-categorize-prompts= :: Whether to categorize
  prompts based on common keywords.

  #+begin_src example-preformatted
  (setq ollama-buddy-awesome-categorize-prompts t)
  #+end_src

--------------

<<Quick-Start>>

Next: [[#Core-Features][Core Features]], Previous:
[[#Configuration][Configuration]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 4 Quick Start [[#Quick-Start-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Quick-Start-1
:CLASS: chapter
:END:
- [[#Basic-Usage][Basic Usage]]
- [[#Common-Operations][Common Operations]]

<<Basic-Usage>>
*** 4.1 Basic Usage [[#Basic-Usage][¶]]
:PROPERTIES:
:CUSTOM_ID: basic-usage
:CLASS: section
:END:
1. Launch Ollama Buddy:

   #+begin_src example-preformatted
   M-x ollama-buddy-menu
   #+end_src

   or use your configured keybinding (e.g., =C-c o=).

2. The menu will show available options. Press the corresponding key for
   the action you want.

3. To open the chat interface, press =o= or select "Open Chat".

4. In the chat buffer, type your prompt and press =C-c C-c= to send it.

5. The AI will respond in the chat buffer.

<<Common-Operations>>
*** 4.2 Common Operations [[#Common-Operations][¶]]
:PROPERTIES:
:CUSTOM_ID: common-operations
:CLASS: section
:END:
- Sending text from a file :: Select text in any buffer, then press
  =C-c o= and choose "Send Region" (or press =l=).

- Refactoring code :: Select code, press =C-c o=, then choose "Refactor
  Code" (or press =r=).

- Generating a commit message :: Select your changes, press =C-c o=,
  then choose "Git Commit Message" (or press =g=).

- Changing models :: Press =C-c m= to switch between available models,
  or =C-u C-c m= to select from cloud models.

- Using the transient menu :: Press =C-c O= for the main transient menu
  with all features organized by category.

- Attaching files :: Press =C-c C-a= to attach a file, or use the
  transient menu "Attachments" section.

- Toggling reasoning visibility :: Press =C-c V= to hide or show
  reasoning/thinking sections in responses.

- Using Awesome ChatGPT Prompts :: Select text, press =C-c o=, then =a=
  for the Awesome prompts menu, then =s= to send with a prompt.

- Using Fabric patterns :: Select text, press =C-c o=, then =f= for the
  Fabric menu, then =s= to send with a pattern.

- Getting help :: In the chat buffer, press =C-c h= to display the help
  screen with available commands and models.

--------------

<<Core-Features>>

Next: [[#Chat-Interface][Chat Interface]], Previous:
[[#Quick-Start][Quick Start]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 5 Core Features [[#Core-Features-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Core-Features-1
:CLASS: chapter
:END:
- [[#Chat-Interface-1][Chat Interface]]
- [[#Pre_002dbuilt-Commands][Pre-built Commands]]
- [[#Model-Management][Model Management]]
- [[#Parameter-Control-1][Parameter Control]]
- [[#Roles-and-Custom-Commands][Roles and Custom Commands]]
- [[#Prompt-Template-Collections][Prompt Template Collections]]
- [[#External-API-Integration][External API Integration]]
- [[#Vision-Support][Vision Support]]
- [[#File-Attachments-1][File Attachments]]

<<Chat-Interface-1>>
*** 5.1 Chat Interface [[#Chat-Interface-1][¶]]
:PROPERTIES:
:CUSTOM_ID: chat-interface
:CLASS: section
:END:
The chat interface is the main way to interact with Ollama Buddy:

- Persistent conversation with history
- Markdown to Org-mode conversion
- Model-specific colors
- System prompt support
- Parameter customization
- Reasoning/thinking section visibility control
- Context window management and monitoring
- Real-time context usage display
- Context size validation before sending prompts
- Customizable context thresholds and warnings
- File attachment support

<<Pre_002dbuilt-Commands>>
*** 5.2 Pre-built Commands [[#Pre_002dbuilt-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: pre-built-commands
:CLASS: section
:END:
Ollama Buddy comes with several pre-built commands:

- Code Refactoring :: Improves code while maintaining functionality

- Code Description :: Explains what code does and how it works

- Git Commit Messages :: Generates meaningful commit messages from code
  changes

- Dictionary Lookups :: Provides comprehensive word definitions

- Synonym Finder :: Suggests alternative words with context

- Proofreading :: Corrects grammar, style, and spelling

<<Model-Management>>
*** 5.3 Model Management [[#Model-Management][¶]]
:PROPERTIES:
:CUSTOM_ID: model-management
:CLASS: section
:END:
- Switch between any model available in Ollama
- Use ChatGPT and Claude models with API keys
- Pull new models directly from the interface
- View model information and statistics
- Delete models you no longer need
- Import GGUF files to create new models

<<Parameter-Control-1>>
*** 5.4 Parameter Control [[#Parameter-Control-1][¶]]
:PROPERTIES:
:CUSTOM_ID: parameter-control
:CLASS: section
:END:
- Fine-tune model behavior with customizable parameters
- Save and use parameter profiles for different use cases
- Command-specific parameter settings
- Real-time parameter adjustment

<<Roles-and-Custom-Commands>>
*** 5.5 Roles and Custom Commands [[#Roles-and-Custom-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: roles-and-custom-commands
:CLASS: section
:END:
- Create custom command sets for specific workflows
- Design specialized AI assistants with custom system prompts
- Save and switch between different roles
- Share role configurations across your team

<<Prompt-Template-Collections>>
*** 5.6 Prompt Template Collections [[#Prompt-Template-Collections][¶]]
:PROPERTIES:
:CUSTOM_ID: prompt-template-collections
:CLASS: section
:END:
- Use pre-built prompt patterns from Fabric project
- Utilize the Awesome ChatGPT Prompts collection
- Apply specialized prompts to your content with one command
- Browse prompts by category

<<External-API-Integration>>
*** 5.7 External API Integration [[#External-API-Integration][¶]]
:PROPERTIES:
:CUSTOM_ID: external-api-integration
:CLASS: section
:END:
- Connect to OpenAI's ChatGPT API (prefix: =a:=)
- Connect to Anthropic's Claude API (prefix: =c:=)
- Connect to Google Gemini API (prefix: =g:=)
- Connect to X Grok API (prefix: =k:=)
- Connect to GitHub Copilot Chat API (prefix: =p:=)
- Connect to Mistral Codestral API (prefix: =s:=)
- Use Ollama cloud models (indicated by =☁=)
- Seamlessly switch between local and cloud models
- Secure API key management via auth-source

<<Vision-Support>>
*** 5.8 Vision Support [[#Vision-Support][¶]]
:PROPERTIES:
:CUSTOM_ID: vision-support
:CLASS: section
:END:
- Analyze images with vision-capable models
- Automatic image detection in prompts
- Support for PNG, JPG, JPEG, WebP, and GIF formats
- Configurable list of vision-capable models

<<File-Attachments-1>>
*** 5.9 File Attachments [[#File-Attachments-1][¶]]
:PROPERTIES:
:CUSTOM_ID: file-attachments
:CLASS: section
:END:
- Attach text files, code, and documentation to conversations
- Automatic context inclusion with proper token counting
- Session persistence for attachments
- Support for various file types
- Dired integration for bulk file attachment

--------------

<<Chat-Interface>>

Next: [[#Working-with-Models][Working with Models]], Previous:
[[#Core-Features][Core Features]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 6 Chat Interface [[#Chat-Interface-2][¶]]
:PROPERTIES:
:CUSTOM_ID: Chat-Interface-2
:CLASS: chapter
:END:
- [[#Opening-the-Chat][Opening the Chat]]
- [[#Interface-Overview][Interface Overview]]
- [[#Sending-Prompts][Sending Prompts]]
- [[#System-Prompts][System Prompts]]
- [[#Markdown-to-Org-Conversion][Markdown to Org Conversion]]
- [[#Reasoning-Visibility-Control][Reasoning Visibility Control]]

<<Opening-the-Chat>>
*** 6.1 Opening the Chat [[#Opening-the-Chat][¶]]
:PROPERTIES:
:CUSTOM_ID: opening-the-chat
:CLASS: section
:END:
To open the chat interface:

1. Use =M-x ollama-buddy-menu= or your configured keybinding
2. Press =o= to select "Open Chat"
3. A new buffer will open with the Ollama Buddy chat interface

<<Interface-Overview>>
*** 6.2 Interface Overview [[#Interface-Overview][¶]]
:PROPERTIES:
:CUSTOM_ID: interface-overview
:CLASS: section
:END:
The chat interface consists of:

- A welcome message with available models and providers
- Conversation history (previous prompts and responses)
- A prompt area for entering your queries
- A header line with status information including:
  - Cloud model indicator (=☁=)
  - Loaded provider indicators (=acgks=)
  - Context usage bar (when enabled)
  - History count (=H5/10=)
  - Current model name
  - System prompt indicator
  - Modified parameters
- Context warnings and validation
- Attachment indicators when files are attached

<<Sending-Prompts>>
*** 6.3 Sending Prompts [[#Sending-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: sending-prompts
:CLASS: section
:END:
To send a prompt to the AI:

1. Type your message in the prompt area (after ">> PROMPT:")
2. Press =C-c C-c= to send
3. Wait for the AI to generate a response

You can also:

- Use =M-p= and =M-n= to navigate through prompt history
- Press =C-c k= to cancel a request if it's taking too long

<<System-Prompts>>
*** 6.4 System Prompts [[#System-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: system-prompts
:CLASS: section
:END:
System prompts allow you to define the AI's behavior:

- Setting a system prompt :: Type your system prompt, then press =C-c s=

- Viewing the current system prompt :: Press =C-c C-s=

- Resetting the system prompt :: Press =C-c r=

- Using a pre-built prompt :: Use Fabric patterns (=C-c f p=) or Awesome
  ChatGPT prompts (=C-c w p=)

Example system prompt:

#+begin_src example-preformatted
You are a programming expert who specializes in Python. 
Provide concise, efficient solutions with explanations.
#+end_src

<<Markdown-to-Org-Conversion>>
*** 6.5 Markdown to Org Conversion [[#Markdown-to-Org-Conversion][¶]]
:PROPERTIES:
:CUSTOM_ID: markdown-to-org-conversion
:CLASS: section
:END:
By default, Ollama Buddy converts markdown in responses to Org-mode
syntax:

- Code blocks are converted to Org-mode source blocks
- Headers are converted to Org-mode headings
- Lists are properly formatted
- Links are converted to Org-mode format

To toggle this feature:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-markdown-conversion
#+end_src

or press =C-c C-o= in the chat buffer.

<<Reasoning-Visibility-Control>>
*** 6.6 Reasoning Visibility Control [[#Reasoning-Visibility-Control][¶]]
:PROPERTIES:
:CUSTOM_ID: reasoning-visibility-control
:CLASS: section
:END:
Ollama Buddy can hide reasoning/thinking sections in responses, making
the output cleaner:

- Toggle visibility with =C-c V= or
  =M-x ollama-buddy-toggle-reasoning-visibility=
- Configure markers with the =ollama-buddy-reasoning-markers= variable
- When hidden, a status message shows the current reasoning section
  (e.g., "Think...")
- Header line indicates when reasoning is hidden with "REASONING HIDDEN"
  text

This feature helps focus on final answers while preserving the option to
view the full reasoning process.

--------------

<<Working-with-Models>>

Next: [[#Context-Management][Context Management]], Previous:
[[#Chat-Interface][Chat Interface]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 7 Working with Models [[#Working-with-Models-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Working-with-Models-1
:CLASS: chapter
:END:
- [[#Available-Models][Available Models]]
- [[#Switching-Models][Switching Models]]
- [[#Local-vs_002e-Cloud-Models][Local vs. Cloud Models]]
- [[#Managing-Models][Managing Models]]
- [[#Pulling-New-Models][Pulling New Models]]
- [[#Importing-GGUF-Files][Importing GGUF Files]]
- [[#Vision-Models][Vision Models]]
- [[#Multishot-Mode][Multishot Mode]]

<<Available-Models>>
*** 7.1 Available Models [[#Available-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: available-models
:CLASS: section
:END:
Ollama Buddy displays available models in the chat interface. Each model
is assigned a letter for quick selection.

To view detailed model information:

#+begin_src example-preformatted
M-x ollama-buddy-show-model-status
#+end_src

or press =C-c v= in the chat buffer.

<<Switching-Models>>
*** 7.2 Switching Models [[#Switching-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: switching-models
:CLASS: section
:END:
To change the current model:

1. Press =C-c m= in the chat buffer
2. Select a model from the completion list
3. The new model will be used for future requests

You can also switch models from the main menu with =m=.

<<Local-vs_002e-Cloud-Models>>
*** 7.3 Local vs. Cloud Models [[#Local-vs_002e-Cloud-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: local-vs.-cloud-models
:CLASS: section
:END:
Ollama Buddy supports both local Ollama models and cloud-based models:

- Local models (via Ollama): llama3, codellama, mistral, etc.
- OpenAI models: gpt-3.5-turbo, gpt-4, etc.
- Claude models: claude-3-opus, claude-3-sonnet, etc.

To use cloud models, you need to configure API keys as described in the
Installation chapter.

<<Managing-Models>>
*** 7.4 Managing Models [[#Managing-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-models
:CLASS: section
:END:
Ollama Buddy provides a comprehensive model management interface. To
access it:

#+begin_src example-preformatted
M-x ollama-buddy-manage-models
#+end_src

or press =C-c W= in the chat buffer.

From this interface, you can:

- See which models are currently running
- Pull new models from Ollama Hub
- Delete models you no longer need
- View detailed model information
- Select models for use

<<Pulling-New-Models>>
*** 7.5 Pulling New Models [[#Pulling-New-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: pulling-new-models
:CLASS: section
:END:
To pull a new model:

1. Open the model management interface with =C-c W=
2. Click "[Pull Any Model]" or press the appropriate key
3. Enter the model name (e.g., "phi:latest", "codellama:7b")
4. Wait for the model to download

<<Importing-GGUF-Files>>
*** 7.6 Importing GGUF Files [[#Importing-GGUF-Files][¶]]
:PROPERTIES:
:CUSTOM_ID: importing-gguf-files
:CLASS: section
:END:
You can import custom GGUF model files:

1. Press =C-c W= to open the model management interface
2. Click "[Import GGUF File]" or press the appropriate key
3. Select the GGUF file from your file system
4. Enter a name for the model
5. Optionally provide model parameters
6. Wait for Ollama to create the model

<<Vision-Models>>
*** 7.7 Vision Models [[#Vision-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: vision-models
:CLASS: section
:END:
Some models support vision capabilities for image analysis.

- [[#Configuring-Vision-Support][Configuring Vision Support]]
- [[#Using-Vision-Models][Using Vision Models]]

<<Configuring-Vision-Support>>
**** 7.7.1 Configuring Vision Support [[#Configuring-Vision-Support][¶]]
:PROPERTIES:
:CUSTOM_ID: configuring-vision-support
:CLASS: subsection
:END:
- =ollama-buddy-vision-enabled= :: Whether to enable vision support
  (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-vision-enabled t)
  #+end_src

- =ollama-buddy-vision-models= :: List of models known to support
  vision:

  #+begin_src example-preformatted
  (setq ollama-buddy-vision-models
        '("gemma3:4b" "llama3.2:3b" "llama3.2:8b"))
  #+end_src

- =ollama-buddy-image-formats= :: Supported image file formats:

  #+begin_src example-preformatted
  (setq ollama-buddy-image-formats
        '("\\.png$" "\\.jpg$" "\\.jpeg$" "\\.webp$" "\\.gif$"))
  #+end_src

<<Using-Vision-Models>>
**** 7.7.2 Using Vision Models [[#Using-Vision-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: using-vision-models
:CLASS: subsection
:END:
To analyze an image:

1. Select a vision-capable model
2. Include an image path in your prompt, or use the "Analyze image"
   command
3. The image will be automatically encoded and sent to the model

Image paths can be:

- Quoted paths: ="path/to/image.png"=
- Unquoted paths: =/home/user/image.jpg=

<<Multishot-Mode>>
*** 7.8 Multishot Mode [[#Multishot-Mode][¶]]
:PROPERTIES:
:CUSTOM_ID: multishot-mode
:CLASS: section
:END:
Multishot mode allows you to send the same prompt to multiple models
simultaneously:

1. Type your prompt in the chat buffer
2. Press =C-c M=
3. Enter the sequence of model letters you want to use (e.g., "a,b,c" to
   use models a, b, and c)
4. Note that each item should be separated with a comma
5. Watch as Ollama Buddy processes your request with each model in
   sequence

--------------

<<Context-Management>>

Next: [[#File-Attachments][File Attachments]], Previous:
[[#Working-with-Models][Working with Models]], Up: [[#Top][Ollama
Buddy]]   [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 8 Context Management [[#Context-Management-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Context-Management-1
:CLASS: chapter
:END:
- [[#Understanding-Context-Windows][Understanding Context Windows]]
- [[#Context-Size-Detection][Context Size Detection]]
- [[#Enabling-Context-Monitoring][Enabling Context Monitoring]]
- [[#Context-with-File-Attachments][Context with File Attachments]]
- [[#Context-Management-Commands][Context Management Commands]]
- [[#Token-Estimation][Token Estimation]]
- [[#Managing-Context-in-Practice][Managing Context in Practice]]
- [[#Context-Display-Configuration][Context Display Configuration]]
- [[#Fallback-Context-Sizes][Fallback Context Sizes]]
- [[#Troubleshooting-Context-Issues][Troubleshooting Context Issues]]

<<Understanding-Context-Windows>>
*** 8.1 Understanding Context Windows [[#Understanding-Context-Windows][¶]]
:PROPERTIES:
:CUSTOM_ID: understanding-context-windows
:CLASS: section
:END:
Context windows define how much text (measured in tokens) a model can
process at once. This includes your current prompt, conversation
history, any system prompts, and attached files. Understanding and
managing context is crucial for:

- Preventing errors when context limits are exceeded
- Optimizing model performance for different tasks
- Managing longer conversations efficiently
- Including files without exceeding context limits

<<Context-Size-Detection>>
*** 8.2 Context Size Detection [[#Context-Size-Detection][¶]]
:PROPERTIES:
:CUSTOM_ID: context-size-detection
:CLASS: section
:END:
Ollama Buddy uses multiple methods to determine a model's context size:

1. Built-in mappings for popular models (llama3, mistral, codellama,
   etc.)
2. Custom context sizes set via the =num_ctx= parameter
3. Manual configuration through interactive commands
4. Fallback to reasonable defaults (4096 tokens) for unknown models

<<Enabling-Context-Monitoring>>
*** 8.3 Enabling Context Monitoring [[#Enabling-Context-Monitoring][¶]]
:PROPERTIES:
:CUSTOM_ID: enabling-context-monitoring
:CLASS: section
:END:
Context monitoring is disabled by default. To enable it:

#+begin_src example-preformatted
(setq ollama-buddy-show-context-percentage t)
#+end_src

With context monitoring enabled:

- The status bar shows current/max context usage (e.g., "2048/8192")
- Text formatting indicates usage levels:
  - Normal font: Under 85% usage
  - Bold and underlined: 85-100% usage
  - Inverted: At or exceeding 100% usage
- Warnings appear before sending prompts that exceed limits

<<Context-with-File-Attachments>>
*** 8.4 Context with File Attachments [[#Context-with-File-Attachments][¶]]
:PROPERTIES:
:CUSTOM_ID: context-with-file-attachments
:CLASS: section
:END:
File attachments are included in context calculations:

- Each attached file contributes to the total token count
- The context breakdown shows attachment tokens separately
- File content is included in the request context
- Large files can significantly impact context usage

<<Context-Management-Commands>>
*** 8.5 Context Management Commands [[#Context-Management-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: context-management-commands
:CLASS: section
:END:
- Show Context Information (C-c C) :: Displays a breakdown of current
  context usage, including:

  - Conversation history token count
  - System prompt token count
  - Attachment token count
  - Current prompt token count
  - Total usage percentage

- Set Model Context Size (C-c $) :: Manually configure the context size
  for a specific model.

- Toggle Context Display (C-c %) :: Show or hide the context percentage
  in the status bar.

<<Token-Estimation>>
*** 8.6 Token Estimation [[#Token-Estimation][¶]]
:PROPERTIES:
:CUSTOM_ID: token-estimation
:CLASS: section
:END:
Ollama Buddy estimates token counts using a heuristic approach:

- Each word is multiplied by 1.3 (following common approximations)
- This provides a reasonable estimate for most use cases
- Actual token counts may vary slightly between models

<<Managing-Context-in-Practice>>
*** 8.7 Managing Context in Practice [[#Managing-Context-in-Practice][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-context-in-practice
:CLASS: section
:END:
- [[#Workflow-Strategies][Workflow Strategies]]
- [[#Using-num_005fctx-Parameter][Using num_ctx Parameter]]

<<Workflow-Strategies>>
**** 8.7.1 Workflow Strategies [[#Workflow-Strategies][¶]]
:PROPERTIES:
:CUSTOM_ID: workflow-strategies
:CLASS: subsection
:END:
- [[#Paste_002dand_002dSend-Approach][Paste-and-Send Approach]]
- [[#Preemptive-Checking][Preemptive Checking]]
- [[#History-Length-Management][History Length Management]]

<<Paste_002dand_002dSend-Approach>>
**** 8.7.1.1 Paste-and-Send Approach [[#Paste_002dand_002dSend-Approach][¶]]
:PROPERTIES:
:CUSTOM_ID: paste-and-send-approach
:CLASS: subsubsection
:END:
1. Paste your content into the chat buffer
2. Press the send keybinding
3. If context is exceeded, you'll get a warning dialog
4. Choose whether to proceed or modify your content

<<Preemptive-Checking>>
**** 8.7.1.2 Preemptive Checking [[#Preemptive-Checking][¶]]
:PROPERTIES:
:CUSTOM_ID: preemptive-checking
:CLASS: subsubsection
:END:
1. Paste your content
2. Use C-c C to check context usage
3. If too high:
   - Trim your current prompt
   - Edit conversation history (C-c J)
   - Switch to a larger context model
   - Adjust system prompt length
   - Remove or reduce file attachments

<<History-Length-Management>>
**** 8.7.1.3 History Length Management [[#History-Length-Management][¶]]
:PROPERTIES:
:CUSTOM_ID: history-length-management
:CLASS: subsubsection
:END:
Control context by limiting conversation history:

#+begin_src example-preformatted
(setq ollama-buddy-max-history-length 5)
#+end_src

This keeps only the last 5 message pairs, reducing context usage.

<<Using-num_005fctx-Parameter>>
**** 8.7.2 Using num_ctx Parameter [[#Using-num_005fctx-Parameter][¶]]
:PROPERTIES:
:CUSTOM_ID: using-num_ctx-parameter
:CLASS: subsection
:END:
The =num_ctx= parameter allows you to set a specific context size:

1. Access the parameter menu with C-c P
2. Select =num_ctx=
3. Enter your desired context size
4. Ollama Buddy will respect this limit

<<Context-Display-Configuration>>
*** 8.8 Context Display Configuration [[#Context-Display-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: context-display-configuration
:CLASS: section
:END:
Customize how context information is displayed:

- =ollama-buddy-show-context-percentage= :: Whether to show context
  percentage in the status bar (default: nil).

- =ollama-buddy-context-warning-threshold= :: Percentage at which to
  warn about high context usage (default: 90).

- =ollama-buddy-context-error-threshold= :: Percentage at which to block
  sending (default: 100).

<<Fallback-Context-Sizes>>
*** 8.9 Fallback Context Sizes [[#Fallback-Context-Sizes][¶]]
:PROPERTIES:
:CUSTOM_ID: fallback-context-sizes
:CLASS: section
:END:
Ollama Buddy includes predefined context sizes for popular models. You
can customize these via:

#+begin_src example-preformatted
(setq ollama-buddy-fallback-context-sizes
  '(("llama3:8b" . 4096)
    ("codellama:7b" . 8192)
    ("mistral:7b" . 8192)))
#+end_src

<<Troubleshooting-Context-Issues>>
*** 8.10 Troubleshooting Context Issues [[#Troubleshooting-Context-Issues][¶]]
:PROPERTIES:
:CUSTOM_ID: troubleshooting-context-issues
:CLASS: section
:END:
- Context warnings appear unexpectedly :: - Check if you have a long
    system prompt
  - Review conversation history length
  - Verify the model's actual context size
  - Check if files are attached and their sizes
- Model responses are truncated :: - Increase the =num_ctx= parameter
  - Reduce history length with C-c Y
  - Clear some conversation history
  - Remove large file attachments
- Context calculations seem inaccurate :: - Remember that token
    estimation is approximate
  - Different models may tokenize text differently
  - Use C-c C to see detailed breakdowns

--------------

<<File-Attachments>>

Next: [[#Web-Search][Web Search]], Previous:
[[#Context-Management][Context Management]], Up: [[#Top][Ollama Buddy]]
  [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 9 File Attachments [[#File-Attachments-2][¶]]
:PROPERTIES:
:CUSTOM_ID: File-Attachments-2
:CLASS: chapter
:END:
- [[#Overview][Overview]]
- [[#Supported-File-Types][Supported File Types]]
- [[#Attaching-Files][Attaching Files]]
- [[#Managing-Attachments][Managing Attachments]]
- [[#File-Size-Limits][File Size Limits]]
- [[#How-Attachments-Work][How Attachments Work]]
- [[#File-Attachment-Workflow-Examples][File Attachment Workflow
  Examples]]
- [[#Context-Considerations][Context Considerations]]
- [[#Best-Practices][Best Practices]]
- [[#Troubleshooting-Attachments][Troubleshooting Attachments]]

<<Overview>>
*** 9.1 Overview [[#Overview][¶]]
:PROPERTIES:
:CUSTOM_ID: overview
:CLASS: section
:END:
File attachments allow you to include the contents of text files, code
files, documentation, and configuration files directly in your
conversations with AI models. This feature is particularly useful for:

- Code review and analysis
- Documentation generation
- Configuration file troubleshooting
- Multi-file project discussions
- Research with multiple text sources

<<Supported-File-Types>>
*** 9.2 Supported File Types [[#Supported-File-Types][¶]]
:PROPERTIES:
:CUSTOM_ID: supported-file-types
:CLASS: section
:END:
Ollama Buddy supports a wide range of file types by default:

- Text and Documentation :: =.txt=, =.md=, =.org=

- Programming Languages :: =.py=, =.js=, =.el=, =.cpp=, =.c=, =.java=

- Web Technologies :: =.html=, =.css=, =.json=, =.xml=

- Configuration Files :: =.yaml=, =.yml=, =.toml=, =.ini=, =.cfg=

- Scripts :: =.sh=, =.sql=

You can customize supported file types by modifying
=ollama-buddy-supported-file-types=.

<<Attaching-Files>>
*** 9.3 Attaching Files [[#Attaching-Files][¶]]
:PROPERTIES:
:CUSTOM_ID: attaching-files
:CLASS: section
:END:
- [[#Basic-File-Attachment][Basic File Attachment]]
- [[#Dired-Integration][Dired Integration]]

<<Basic-File-Attachment>>
**** 9.3.1 Basic File Attachment [[#Basic-File-Attachment][¶]]
:PROPERTIES:
:CUSTOM_ID: basic-file-attachment
:CLASS: subsection
:END:
To attach a single file:

1. Press =C-c A= to open the attachment menu
2. Press =a= for "Attach file"
3. Select the file from the file browser
4. The file will be attached and its contents included in future prompts

Alternatively, you can use =C-c C-a= directly.

<<Dired-Integration>>
**** 9.3.2 Dired Integration [[#Dired-Integration][¶]]
:PROPERTIES:
:CUSTOM_ID: dired-integration
:CLASS: subsection
:END:
When working in Dired, you can attach files directly:

- Attach file at point :: Position the cursor on a file and press
  =C-c C-a=

- Attach multiple marked files :: Mark files with =m=, then run
  =M-x ollama-buddy-dired-attach-marked-files=

<<Managing-Attachments>>
*** 9.4 Managing Attachments [[#Managing-Attachments][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-attachments
:CLASS: section
:END:
- [[#Viewing-Attachments][Viewing Attachments]]
- [[#Detaching-Files][Detaching Files]]
- [[#Clearing-All-Attachments][Clearing All Attachments]]

<<Viewing-Attachments>>
**** 9.4.1 Viewing Attachments [[#Viewing-Attachments][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-attachments
:CLASS: subsection
:END:
To see currently attached files:

#+begin_src example-preformatted
M-x ollama-buddy-show-attachments
#+end_src

or press =C-c C-w=.

This opens a dedicated buffer showing:

- File names and paths
- File sizes
- File content preview

<<Detaching-Files>>
**** 9.4.2 Detaching Files [[#Detaching-Files][¶]]
:PROPERTIES:
:CUSTOM_ID: detaching-files
:CLASS: subsection
:END:
To remove a specific file:

#+begin_src example-preformatted
M-x ollama-buddy-detach-file
#+end_src

or press =C-c C-d=.

You'll be prompted to select which file to detach from the list of
currently attached files.

<<Clearing-All-Attachments>>
**** 9.4.3 Clearing All Attachments [[#Clearing-All-Attachments][¶]]
:PROPERTIES:
:CUSTOM_ID: clearing-all-attachments
:CLASS: subsection
:END:
To remove all attached files at once:

#+begin_src example-preformatted
M-x ollama-buddy-clear-attachments
#+end_src

or press =C-c 0=.

<<File-Size-Limits>>
*** 9.5 File Size Limits [[#File-Size-Limits][¶]]
:PROPERTIES:
:CUSTOM_ID: file-size-limits
:CLASS: section
:END:
Ollama Buddy enforces file size limits to prevent overwhelming the
context window:

- Default maximum file size: 10MB
- Configurable via =ollama-buddy-max-file-size=
- Files exceeding the limit will trigger an error

Example configuration:

#+begin_src example-preformatted
;; Set maximum file size to 5MB
(setq ollama-buddy-max-file-size (* 5 1024 1024))
#+end_src

<<How-Attachments-Work>>
*** 9.6 How Attachments Work [[#How-Attachments-Work][¶]]
:PROPERTIES:
:CUSTOM_ID: how-attachments-work
:CLASS: section
:END:
- [[#Context-Integration][Context Integration]]
- [[#Session-Persistence][Session Persistence]]

<<Context-Integration>>
**** 9.6.1 Context Integration [[#Context-Integration][¶]]
:PROPERTIES:
:CUSTOM_ID: context-integration
:CLASS: subsection
:END:
When files are attached:

1. File contents are read and stored in memory
2. Content is included in the prompt context when sending requests
3. Token counting includes attachment content
4. Files are formatted with clear delimiters showing filename and type

<<Session-Persistence>>
**** 9.6.2 Session Persistence [[#Session-Persistence][¶]]
:PROPERTIES:
:CUSTOM_ID: session-persistence
:CLASS: subsection
:END:
File attachments are preserved across sessions:

- Saving a session (=C-c S=) includes all attached files
- Loading a session (=C-c L=) restores attachments
- Session files store both file paths and content
- Attachment metadata is preserved (size, type, attachment time)

<<File-Attachment-Workflow-Examples>>
*** 9.7 File Attachment Workflow Examples [[#File-Attachment-Workflow-Examples][¶]]
:PROPERTIES:
:CUSTOM_ID: file-attachment-workflow-examples
:CLASS: section
:END:
- [[#Code-Review-Workflow][Code Review Workflow]]
- [[#Multi_002dFile-Analysis][Multi-File Analysis]]
- [[#Configuration-Troubleshooting][Configuration Troubleshooting]]

<<Code-Review-Workflow>>
**** 9.7.1 Code Review Workflow [[#Code-Review-Workflow][¶]]
:PROPERTIES:
:CUSTOM_ID: code-review-workflow
:CLASS: subsection
:END:
1. Attach source files using =C-c C-a=
2. Set a system prompt for code review: "You are an expert code
   reviewer"
3. Ask questions about the code: "What potential issues do you see in
   this code?"
4. The AI can reference all attached files in its analysis

<<Multi_002dFile-Analysis>>
**** 9.7.2 Multi-File Analysis [[#Multi_002dFile-Analysis][¶]]
:PROPERTIES:
:CUSTOM_ID: multi-file-analysis
:CLASS: subsection
:END:
1. Use Dired to mark multiple related files
2. Attach them all with =M-x ollama-buddy-dired-attach-marked-files=
3. Ask for analysis: "Compare the approaches used in these files"
4. The AI can cross-reference content between files

<<Configuration-Troubleshooting>>
**** 9.7.3 Configuration Troubleshooting [[#Configuration-Troubleshooting][¶]]
:PROPERTIES:
:CUSTOM_ID: configuration-troubleshooting
:CLASS: subsection
:END:
1. Attach configuration files (.yaml, .json, .ini)
2. Describe the issue: "This configuration isn't working as expected"
3. The AI can analyze the config and suggest fixes

<<Context-Considerations>>
*** 9.8 Context Considerations [[#Context-Considerations][¶]]
:PROPERTIES:
:CUSTOM_ID: context-considerations
:CLASS: section
:END:
File attachments impact context usage:

- Each attached file counts toward the total token limit
- Large files can quickly fill available context
- Monitor context usage with =C-c C= when using attachments
- Consider detaching unnecessary files to free up context

<<Best-Practices>>
*** 9.9 Best Practices [[#Best-Practices][¶]]
:PROPERTIES:
:CUSTOM_ID: best-practices
:CLASS: section
:END:
1. Start with smaller files to avoid context issues
2. Use descriptive filenames for clarity
3. Remove attachments when no longer needed
4. Monitor context usage with large files
5. Use attachment history to avoid re-attaching the same files

<<Troubleshooting-Attachments>>
*** 9.10 Troubleshooting Attachments [[#Troubleshooting-Attachments][¶]]
:PROPERTIES:
:CUSTOM_ID: troubleshooting-attachments
:CLASS: section
:END:
- File won't attach :: - Check if file type is supported (or override
    with "y")
  - Verify file size is under the limit
  - Ensure file exists and is readable
- Context errors with attachments :: - Remove some attachments with
    =C-c C-d=
  - Switch to a model with larger context
  - Reduce conversation history length
- Attachments not showing in session :: - Ensure you saved the session
    after attaching files
  - Check that the session file includes attachment data
  - Verify file paths are still valid when loading

--------------

<<Web-Search>>

Next: [[#Parameter-Control][Parameter Control]], Previous:
[[#File-Attachments][File Attachments]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 10 Web Search [[#Web-Search-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Web-Search-1
:CLASS: chapter
:END:
- [[#Overview-1][Overview]]
- [[#How-It-Works][How It Works]]
- [[#Configuration-2][Configuration]]
- [[#Usage][Usage]]
- [[#Viewing-Web-Search-Results][Viewing Web Search Results]]
- [[#Managing-Web-Searches][Managing Web Searches]]
- [[#Provider-Support][Provider Support]]
- [[#Troubleshooting-1][Troubleshooting]]

<<Overview-1>>
*** 10.1 Overview [[#Overview-1][¶]]
:PROPERTIES:
:CUSTOM_ID: overview-1
:CLASS: section
:END:
Web search integration allows Ollama Buddy to fetch real-time
information from the web and include it as context for your LLM
conversations. This is particularly useful for:

- Getting current information beyond the model's training cutoff
- Researching topics and summarizing findings
- Fact-checking with up-to-date sources
- Comparing information from multiple web sources

<<How-It-Works>>
*** 10.2 How It Works [[#How-It-Works][¶]]
:PROPERTIES:
:CUSTOM_ID: how-it-works
:CLASS: section
:END:
The web search feature implements a multi-stage pipeline. Content can be
retrieved using two methods, controlled by
=ollama-buddy-web-search-content-source=:

- [[#eww-Mode-_0028Default_002c-Recommended_0029][eww Mode (Default,
  Recommended)]]
- [[#API-Mode-_0028Experimental_0029][API Mode (Experimental)]]

<<eww-Mode-_0028Default_002c-Recommended_0029>>
**** 10.2.1 eww Mode (Default, Recommended) [[#eww-Mode-_0028Default_002c-Recommended_0029][¶]]
:PROPERTIES:
:CUSTOM_ID: eww-mode-default-recommended
:CLASS: subsection
:END:
When =ollama-buddy-web-search-content-source= is set to ='eww= (the
default):

1. *Query to Ollama API*: Search queries are sent to Ollama's Web Search
   API (=https://ollama.com/api/web_search=) via REST with Bearer token
   authentication.
2. *URL Extraction*: The API returns search results containing URLs.
3. *eww/shr Processing*: Each URL is fetched and processed through
   Emacs' built-in eww/shr HTML renderer:
   - HTML is parsed using =libxml-parse-html-region=
   - Main content is extracted (looking for =<article>=, =<main>=, or
     content divs)
   - Content is rendered to clean plain text using =shr-insert-document=
   - Org-mode special characters are escaped (=*= and =#= at line starts
     become =,*= and =,#=)
4. *Context Attachment*: The cleaned text is formatted with org headings
   and attached to the conversation context.
5. *LLM Submission*: The search results are included when sending
   prompts to any configured LLM provider.

This mode produces cleaner, more complete content but requires
additional HTTP requests to fetch each URL.

<<API-Mode-_0028Experimental_0029>>
**** 10.2.2 API Mode (Experimental) [[#API-Mode-_0028Experimental_0029][¶]]
:PROPERTIES:
:CUSTOM_ID: api-mode-experimental
:CLASS: subsection
:END:
When =ollama-buddy-web-search-content-source= is set to ='api=:

1. *Query to Ollama API*: Search queries are sent to the Web Search API.
2. *Direct Content*: Content snippets returned directly by the Ollama
   API are used without fetching individual URLs.
3. *Context Attachment*: The API-provided content is formatted and
   attached to the conversation context.

This mode is faster (no additional HTTP requests) but the content
quality depends on what the Ollama API returns, which may be less
refined than eww-rendered pages. Use this mode when speed is more
important than content completeness.

<<Configuration-2>>
*** 10.3 Configuration [[#Configuration-2][¶]]
:PROPERTIES:
:CUSTOM_ID: configuration
:CLASS: section
:END:
To use web search, you need an API key from Ollama:

1. Visit [[https://ollama.com/settings/keys]]
2. Create a new API key
3. Configure in Emacs:

#+begin_src lisp-preformatted
;; Required: Set your API key
(setq ollama-buddy-web-search-api-key "your-api-key-here")

;; Optional customizations
(setq ollama-buddy-web-search-max-results 5)        ; Number of URLs to fetch (default: 5)
(setq ollama-buddy-web-search-snippet-length 2000)  ; Max chars per result (default: 500)
(setq ollama-buddy-web-search-include-urls nil)     ; Include URLs in context (default: nil)
(setq ollama-buddy-web-search-content-source 'eww)  ; Content source: 'eww (default) or 'api

;; Load the module
(require 'ollama-buddy-web-search nil t)
#+end_src

<<Usage>>
*** 10.4 Usage [[#Usage][¶]]
:PROPERTIES:
:CUSTOM_ID: usage
:CLASS: section
:END:
- [[#Inline-Search-Syntax][Inline Search Syntax]]
- [[#Manual-Search-Commands][Manual Search Commands]]
- [[#Transient-Menu][Transient Menu]]

<<Inline-Search-Syntax>>
**** 10.4.1 Inline Search Syntax [[#Inline-Search-Syntax][¶]]
:PROPERTIES:
:CUSTOM_ID: inline-search-syntax
:CLASS: subsection
:END:
The most convenient way to use web search is with inline syntax directly
in your prompts:

#+begin_src example-preformatted
Tell me about @search(latest emacs 30 features) and summarize the key points.
#+end_src

Multiple searches can be combined:

#+begin_src example-preformatted
Compare @search(rust async programming) with @search(go concurrency model)
#+end_src

When you send the prompt, Ollama Buddy will:

1. Extract all =@search(query)= patterns
2. Perform each search and fetch URL contents
3. Attach results to the conversation context
4. Replace the search syntax with just the query text
5. Send the prompt with the attached web context

<<Manual-Search-Commands>>
**** 10.4.2 Manual Search Commands [[#Manual-Search-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: manual-search-commands
:CLASS: subsection
:END:
- C-c / s :: Search and display results in a buffer
  (=ollama-buddy-web-search=)

- C-c / a :: Search and attach results to conversation context
  (=ollama-buddy-web-search-attach=)

<<Transient-Menu>>
**** 10.4.3 Transient Menu [[#Transient-Menu][¶]]
:PROPERTIES:
:CUSTOM_ID: transient-menu
:CLASS: subsection
:END:
Access the web search menu via =C-c O /= which provides:

- Search & Display - perform search and show results
- Search & Attach - perform search and attach to context
- Show Attachments - view all attachments including web searches
- Clear All - remove all attachments and web searches

<<Viewing-Web-Search-Results>>
*** 10.5 Viewing Web Search Results [[#Viewing-Web-Search-Results][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-web-search-results
:CLASS: section
:END:
Web search results are integrated into the attachment system. Use
=C-c C-w= to view all attachments, which displays both file attachments
and web searches in an org-mode buffer:

#+begin_src example-preformatted
,* Web Searches (1)

,** what is the latest manchester united result
:PROPERTIES:
:RESULTS: 5
:TOKENS: ~1234
:SIZE: 5678 bytes
:TIME: 2024-01-15 10:30:00
:END:

,*** 1. First Result Title
:PROPERTIES:
:URL: https://example.com/page1
:END:
,#+begin_example
Full content from this URL...
,#+end_example

,*** 2. Second Result Title
...
#+end_src

The org-mode structure allows you to collapse/expand individual results.

<<Managing-Web-Searches>>
*** 10.6 Managing Web Searches [[#Managing-Web-Searches][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-web-searches
:CLASS: section
:END:
Web searches are treated as attachments:

- Status line shows =🔍N= when N web searches are attached
- =C-c 0= clears all attachments including web searches
- Web search context is automatically included in prompts to all LLM
  providers

<<Provider-Support>>
*** 10.7 Provider Support [[#Provider-Support][¶]]
:PROPERTIES:
:CUSTOM_ID: provider-support
:CLASS: section
:END:
Web search works with all configured LLM providers:

- Local Ollama :: Models with =o:= prefix or no prefix

- OpenAI :: Models with =a:= prefix

- Anthropic Claude :: Models with =c:= prefix

- Google Gemini :: Models with =g:= prefix

- X Grok :: Models with =k:= prefix

- GitHub Copilot :: Models with =p:= prefix

- Mistral Codestral :: Models with =s:= prefix

<<Troubleshooting-1>>
*** 10.8 Troubleshooting [[#Troubleshooting-1][¶]]
:PROPERTIES:
:CUSTOM_ID: troubleshooting
:CLASS: section
:END:
- 401 Unauthorized error :: Ensure you're using an API key from
  [[https://ollama.com/settings/keys]], not the token from
  =ollama signin= (cloud authentication).

- Empty search results :: Some websites may block automated requests or
  have content that doesn't extract well. Try different search queries.

- Slow searches :: URL fetching is asynchronous for display and attach
  commands. Inline searches in prompts use synchronous fetching to
  ensure results are ready before sending.

--------------

<<Parameter-Control>>

Next: [[#Session-Management][Session Management]], Previous:
[[#Web-Search][Web Search]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 11 Parameter Control [[#Parameter-Control-2][¶]]
:PROPERTIES:
:CUSTOM_ID: Parameter-Control-2
:CLASS: chapter
:END:
- [[#Understanding-Parameters][Understanding Parameters]]
- [[#Viewing-Current-Parameters][Viewing Current Parameters]]
- [[#Editing-Parameters][Editing Parameters]]
- [[#Parameter-Profiles][Parameter Profiles]]
- [[#Command_002dSpecific-Parameters][Command-Specific Parameters]]
- [[#Reset-Parameters][Reset Parameters]]
- [[#Displaying-Parameters-in-Header][Displaying Parameters in Header]]

<<Understanding-Parameters>>
*** 11.1 Understanding Parameters [[#Understanding-Parameters][¶]]
:PROPERTIES:
:CUSTOM_ID: understanding-parameters
:CLASS: section
:END:
Ollama's models support various parameters that control their behavior:

- temperature :: Controls randomness (0.0-1.0+), higher values produce
  more creative outputs

- top_k :: Limits token selection to top K most probable tokens

- top_p :: Nucleus sampling threshold (0.0-1.0)

- repeat_penalty :: Penalty for repeating tokens (higher values reduce
  repetition)

<<Viewing-Current-Parameters>>
*** 11.2 Viewing Current Parameters [[#Viewing-Current-Parameters][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-current-parameters
:CLASS: section
:END:
To view all current parameters:

#+begin_src example-preformatted
M-x ollama-buddy-params-display
#+end_src

or press =C-c G= in the chat buffer.

Parameters that have been modified from default values are marked with
an asterisk (*).

<<Editing-Parameters>>
*** 11.3 Editing Parameters [[#Editing-Parameters][¶]]
:PROPERTIES:
:CUSTOM_ID: editing-parameters
:CLASS: section
:END:
To edit parameters:

1. Press =C-c P= to open the parameter menu
2. Select the parameter you want to modify
3. Enter the new value

You can also use =M-x ollama-buddy-params-edit= and select from a
completion list.

<<Parameter-Profiles>>
*** 11.4 Parameter Profiles [[#Parameter-Profiles][¶]]
:PROPERTIES:
:CUSTOM_ID: parameter-profiles
:CLASS: section
:END:
Ollama Buddy comes with predefined parameter profiles for different use
cases:

- Default :: Standard balanced settings

- Creative :: Higher temperature, lower penalties for more creative
  responses

- Precise :: Lower temperature, higher penalties for more deterministic
  responses

To apply a profile:

#+begin_src example-preformatted
M-x ollama-buddy-transient-profile-menu
#+end_src

or press =C-c p= and select a profile.

<<Command_002dSpecific-Parameters>>
*** 11.5 Command-Specific Parameters [[#Command_002dSpecific-Parameters][¶]]
:PROPERTIES:
:CUSTOM_ID: command-specific-parameters
:CLASS: section
:END:
Some commands have pre-configured parameters. For example:

- The "Refactor Code" command uses lower temperature for more
  deterministic results
- The "Creative Writing" command uses higher temperature for more varied
  outputs

These parameters are automatically applied when you use these commands
and restored afterward.

<<Reset-Parameters>>
*** 11.6 Reset Parameters [[#Reset-Parameters][¶]]
:PROPERTIES:
:CUSTOM_ID: reset-parameters
:CLASS: section
:END:
To reset all parameters to default values:

#+begin_src example-preformatted
M-x ollama-buddy-params-reset
#+end_src

or press =C-c K= in the chat buffer.

<<Displaying-Parameters-in-Header>>
*** 11.7 Displaying Parameters in Header [[#Displaying-Parameters-in-Header][¶]]
:PROPERTIES:
:CUSTOM_ID: displaying-parameters-in-header
:CLASS: section
:END:
To toggle whether modified parameters are shown in the header:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-params-in-header
#+end_src

or press =C-c F= in the chat buffer.

--------------

<<Session-Management>>

Next: [[#User-System-Prompts][User System Prompts]], Previous:
[[#Parameter-Control][Parameter Control]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 12 Session Management [[#Session-Management-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Session-Management-1
:CLASS: chapter
:END:
- [[#Understanding-Sessions][Understanding Sessions]]
- [[#Creating-a-New-Session][Creating a New Session]]
- [[#Saving-a-Session][Saving a Session]]
- [[#Loading-a-Session][Loading a Session]]
- [[#Managing-Sessions][Managing Sessions]]
- [[#Conversation-History][Conversation History]]

<<Understanding-Sessions>>
*** 12.1 Understanding Sessions [[#Understanding-Sessions][¶]]
:PROPERTIES:
:CUSTOM_ID: understanding-sessions
:CLASS: section
:END:
Sessions in Ollama Buddy allow you to:

- Save the entire conversation history
- Save the current model selection
- Restore previous conversations later
- Switch between different conversation contexts

<<Creating-a-New-Session>>
*** 12.2 Creating a New Session [[#Creating-a-New-Session][¶]]
:PROPERTIES:
:CUSTOM_ID: creating-a-new-session
:CLASS: section
:END:
To start a fresh session:

#+begin_src example-preformatted
M-x ollama-buddy-sessions-new
#+end_src

or press =C-c N= in the chat buffer.

This will clear the current conversation history and let you start
fresh.

<<Saving-a-Session>>
*** 12.3 Saving a Session [[#Saving-a-Session][¶]]
:PROPERTIES:
:CUSTOM_ID: saving-a-session
:CLASS: section
:END:
To save the current session:

#+begin_src example-preformatted
M-x ollama-buddy-sessions-save
#+end_src

or press =C-c S= in the chat buffer.

You'll be prompted to enter a name for the session.

<<Loading-a-Session>>
*** 12.4 Loading a Session [[#Loading-a-Session][¶]]
:PROPERTIES:
:CUSTOM_ID: loading-a-session
:CLASS: section
:END:
To load a previously saved session:

#+begin_src example-preformatted
M-x ollama-buddy-sessions-load
#+end_src

or press =C-c L= in the chat buffer.

You'll be presented with a list of saved sessions to choose from.

<<Managing-Sessions>>
*** 12.5 Managing Sessions [[#Managing-Sessions][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-sessions
:CLASS: section
:END:
To see a list of all saved sessions:

#+begin_src example-preformatted
M-x ollama-buddy-sessions-list
#+end_src

or press =C-c Q= in the chat buffer.

From this view, you can see:

- Session names
- Last modified times
- Which models are used in each session

To delete a session:

#+begin_src example-preformatted
M-x ollama-buddy-sessions-delete
#+end_src

or press =C-c Z= in the chat buffer.

<<Conversation-History>>
*** 12.6 Conversation History [[#Conversation-History][¶]]
:PROPERTIES:
:CUSTOM_ID: conversation-history
:CLASS: section
:END:
Sessions save the conversation history for each model separately.

To view the current conversation history:

#+begin_src example-preformatted
M-x ollama-buddy-history-edit
#+end_src

or press =C-c J= in the chat buffer.

To clear the history:

#+begin_src example-preformatted
M-x ollama-buddy-clear-history
#+end_src

or press =C-c X= in the chat buffer.

To toggle whether history is used in requests:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-history
#+end_src

or press =C-c H= in the chat buffer.

--------------

<<User-System-Prompts>>

Next: [[#Roles-and-Commands][Roles and Commands]], Previous:
[[#Session-Management][Session Management]], Up: [[#Top][Ollama Buddy]]
  [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 13 User System Prompts [[#User-System-Prompts-1][¶]]
:PROPERTIES:
:CUSTOM_ID: User-System-Prompts-1
:CLASS: chapter
:END:
- [[#Overview-2][Overview]]
- [[#Accessing-the-System-Prompts-Menu][Accessing the System Prompts
  Menu]]
- [[#Saving-System-Prompts][Saving System Prompts]]
- [[#Loading-Saved-Prompts][Loading Saved Prompts]]
- [[#Managing-Your-Prompt-Library][Managing Your Prompt Library]]
- [[#Categories-and-Organization][Categories and Organization]]
- [[#Prompt-Storage-Format][Prompt Storage Format]]
- [[#Best-Practices-for-System-Prompts][Best Practices for System
  Prompts]]
- [[#Example-System-Prompts][Example System Prompts]]
- [[#Workflow-Examples][Workflow Examples]]
- [[#Integration-with-Roles][Integration with Roles]]

<<Overview-2>>
*** 13.1 Overview [[#Overview-2][¶]]
:PROPERTIES:
:CUSTOM_ID: overview-2
:CLASS: section
:END:
The User System Prompts feature allows you to save, organize, and reuse
effective system prompts for your conversations with AI models. This
feature is particularly valuable for:

- Building a personal library of effective prompts
- Maintaining context continuity across sessions
- Sharing prompt templates with teammates
- Refining your prompts over time
- Categorizing prompts by domain or purpose

System prompts play a crucial role in guiding AI behavior and response
quality. A well-crafted system prompt can dramatically improve the
relevance, accuracy, and style of AI responses.

<<Accessing-the-System-Prompts-Menu>>
*** 13.2 Accessing the System Prompts Menu [[#Accessing-the-System-Prompts-Menu][¶]]
:PROPERTIES:
:CUSTOM_ID: accessing-the-system-prompts-menu
:CLASS: section
:END:
To access the system prompts menu:

#+begin_src example-preformatted
M-x ollama-buddy-transient-user-prompts-menu
#+end_src

or press =C-c s= in the chat buffer.

This opens a transient menu with the following options:

- Save current (S) :: Save your active system prompt for future reuse

- Load prompt (L) :: Select a previously saved prompt to apply

- Create new (N) :: Start fresh with a new prompt

- List all Prompts (l) :: View your entire prompt collection

- Edit prompt (e) :: Modify an existing prompt

- Set with current prompt (s) :: Set the current text as a system prompt

- Delete prompt (d) :: Remove prompts you no longer need

- Reset prompt (r) :: Clear the system prompt setting

<<Saving-System-Prompts>>
*** 13.3 Saving System Prompts [[#Saving-System-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: saving-system-prompts
:CLASS: section
:END:
To save a system prompt:

1. Set a system prompt by typing it and pressing =C-c s s=
2. Open the system prompts menu with =C-c s=
3. Press =S= to save the current system prompt
4. Enter a category (from predefined options or create your own)
5. Enter a descriptive title for your prompt
6. The prompt will be saved to your prompts directory

<<Loading-Saved-Prompts>>
*** 13.4 Loading Saved Prompts [[#Loading-Saved-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: loading-saved-prompts
:CLASS: section
:END:
To load a previously saved prompt:

1. Press =C-c s= to open the system prompts menu
2. Press =L= to list available prompts
3. Select a prompt from the completion interface
4. The prompt will be loaded and set as your current system prompt

Prompts are displayed in the format "=category: title=" for easy
selection.

<<Managing-Your-Prompt-Library>>
*** 13.5 Managing Your Prompt Library [[#Managing-Your-Prompt-Library][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-your-prompt-library
:CLASS: section
:END:
- [[#Viewing-All-Prompts][Viewing All Prompts]]
- [[#Editing-Prompts][Editing Prompts]]
- [[#Creating-New-Prompts][Creating New Prompts]]
- [[#Deleting-Prompts][Deleting Prompts]]

<<Viewing-All-Prompts>>
**** 13.5.1 Viewing All Prompts [[#Viewing-All-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-all-prompts
:CLASS: subsection
:END:
To view your entire prompt collection:

#+begin_src example-preformatted
M-x ollama-buddy-user-prompts-list
#+end_src

or press =C-c s l=.

This displays a buffer showing:

- Prompts organized by category
- Prompt titles
- Preview of prompt content

<<Editing-Prompts>>
**** 13.5.2 Editing Prompts [[#Editing-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: editing-prompts
:CLASS: subsection
:END:
To edit an existing prompt:

#+begin_src example-preformatted
M-x ollama-buddy-user-prompts-edit
#+end_src

or press =C-c s e=.

This opens the prompt file in an Org mode buffer where you can make
changes and save.

<<Creating-New-Prompts>>
**** 13.5.3 Creating New Prompts [[#Creating-New-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: creating-new-prompts
:CLASS: subsection
:END:
To create a new prompt from scratch:

#+begin_src example-preformatted
M-x ollama-buddy-user-prompts-create-new
#+end_src

or press =C-c s N=.

This opens a template with Org headers where you can enter your prompt
content.

<<Deleting-Prompts>>
**** 13.5.4 Deleting Prompts [[#Deleting-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: deleting-prompts
:CLASS: subsection
:END:
To delete a prompt:

#+begin_src example-preformatted
M-x ollama-buddy-user-prompts-delete
#+end_src

or press =C-c s d=.

You'll be asked to confirm before the prompt is deleted.

<<Categories-and-Organization>>
*** 13.6 Categories and Organization [[#Categories-and-Organization][¶]]
:PROPERTIES:
:CUSTOM_ID: categories-and-organization
:CLASS: section
:END:
Prompts are organized into categories for easier management. Default
categories include:

- general - General-purpose system prompts
- coding - Programming-specific prompts
- writing - Content creation and editing prompts
- analysis - Data and research analysis prompts
- creative - Prompts for creative tasks
- technical - Technical documentation and explanation prompts
- documentation - Documentation-focused prompts

You can customize the default categories:

#+begin_src example-preformatted
(setq ollama-buddy-user-prompts-default-categories
      '("general" "coding" "writing" "analysis" "creative" "custom"))
#+end_src

<<Prompt-Storage-Format>>
*** 13.7 Prompt Storage Format [[#Prompt-Storage-Format][¶]]
:PROPERTIES:
:CUSTOM_ID: prompt-storage-format
:CLASS: section
:END:
System prompts are stored as Org mode files with a specific naming
convention:

#+begin_src example-preformatted
category__title__system.org
#+end_src

Each file contains:

- Org properties with metadata (title, category, date)
- The full prompt content

Example prompt file content:

#+begin_src example-preformatted
,#+TITLE: Python Expert
,#+CATEGORY: coding
,#+DATE: 2025-05-19 14:32:45

You are a Python programming expert with deep knowledge of both modern and 
legacy Python code. When analyzing or writing code:

1. Prioritize readability and maintainability over clever tricks
2. Follow PEP 8 conventions
3. Include docstrings and comments for non-obvious operations
4. Explain your thinking step-by-step
5. Provide examples when helpful

When asked to debug, first identify the likely cause before suggesting fixes.
#+end_src

<<Best-Practices-for-System-Prompts>>
*** 13.8 Best Practices for System Prompts [[#Best-Practices-for-System-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: best-practices-for-system-prompts
:CLASS: section
:END:
- [[#Components-of-Effective-Prompts][Components of Effective Prompts]]
- [[#Example-Patterns][Example Patterns]]

<<Components-of-Effective-Prompts>>
**** 13.8.1 Components of Effective Prompts [[#Components-of-Effective-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: components-of-effective-prompts
:CLASS: subsection
:END:
Well-designed system prompts typically include:

- Clear role definition (who/what the AI is supposed to be)
- Guidelines for response style and format
- Constraints or limitations to observe
- Specific instructions for handling certain types of queries
- Examples of desired responses (optional)

<<Example-Patterns>>
**** 13.8.2 Example Patterns [[#Example-Patterns][¶]]
:PROPERTIES:
:CUSTOM_ID: example-patterns
:CLASS: subsection
:END:
- Expert Role :: "You are a [domain] expert with [X years] of experience
  in [specific areas]..."

- Response Format :: "Format your responses with a brief summary first,
  followed by detailed analysis..."

- Specific Guidelines :: "When responding to code queries, always
  include sample code and explain line-by-line..."

- Thinking Process :: "Think step-by-step, breaking down complex
  problems into smaller components..."

<<Example-System-Prompts>>
*** 13.9 Example System Prompts [[#Example-System-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: example-system-prompts
:CLASS: section
:END:
- [[#Technical-Writing-Assistant][Technical Writing Assistant]]
- [[#Code-Reviewer][Code Reviewer]]

<<Technical-Writing-Assistant>>
**** 13.9.1 Technical Writing Assistant [[#Technical-Writing-Assistant][¶]]
:PROPERTIES:
:CUSTOM_ID: technical-writing-assistant
:CLASS: subsection
:END:

#+begin_src example-preformatted
You are a technical writing expert who specializes in creating clear, concise, 
and accessible documentation. Your writing should:

1. Use plain language and avoid jargon where possible
2. Include appropriate headings and structural elements
3. Provide concrete examples that illustrate complex concepts
4. Use active voice and direct instructions for procedures
5. Anticipate common user questions and address them proactively

When presented with technical content, focus on making it understandable to 
the target audience while preserving technical accuracy.
#+end_src

<<Code-Reviewer>>
**** 13.9.2 Code Reviewer [[#Code-Reviewer][¶]]
:PROPERTIES:
:CUSTOM_ID: code-reviewer
:CLASS: subsection
:END:

#+begin_src example-preformatted
You are an experienced code reviewer with expertise in software engineering 
best practices. When reviewing code:

1. Identify potential bugs, edge cases, and performance issues
2. Suggest improvements to readability and maintainability
3. Highlight security vulnerabilities or potential risks
4. Reference design patterns or library functions that could improve the implementation
5. Provide specific, actionable feedback with examples

Balance constructive criticism with acknowledgment of well-written code.
#+end_src

<<Workflow-Examples>>
*** 13.10 Workflow Examples [[#Workflow-Examples][¶]]
:PROPERTIES:
:CUSTOM_ID: workflow-examples
:CLASS: section
:END:
- [[#Python-Code-Assistance][Python Code Assistance]]
- [[#Technical-Writing-Help][Technical Writing Help]]

<<Python-Code-Assistance>>
**** 13.10.1 Python Code Assistance [[#Python-Code-Assistance][¶]]
:PROPERTIES:
:CUSTOM_ID: python-code-assistance
:CLASS: subsection
:END:
1. Load your "Python Expert" system prompt with =C-c s L=
2. Ask coding questions or paste code for analysis
3. The AI responds with Python-specific expertise
4. Save the conversation as a session for future reference

<<Technical-Writing-Help>>
**** 13.10.2 Technical Writing Help [[#Technical-Writing-Help][¶]]
:PROPERTIES:
:CUSTOM_ID: technical-writing-help
:CLASS: subsection
:END:
1. Create a new system prompt for technical writing (=C-c s N=)
2. Define the AI's role as a technical writing assistant
3. Save the prompt in the "writing" category
4. Load this prompt whenever you need help with documentation
5. The AI consistently provides responses optimized for technical
   writing

<<Integration-with-Roles>>
*** 13.11 Integration with Roles [[#Integration-with-Roles][¶]]
:PROPERTIES:
:CUSTOM_ID: integration-with-roles
:CLASS: section
:END:
System prompts can be integrated with Ollama Buddy roles for more
specialized workflows:

1. Create a system prompt for a specific purpose
2. Test and refine it through direct interaction
3. Once effective, save it to your prompt library
4. Reference this prompt in a custom role definition

This creates a reusable AI assistant configuration that can be shared
and improved over time.

--------------

<<Roles-and-Commands>>

Next: [[#Fabric-Pattern-Integration][Fabric Pattern Integration]],
Previous: [[#User-System-Prompts][User System Prompts]], Up:
[[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 14 Roles and Commands [[#Roles-and-Commands-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Roles-and-Commands-1
:CLASS: chapter
:END:
- [[#Understanding-Roles][Understanding Roles]]
- [[#Role-File-Naming-Convention][Role File Naming Convention]]
- [[#Built_002din-Commands][Built-in Commands]]
- [[#Creating-Custom-Roles][Creating Custom Roles]]
- [[#Switching-Roles][Switching Roles]]
- [[#Managing-Role-Files][Managing Role Files]]
- [[#Advanced-Role-Customization][Advanced Role Customization]]
- [[#Role-Examples][Role Examples]]
- [[#Tips-for-Effective-Role-Usage][Tips for Effective Role Usage]]

<<Understanding-Roles>>
*** 14.1 Understanding Roles [[#Understanding-Roles][¶]]
:PROPERTIES:
:CUSTOM_ID: understanding-roles
:CLASS: section
:END:
Roles in Ollama Buddy are collections of commands with specific
configurations:

- Each role has its own set of commands
- Commands can use specific models
- Commands can have specialized system prompts
- Commands can have specialized parameters

This allows you to create specialized assistants for different
workflows.

<<Role-File-Naming-Convention>>
*** 14.2 Role File Naming Convention [[#Role-File-Naming-Convention][¶]]
:PROPERTIES:
:CUSTOM_ID: role-file-naming-convention
:CLASS: section
:END:
The file naming convention is critical to understand how roles, preset
files, and menu configurations work together:

- Required filename format :: =ollama-buddy--preset__ROLE-NAME.el=

  - The double underscore =__= separates the prefix from your role name
  - The role name portion becomes the identifier shown when switching
    roles
  - Example: =ollama-buddy--preset__programmer.el= creates a role named
    "programmer"

This naming convention is how Ollama Buddy discovers and identifies role
files. When you run =ollama-buddy-roles-switch-role=, the system:

1. Scans the =ollama-buddy-roles-directory= for files matching the
   pattern
2. Extracts the role name from each filename (the part after =__=)
3. Presents these names in the role selection interface
4. When selected, loads the corresponding file which redefines
   =ollama-buddy-command-definitions=
5. This redefinition immediately changes the available commands in your
   Ollama Buddy menu

The relationship chain works like this:

#+begin_src example-preformatted
ollama-buddy--preset__ROLE-NAME.el → Defines ollama-buddy-command-definitions → Controls menu content
#+end_src

When creating roles using the interactive role creator (=C-c E=), this
naming convention is automatically handled for you. When creating roles
manually, you must follow this pattern for Ollama Buddy to recognize
your role files correctly.

<<Built_002din-Commands>>
*** 14.3 Built-in Commands [[#Built_002din-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: built-in-commands
:CLASS: section
:END:
Ollama Buddy comes with several built-in commands:

- refactor-code :: Improves code while maintaining functionality

- describe-code :: Explains what code does and how it works

- git-commit :: Generates meaningful commit messages

- dictionary-lookup :: Provides comprehensive word definitions

- synonym :: Suggests alternative words with context

- proofread :: Corrects grammar, style, and spelling

<<Creating-Custom-Roles>>
*** 14.4 Creating Custom Roles [[#Creating-Custom-Roles][¶]]
:PROPERTIES:
:CUSTOM_ID: creating-custom-roles
:CLASS: section
:END:
There are two ways to create custom roles:

- [[#Interactive-Role-Creator][Interactive Role Creator]]
- [[#Manual-Role-Creation][Manual Role Creation]]

<<Interactive-Role-Creator>>
**** 14.4.1 Interactive Role Creator [[#Interactive-Role-Creator][¶]]
:PROPERTIES:
:CUSTOM_ID: interactive-role-creator
:CLASS: subsection
:END:
The most user-friendly approach:

1. Press =C-c E= or run =M-x ollama-buddy-role-creator-create-new-role=
2. Enter a name for your role (e.g., "programmer")
3. Enter the number of menu columns (default: 2)
4. For each command you want to add:
   - Specify a command name (e.g., "refactor-code")
   - Choose a key shortcut for the menu
   - Add a description
   - Optionally specify a model
   - Optionally add prompt prefixes and system messages

The interactive creator automatically handles file naming and placement.

<<Manual-Role-Creation>>
**** 14.4.2 Manual Role Creation [[#Manual-Role-Creation][¶]]
:PROPERTIES:
:CUSTOM_ID: manual-role-creation
:CLASS: subsection
:END:
For more advanced customization, create role files manually:

1. Create a file named =ollama-buddy--preset__your-role-name.el= in your
   =ollama-buddy-roles-directory=
2. Structure your file like this:

#+begin_src example-preformatted
;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

;; Menu display columns for this role (optional, default is 2)
(setq ollama-buddy-menu-columns 3)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands - always include these
    (open-chat
     :key ?o
     :description "Open chat buffer"
     :action ollama-buddy--open-chat)

    (show-models
     :key ?v
     :description "View model status"
     :action ollama-buddy-show-model-status)

    (switch-role
     :key ?R
     :description "Switch roles"
     :action ollama-buddy-roles-switch-role)
    
    (open-roles-directory
     :key ?D
     :description "Open roles directory"
     :action ollama-buddy-roles-open-directory)
    
    ;; Custom commands for this role
    (refactor-code
     :key ?r
     :description "Refactor code"
     :model "codellama:7b"
     :prompt "Refactor this code to improve readability and efficiency:"
     :system "You are an expert software engineer who improves code quality."
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description "Explain code"
     :model "deepseek-r1:7b"
     :prompt "Explain what this code does in detail:"
     :system "You are a programming teacher who explains code clearly."
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (git-commit
     :key ?g
     :description "Git commit message"
     :prompt "Write a concise git commit message for these changes:"
     :system "You are a version control expert who creates clear commit messages."
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
#+end_src

<<Switching-Roles>>
*** 14.5 Switching Roles [[#Switching-Roles][¶]]
:PROPERTIES:
:CUSTOM_ID: switching-roles
:CLASS: section
:END:
To switch between roles:

#+begin_src example-preformatted
M-x ollama-buddy-roles-switch-role
#+end_src

or press =C-c R= in the chat buffer.

You'll be presented with a list of available roles to choose from.

<<Managing-Role-Files>>
*** 14.6 Managing Role Files [[#Managing-Role-Files][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-role-files
:CLASS: section
:END:
Roles are stored as Elisp files in the =ollama-buddy-roles-directory=.

To locate your roles directory:

#+begin_src example-preformatted
;; Check where your roles are stored
(message ollama-buddy-roles-directory)
#+end_src

By default, this is set to =~/.emacs.d/ollama-buddy-presets/=, but you
can customize it:

#+begin_src example-preformatted
(setq ollama-buddy-roles-directory "/your/custom/path/to/presets")
#+end_src

To open this directory:

#+begin_src example-preformatted
M-x ollama-buddy-roles-open-directory
#+end_src

or press =C-c D= in the chat buffer.

<<Advanced-Role-Customization>>
*** 14.7 Advanced Role Customization [[#Advanced-Role-Customization][¶]]
:PROPERTIES:
:CUSTOM_ID: advanced-role-customization
:CLASS: section
:END:
- [[#Per_002dRole-Menu-Columns][Per-Role Menu Columns]]
- [[#Command_002dSpecific-Models][Command-Specific Models]]
- [[#Command_002dSpecific-Parameters-1][Command-Specific Parameters]]
- [[#Creating-New-Commands][Creating New Commands]]

<<Per_002dRole-Menu-Columns>>
**** 14.7.1 Per-Role Menu Columns [[#Per_002dRole-Menu-Columns][¶]]
:PROPERTIES:
:CUSTOM_ID: per-role-menu-columns
:CLASS: subsection
:END:
Each role can define its own menu column layout by setting
=ollama-buddy-menu-columns= in the role file. This allows roles with
many commands to use more columns, while simpler roles can use fewer:

#+begin_src example-preformatted
;; In your role preset file:
(setq ollama-buddy-menu-columns 3)  ; Use 3 columns for this role
#+end_src

When you switch roles, the menu column setting changes accordingly. The
menu display automatically adjusts its height to ensure all items are
visible, regardless of the number of rows.

<<Command_002dSpecific-Models>>
**** 14.7.2 Command-Specific Models [[#Command_002dSpecific-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: command-specific-models
:CLASS: subsection
:END:
Assign specific models to commands for optimal performance:

#+begin_src example-preformatted
(ollama-buddy-add-model-to-menu-entry 'refactor-code "codellama:7b")
#+end_src

<<Command_002dSpecific-Parameters-1>>
**** 14.7.3 Command-Specific Parameters [[#Command_002dSpecific-Parameters-1][¶]]
:PROPERTIES:
:CUSTOM_ID: command-specific-parameters-1
:CLASS: subsection
:END:
Optimize parameters for specific commands:

#+begin_src example-preformatted
(ollama-buddy-add-parameters-to-command 'refactor-code
  'temperature 0.2
  'top_p 0.7
  'repeat_penalty 1.3)
#+end_src

<<Creating-New-Commands>>
**** 14.7.4 Creating New Commands [[#Creating-New-Commands][¶]]
:PROPERTIES:
:CUSTOM_ID: creating-new-commands
:CLASS: subsection
:END:
Add entirely new commands to your menu:

#+begin_src example-preformatted
(ollama-buddy-update-menu-entry 'my-new-command
  :key ?z
  :description "My new awesome command"
  :prompt "Here is what I want you to do:"
  :system "You are an expert system specialized in this task."
  :action (lambda () (ollama-buddy--send-with-command 'my-new-command)))
#+end_src

<<Role-Examples>>
*** 14.8 Role Examples [[#Role-Examples][¶]]
:PROPERTIES:
:CUSTOM_ID: role-examples
:CLASS: section
:END:
- [[#Programming-Role][Programming Role]]
- [[#Writing-Role][Writing Role]]

<<Programming-Role>>
**** 14.8.1 Programming Role [[#Programming-Role][¶]]
:PROPERTIES:
:CUSTOM_ID: programming-role
:CLASS: subsection
:END:
A complete example of a programming-focused role:

#+begin_src example-preformatted
;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
    (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)
    
    ;; Programming-specific commands
    (refactor-code
     :key ?r
     :description "Refactor code"
     :model "codellama:7b"
     :prompt "Refactor this code to improve readability and efficiency:"
     :system "You are an expert software engineer who improves code quality."
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description "Explain code"
     :model "deepseek-r1:7b"
     :prompt "Explain what this code does in detail:"
     :system "You are a programming teacher who explains code clearly."
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (add-tests
     :key ?t
     :description "Generate tests"
     :model "qwen2.5-coder:7b"
     :prompt "Generate unit tests for this code:"
     :system "You are a test automation expert who creates comprehensive test cases."
     :action (lambda () (ollama-buddy--send-with-command 'add-tests)))
    
    (git-commit
     :key ?g
     :description "Git commit message"
     :prompt "Write a concise git commit message for these changes:"
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
#+end_src

<<Writing-Role>>
**** 14.8.2 Writing Role [[#Writing-Role][¶]]
:PROPERTIES:
:CUSTOM_ID: writing-role
:CLASS: subsection
:END:
A complete example of a writing-focused role:

#+begin_src example-preformatted
;; ollama-buddy preset for role: writer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (abbreviated for clarity)
    (open-chat :key ?o :description "Open chat buffer" :action ollama-buddy--open-chat)
    (show-models :key ?v :description "View model status" :action ollama-buddy-show-model-status)
    (switch-role :key ?R :description "Switch roles" :action ollama-buddy-roles-switch-role)
    
    ;; Writing-focused commands
    (summarize
     :key ?s
     :description "Summarize text"
     :prompt "Summarize the following text in a concise manner:"
     :system "You are an expert at extracting the key points from any text."
     :action (lambda () (ollama-buddy--send-with-command 'summarize)))
    
    (proofread
     :key ?p
     :description "Proofread text"
     :model "deepseek-r1:7b"
     :prompt "Proofread the following text and correct any errors:"
     :system "You are a professional editor who identifies and corrects grammar and style errors."
     :action (lambda () (ollama-buddy--send-with-command 'proofread)))
    
    (rewrite
     :key ?r
     :description "Rewrite text"
     :prompt "Rewrite the following text to improve clarity and flow:"
     :system "You are a skilled writer who can improve any text while preserving its meaning."
     :action (lambda () (ollama-buddy--send-with-command 'rewrite)))
    
    (brainstorm
     :key ?b
     :description "Brainstorm ideas"
     :model "llama3.2:3b"
     :prompt "Generate creative ideas related to the following topic:"
     :parameters ((temperature . 1.0) (top_p . 0.95))
     :action (lambda () (ollama-buddy--send-with-command 'brainstorm)))
    ))
#+end_src

<<Tips-for-Effective-Role-Usage>>
*** 14.9 Tips for Effective Role Usage [[#Tips-for-Effective-Role-Usage][¶]]
:PROPERTIES:
:CUSTOM_ID: tips-for-effective-role-usage
:CLASS: section
:END:
1. Group related commands: Create roles around specific workflows or
   tasks
2. Match models to tasks: Use lightweight models for simple tasks and
   more powerful models for complex ones
3. Customize system prompts: Craft specific system prompts to guide the
   model for each command
4. Use the roles directory: Press =C-c D= to quickly access and manage
   your role files
5. Create specialized roles: Consider roles for programming, writing,
   translation, or domain-specific knowledge

--------------

<<Fabric-Pattern-Integration>>

Next: [[#Awesome-ChatGPT-Prompts][Awesome ChatGPT Prompts]], Previous:
[[#Roles-and-Commands][Roles and Commands]], Up: [[#Top][Ollama Buddy]]
  [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 15 Fabric Pattern Integration [[#Fabric-Pattern-Integration-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Fabric-Pattern-Integration-1
:CLASS: chapter
:END:
- [[#What-are-Fabric-Patterns_003f][What are Fabric Patterns?]]
- [[#Setting-Up-Fabric-Integration][Setting Up Fabric Integration]]
- [[#Using-Fabric-Patterns][Using Fabric Patterns]]
- [[#Browsing-Available-Patterns][Browsing Available Patterns]]
- [[#Viewing-Pattern-Details][Viewing Pattern Details]]
- [[#Updating-Patterns][Updating Patterns]]
- [[#Using-Patterns-by-Category][Using Patterns by Category]]

<<What-are-Fabric-Patterns_003f>>
*** 15.1 What are Fabric Patterns? [[#What-are-Fabric-Patterns_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-are-fabric-patterns
:CLASS: section
:END:
Fabric patterns are pre-defined prompt templates from Daniel Miessler's
Fabric project ([[https://github.com/danielmiessler/fabric]]). They
provide optimized prompts for various tasks, categorized as:

- universal - General-purpose patterns
- code - Programming and development
- writing - Content creation and editing
- analysis - Data and concept examination

<<Setting-Up-Fabric-Integration>>
*** 15.2 Setting Up Fabric Integration [[#Setting-Up-Fabric-Integration][¶]]
:PROPERTIES:
:CUSTOM_ID: setting-up-fabric-integration
:CLASS: section
:END:
To set up Fabric integration:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-setup
#+end_src

This will:

1. Clone the Fabric repository (or set up sparse checkout)
2. Populate available patterns
3. Make patterns available for use

<<Using-Fabric-Patterns>>
*** 15.3 Using Fabric Patterns [[#Using-Fabric-Patterns][¶]]
:PROPERTIES:
:CUSTOM_ID: using-fabric-patterns
:CLASS: section
:END:
To use a Fabric pattern:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-send
#+end_src

or press =C-c f= and then =s=.

You'll be prompted to:

1. Select a pattern
2. Enter text to process (or use selected text)

The pattern will be used as a system prompt for your request.

<<Browsing-Available-Patterns>>
*** 15.4 Browsing Available Patterns [[#Browsing-Available-Patterns][¶]]
:PROPERTIES:
:CUSTOM_ID: browsing-available-patterns
:CLASS: section
:END:
To see all available patterns:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-list-patterns
#+end_src

or press =C-c f= and then =l=.

This shows:

- Pattern names
- Categories
- Descriptions

<<Viewing-Pattern-Details>>
*** 15.5 Viewing Pattern Details [[#Viewing-Pattern-Details][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-pattern-details
:CLASS: section
:END:
To see the full content of a specific pattern:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-show-pattern
#+end_src

or press =C-c f= and then =v=.

Select a pattern to see:

- The system prompt content
- Full description

<<Updating-Patterns>>
*** 15.6 Updating Patterns [[#Updating-Patterns][¶]]
:PROPERTIES:
:CUSTOM_ID: updating-patterns
:CLASS: section
:END:
To sync with the latest patterns from GitHub:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-sync-patterns
#+end_src

or press =C-c f= and then =S=.

<<Using-Patterns-by-Category>>
*** 15.7 Using Patterns by Category [[#Using-Patterns-by-Category][¶]]
:PROPERTIES:
:CUSTOM_ID: using-patterns-by-category
:CLASS: section
:END:
You can quickly access patterns by category:

- =C-c f u= - Universal patterns
- =C-c f c= - Code patterns
- =C-c f w= - Writing patterns
- =C-c f a= - Analysis patterns

--------------

<<Awesome-ChatGPT-Prompts>>

Next: [[#Remote-Providers][Remote Providers]], Previous:
[[#Fabric-Pattern-Integration][Fabric Pattern Integration]], Up:
[[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 16 Awesome ChatGPT Prompts [[#Awesome-ChatGPT-Prompts-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Awesome-ChatGPT-Prompts-1
:CLASS: chapter
:END:
- [[#What-is-Awesome-ChatGPT-Prompts_003f][What is Awesome ChatGPT
  Prompts?]]
- [[#Setting-Up-Awesome-ChatGPT-Prompts][Setting Up Awesome ChatGPT
  Prompts]]
- [[#Using-Awesome-ChatGPT-Prompts][Using Awesome ChatGPT Prompts]]
- [[#Browsing-Available-Prompts][Browsing Available Prompts]]
- [[#Categorized-Browsing][Categorized Browsing]]
- [[#Viewing-Prompt-Details][Viewing Prompt Details]]
- [[#Updating-Prompts][Updating Prompts]]
- [[#Setting-Without-Sending][Setting Without Sending]]
- [[#Example-Usage][Example Usage]]

<<What-is-Awesome-ChatGPT-Prompts_003f>>
*** 16.1 What is Awesome ChatGPT Prompts? [[#What-is-Awesome-ChatGPT-Prompts_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-is-awesome-chatgpt-prompts
:CLASS: section
:END:
Awesome ChatGPT Prompts is a curated collection of prompt templates
created by the community and maintained in the GitHub repository at
[[https://github.com/f/awesome-chatgpt-prompts]]. These prompts are
designed to make ChatGPT (and other LLMs) act as various specialized
personas or experts, such as:

- Writing professionals (poets, storytellers, copywriters)
- Technical experts (programmers, researchers, scientists)
- Creative professionals (artists, designers, photographers)
- Business experts (marketers, consultants, strategists)
- And many more specialized roles

<<Setting-Up-Awesome-ChatGPT-Prompts>>
*** 16.2 Setting Up Awesome ChatGPT Prompts [[#Setting-Up-Awesome-ChatGPT-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: setting-up-awesome-chatgpt-prompts
:CLASS: section
:END:
To set up the Awesome ChatGPT Prompts integration:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-setup
#+end_src

This will:

1. Create a sparse checkout of the Awesome ChatGPT Prompts repository
2. Download only the necessary files (prompts.csv and README)
3. Populate and categorize the available prompts

<<Using-Awesome-ChatGPT-Prompts>>
*** 16.3 Using Awesome ChatGPT Prompts [[#Using-Awesome-ChatGPT-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: using-awesome-chatgpt-prompts
:CLASS: section
:END:
To use an Awesome ChatGPT Prompt:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-send
#+end_src

or press =C-c w= and then =s=.

You'll be prompted to:

1. Select a prompt from the categorized list
2. Enter text to process (or use selected text)

The selected prompt will be used as a system prompt for your request,
transforming how the AI responds to your text.

<<Browsing-Available-Prompts>>
*** 16.4 Browsing Available Prompts [[#Browsing-Available-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: browsing-available-prompts
:CLASS: section
:END:
To see all available prompts:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-list-prompts
#+end_src

or press =C-c w= and then =l=.

This shows:

- Prompt titles
- Categories
- Preview of prompt content

<<Categorized-Browsing>>
*** 16.5 Categorized Browsing [[#Categorized-Browsing][¶]]
:PROPERTIES:
:CUSTOM_ID: categorized-browsing
:CLASS: section
:END:
Ollama Buddy automatically categorizes the Awesome ChatGPT Prompts into
useful groups:

- writing - For writing, poetry, and creative content
- code - For programming and development
- business - For marketing, entrepreneurship, and business strategy
- academic - For educational and research content
- creative - For artistic and design-related prompts
- philosophy - For philosophical reasoning and ethics
- health - For medical, fitness, and wellness
- legal - For law-related prompts
- finance - For financial advice and analysis
- other - Miscellaneous prompts

To browse by category:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-show-prompts-menu
#+end_src

or press =C-c w= and then =c=.

<<Viewing-Prompt-Details>>
*** 16.6 Viewing Prompt Details [[#Viewing-Prompt-Details][¶]]
:PROPERTIES:
:CUSTOM_ID: viewing-prompt-details
:CLASS: section
:END:
To see the full content of a specific prompt:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-show-prompt
#+end_src

or press =C-c w= and then =v=.

Select a prompt to see its complete template.

<<Updating-Prompts>>
*** 16.7 Updating Prompts [[#Updating-Prompts][¶]]
:PROPERTIES:
:CUSTOM_ID: updating-prompts
:CLASS: section
:END:
To sync with the latest prompts from GitHub:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-sync-prompts
#+end_src

or press =C-c w= and then =S=.

<<Setting-Without-Sending>>
*** 16.8 Setting Without Sending [[#Setting-Without-Sending][¶]]
:PROPERTIES:
:CUSTOM_ID: setting-without-sending
:CLASS: section
:END:
To set a prompt as the system prompt without sending text:

#+begin_src example-preformatted
M-x ollama-buddy-awesome-set-system-prompt
#+end_src

or press =C-c w= and then =p=.

This is useful when you want to set up a specific persona before
starting a conversation.

<<Example-Usage>>
*** 16.9 Example Usage [[#Example-Usage][¶]]
:PROPERTIES:
:CUSTOM_ID: example-usage
:CLASS: section
:END:
Some popular prompts include:

- "Act as a poet" - Transforms your text into poetry
- "Act as a Linux terminal" - Simulates a Linux terminal interface
- "Act as a gaslighter" - Responds in a deliberately confusing manner
- "Act as a javascript console" - Simulates a JavaScript console
- "Act as an English translator" - Translates text to proper English

--------------

<<Remote-Providers>>

Next: [[#Ollama-Cloud-Models][Ollama Cloud Models]], Previous:
[[#Awesome-ChatGPT-Prompts][Awesome ChatGPT Prompts]], Up:
[[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 17 Remote Providers [[#Remote-Providers-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Remote-Providers-1
:CLASS: chapter
:END:
- [[#Overview-3][Overview]]
- [[#Status-Line-Indicators][Status Line Indicators]]
- [[#Setting-Up-API-Access][Setting Up API Access]]
- [[#Selecting-Remote-Models][Selecting Remote Models]]
- [[#Provider-Configuration][Provider Configuration]]
- [[#History-Management][History Management]]

<<Overview-3>>
*** 17.1 Overview [[#Overview-3][¶]]
:PROPERTIES:
:CUSTOM_ID: overview-3
:CLASS: section
:END:
Ollama Buddy integrates with multiple commercial AI services, each
loaded on-demand:

- OpenAI (prefix: =a:=) :: Load with
  =(require 'ollama-buddy-openai nil t)=

- Claude/Anthropic (prefix: =c:=) :: Load with
  =(require 'ollama-buddy-claude nil t)=

- Google Gemini (prefix: =g:=) :: Load with
  =(require 'ollama-buddy-gemini nil t)=

- Grok/X (prefix: =k:=) :: Load with
  =(require 'ollama-buddy-grok nil t)=

- GitHub Copilot (prefix: =p:=) :: Load with
  =(require 'ollama-buddy-copilot nil t)=

- Codestral/Mistral (prefix: =s:=) :: Load with
  =(require 'ollama-buddy-codestral nil t)=

Local Ollama models use the =o:= prefix when remote models are
available, or no prefix when only local models are present.

<<Status-Line-Indicators>>
*** 17.2 Status Line Indicators [[#Status-Line-Indicators][¶]]
:PROPERTIES:
:CUSTOM_ID: status-line-indicators
:CLASS: section
:END:
The header line shows compact indicators for loaded providers:

- =☁= - Currently using an Ollama cloud model
- =a= - OpenAI provider loaded
- =c= - Claude provider loaded
- =g= - Gemini provider loaded
- =k= - Grok provider loaded
- =p= - GitHub Copilot provider loaded
- =s= - Codestral provider loaded

Example: =☁acgkps= means cloud model with all six external providers
loaded.

<<Setting-Up-API-Access>>
*** 17.3 Setting Up API Access [[#Setting-Up-API-Access][¶]]
:PROPERTIES:
:CUSTOM_ID: setting-up-api-access
:CLASS: section
:END:
Before using commercial APIs, you need to set up API keys. The
recommended approach is to use Emacs' built-in auth-source:

#+begin_src example-preformatted
;; Add to ~/.authinfo.gpg (encrypted)
machine ollama-buddy-openai login apikey password YOUR_OPENAI_API_KEY
machine ollama-buddy-claude login apikey password YOUR_CLAUDE_API_KEY
machine ollama-buddy-gemini login apikey password YOUR_GEMINI_API_KEY
machine ollama-buddy-grok login apikey password YOUR_GROK_API_KEY
machine ollama-buddy-copilot login apikey password YOUR_GITHUB_TOKEN
machine ollama-buddy-codestral login apikey password YOUR_CODESTRAL_API_KEY
#+end_src

Then configure with auth-source:

#+begin_src example-preformatted
(setq ollama-buddy-openai-api-key
      (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
#+end_src

<<Selecting-Remote-Models>>
*** 17.4 Selecting Remote Models [[#Selecting-Remote-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: selecting-remote-models
:CLASS: section
:END:
All remote models appear in the model selection list with their provider
prefix:

#+begin_src example-preformatted
M-x ollama-buddy--swap-model
#+end_src

or press =C-c m=.

Models will appear like: =a:gpt-4=, =c:claude-3-opus=, =g:gemini-pro=,
etc.

<<Provider-Configuration>>
*** 17.5 Provider Configuration [[#Provider-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: provider-configuration
:CLASS: section
:END:
- [[#OpenAI-Configuration][OpenAI Configuration]]
- [[#Claude-Configuration][Claude Configuration]]
- [[#Gemini-Configuration][Gemini Configuration]]
- [[#Grok-Configuration][Grok Configuration]]
- [[#GitHub-Copilot-Configuration][GitHub Copilot Configuration]]
- [[#Codestral-Configuration][Codestral Configuration]]

<<OpenAI-Configuration>>
**** 17.5.1 OpenAI Configuration [[#OpenAI-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: openai-configuration
:CLASS: subsection
:END:
- =ollama-buddy-openai-api-key= :: Your OpenAI API key.

- =ollama-buddy-openai-default-model= :: Default OpenAI model to use
  (e.g., "gpt-4").

- =ollama-buddy-openai-temperature= :: Default temperature for OpenAI
  requests (0.0-2.0).

- =ollama-buddy-openai-max-tokens= :: Maximum tokens to generate (nil
  for API default).

<<Claude-Configuration>>
**** 17.5.2 Claude Configuration [[#Claude-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: claude-configuration
:CLASS: subsection
:END:
- =ollama-buddy-claude-api-key= :: Your Anthropic Claude API key.

- =ollama-buddy-claude-default-model= :: Default Claude model to use.

- =ollama-buddy-claude-temperature= :: Default temperature for Claude
  requests (0.0-1.0).

- =ollama-buddy-claude-max-tokens= :: Maximum tokens to generate.

<<Gemini-Configuration>>
**** 17.5.3 Gemini Configuration [[#Gemini-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: gemini-configuration
:CLASS: subsection
:END:
- =ollama-buddy-gemini-api-key= :: Your Google Gemini API key.

- =ollama-buddy-gemini-default-model= :: Default Gemini model to use.

<<Grok-Configuration>>
**** 17.5.4 Grok Configuration [[#Grok-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: grok-configuration
:CLASS: subsection
:END:
- =ollama-buddy-grok-api-key= :: Your X/Grok API key.

- =ollama-buddy-grok-default-model= :: Default Grok model to use.

<<GitHub-Copilot-Configuration>>
**** 17.5.5 GitHub Copilot Configuration [[#GitHub-Copilot-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: github-copilot-configuration
:CLASS: subsection
:END:
- =ollama-buddy-copilot-api-key= :: Your GitHub personal access token
  with Copilot scope. Get your token from
  [[https://github.com/settings/tokens]].

- =ollama-buddy-copilot-default-model= :: Default Copilot model to use
  (e.g., "gpt-4o").

- =ollama-buddy-copilot-available-models= :: List of models available
  through GitHub Copilot.

Note: GitHub Copilot requires an active Copilot subscription.

<<Codestral-Configuration>>
**** 17.5.6 Codestral Configuration [[#Codestral-Configuration][¶]]
:PROPERTIES:
:CUSTOM_ID: codestral-configuration
:CLASS: subsection
:END:
- =ollama-buddy-codestral-api-key= :: Your Mistral Codestral API key.

- =ollama-buddy-codestral-default-model= :: Default Codestral model to
  use.

<<History-Management>>
*** 17.6 History Management [[#History-Management][¶]]
:PROPERTIES:
:CUSTOM_ID: history-management
:CLASS: section
:END:
Each provider maintains its own conversation history, ensuring context
is preserved appropriately per service.

--------------

<<Ollama-Cloud-Models>>

Next: [[#Global-System-Prompt][Global System Prompt]], Previous:
[[#Remote-Providers][Remote Providers]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 18 Ollama Cloud Models [[#Ollama-Cloud-Models-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Ollama-Cloud-Models-1
:CLASS: chapter
:END:
- [[#Overview-4][Overview]]
- [[#Available-Cloud-Models][Available Cloud Models]]
- [[#Selecting-Cloud-Models][Selecting Cloud Models]]
- [[#Cloud-Authentication][Cloud Authentication]]
- [[#Cloud-Model-Indicator][Cloud Model Indicator]]
- [[#Configuration-3][Configuration]]

<<Overview-4>>
*** 18.1 Overview [[#Overview-4][¶]]
:PROPERTIES:
:CUSTOM_ID: overview-4
:CLASS: section
:END:
Ollama cloud models run on ollama.com infrastructure and provide access
to large models without local hardware requirements. These models
require authentication via the Ollama CLI.

<<Available-Cloud-Models>>
*** 18.2 Available Cloud Models [[#Available-Cloud-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: available-cloud-models
:CLASS: section
:END:
Cloud models are configured via =ollama-buddy-cloud-models=:

#+begin_src example-preformatted
(setq ollama-buddy-cloud-models
  '("qwen3-coder:480b-cloud"
    "kimi-k2.5:cloud"
    "deepseek-v3.1:671b-cloud"
    "gpt-oss:120b-cloud"
    "gpt-oss:20b-cloud"
    "glm-4.7:cloud"
    "minimax-m2.1:cloud"))
#+end_src

Cloud models have a =-cloud= suffix in their names.

<<Selecting-Cloud-Models>>
*** 18.3 Selecting Cloud Models [[#Selecting-Cloud-Models][¶]]
:PROPERTIES:
:CUSTOM_ID: selecting-cloud-models
:CLASS: section
:END:
To select a cloud model:

- Use =C-u C-c m= (universal argument with model switch)
- Use the transient menu: =C-c O= then "Model > Cloud"

<<Cloud-Authentication>>
*** 18.4 Cloud Authentication [[#Cloud-Authentication][¶]]
:PROPERTIES:
:CUSTOM_ID: cloud-authentication
:CLASS: section
:END:
Authentication is handled via the Ollama CLI:

- Sign In (=M-x ollama-buddy-cloud-signin=) :: Opens your browser for
  Ollama cloud login. Accessible from transient menu "Cloud Auth > Sign
  In".

- Sign Out (=M-x ollama-buddy-cloud-signout=) :: Signs out from cloud
  services. Accessible from transient menu "Cloud Auth > Sign Out".

- Check Status (=M-x ollama-buddy-cloud-status=) :: Verifies your
  authentication status. Accessible from transient menu "Cloud Auth >
  Auth Status".

<<Cloud-Model-Indicator>>
*** 18.5 Cloud Model Indicator [[#Cloud-Model-Indicator][¶]]
:PROPERTIES:
:CUSTOM_ID: cloud-model-indicator
:CLASS: section
:END:
When using a cloud model, the header line displays a =☁= symbol to
indicate cloud model usage.

<<Configuration-3>>
*** 18.6 Configuration [[#Configuration-3][¶]]
:PROPERTIES:
:CUSTOM_ID: configuration-1
:CLASS: section
:END:
- =ollama-buddy-cloud-models= :: List of available Ollama cloud models.

- =ollama-buddy-ollama-executable= :: Path to the ollama CLI executable
  (default: "ollama"). Used for signin/signout commands.

--------------

<<Global-System-Prompt>>

Next: [[#Advanced-Usage][Advanced Usage]], Previous:
[[#Ollama-Cloud-Models][Ollama Cloud Models]], Up: [[#Top][Ollama
Buddy]]   [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 19 Global System Prompt [[#Global-System-Prompt-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Global-System-Prompt-1
:CLASS: chapter
:END:
- [[#Overview-5][Overview]]
- [[#Configuration-4][Configuration]]
- [[#Toggling-Global-Prompt][Toggling Global Prompt]]
- [[#How-It-Works-1][How It Works]]

<<Overview-5>>
*** 19.1 Overview [[#Overview-5][¶]]
:PROPERTIES:
:CUSTOM_ID: overview-5
:CLASS: section
:END:
The global system prompt provides baseline formatting instructions that
are automatically prepended to all requests, regardless of the
session-specific system prompt. This ensures consistent response
formatting across all interactions.

<<Configuration-4>>
*** 19.2 Configuration [[#Configuration-4][¶]]
:PROPERTIES:
:CUSTOM_ID: configuration-2
:CLASS: section
:END:
- =ollama-buddy-global-system-prompt= :: The global prompt content
  prepended to all requests.

  #+begin_src example-preformatted
  (setq ollama-buddy-global-system-prompt
    "Format responses in plain prose. Avoid markdown tables unless
  specifically requested. Use clear paragraphs and bullet points
  for structured information.")
  #+end_src

- =ollama-buddy-global-system-prompt-enabled= :: Whether to enable the
  global system prompt (default: t).

  #+begin_src example-preformatted
  (setq ollama-buddy-global-system-prompt-enabled t)
  #+end_src

<<Toggling-Global-Prompt>>
*** 19.3 Toggling Global Prompt [[#Toggling-Global-Prompt][¶]]
:PROPERTIES:
:CUSTOM_ID: toggling-global-prompt
:CLASS: section
:END:
To toggle the global system prompt on or off:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-global-system-prompt
#+end_src

or use the transient menu "Settings > Global Prompt".

<<How-It-Works-1>>
*** 19.4 How It Works [[#How-It-Works-1][¶]]
:PROPERTIES:
:CUSTOM_ID: how-it-works-1
:CLASS: section
:END:
When enabled, the global system prompt is combined with any
session-specific system prompt:

- If both global and session prompts exist, they are combined with two
  newlines between them
- If only the global prompt exists, it is used alone
- If only a session prompt exists, it is used alone
- If neither exists, no system prompt is sent

This allows you to maintain consistent formatting while still using
specialized prompts for specific tasks.

--------------

<<Advanced-Usage>>

Next: [[#API-Reference][API Reference]], Previous:
[[#Global-System-Prompt][Global System Prompt]], Up: [[#Top][Ollama
Buddy]]   [[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 20 Advanced Usage [[#Advanced-Usage-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Advanced-Usage-1
:CLASS: chapter
:END:
- [[#Managing-Token-Usage][Managing Token Usage]]
- [[#Customizing-the-Interface][Customizing the Interface]]
- [[#Editing-Conversation-History][Editing Conversation History]]
- [[#Advanced-System-Prompt-Management][Advanced System Prompt
  Management]]
- [[#Using-Direct-API-Access][Using Direct API Access]]

<<Managing-Token-Usage>>
*** 20.1 Managing Token Usage [[#Managing-Token-Usage][¶]]
:PROPERTIES:
:CUSTOM_ID: managing-token-usage
:CLASS: section
:END:
Ollama Buddy can track token usage statistics:

To toggle token statistics display after responses:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-token-display
#+end_src

or press =C-c T= in the chat buffer.

To view detailed token usage statistics:

#+begin_src example-preformatted
M-x ollama-buddy-display-token-stats
#+end_src

or press =C-c #= in the chat buffer.

This displays a combined view with token usage graphs by model, average
token rates, and recent interactions.

<<Customizing-the-Interface>>
*** 20.2 Customizing the Interface [[#Customizing-the-Interface][¶]]
:PROPERTIES:
:CUSTOM_ID: customizing-the-interface
:CLASS: section
:END:
- [[#Streaming-Options][Streaming Options]]
- [[#Debug-Mode][Debug Mode]]

<<Streaming-Options>>
**** 20.2.1 Streaming Options [[#Streaming-Options][¶]]
:PROPERTIES:
:CUSTOM_ID: streaming-options
:CLASS: subsection
:END:
Control how responses are displayed:

- =ollama-buddy-streaming-enabled= :: Toggle streaming mode where
  responses appear token by token.

  #+begin_src example-preformatted
  M-x ollama-buddy-toggle-streaming
  #+end_src

- =ollama-buddy-auto-scroll= :: Enable auto-scrolling during streaming
  output.

- =ollama-buddy-pulse-response= :: Flash the response when streaming
  completes.

<<Debug-Mode>>
**** 20.2.2 Debug Mode [[#Debug-Mode][¶]]
:PROPERTIES:
:CUSTOM_ID: debug-mode
:CLASS: subsection
:END:
For advanced troubleshooting, you can enable debug mode:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-debug-mode
#+end_src

or press =C-c B= in the chat buffer.

This shows raw JSON messages in a debug buffer.

<<Editing-Conversation-History>>
*** 20.3 Editing Conversation History [[#Editing-Conversation-History][¶]]
:PROPERTIES:
:CUSTOM_ID: editing-conversation-history
:CLASS: section
:END:
To manually edit conversation history:

#+begin_src example-preformatted
M-x ollama-buddy-history-edit
#+end_src

or press =C-c J= in the chat buffer.

This opens an editable buffer with the conversation history. You can
modify it and press =C-c C-c= to save or =C-c C-k= to cancel.

To edit history for a specific model, use =C-u C-c J=.

<<Advanced-System-Prompt-Management>>
*** 20.4 Advanced System Prompt Management [[#Advanced-System-Prompt-Management][¶]]
:PROPERTIES:
:CUSTOM_ID: advanced-system-prompt-management
:CLASS: section
:END:
For more control over system prompts:

- [[#Setting-a-system-prompt-without-sending][Setting a system prompt
  without sending]]
- [[#Using-a-system-prompt-from-Fabric][Using a system prompt from
  Fabric]]

<<Setting-a-system-prompt-without-sending>>
**** 20.4.1 Setting a system prompt without sending [[#Setting-a-system-prompt-without-sending][¶]]
:PROPERTIES:
:CUSTOM_ID: setting-a-system-prompt-without-sending
:CLASS: subsection
:END:

#+begin_src example-preformatted
(ollama-buddy-set-system-prompt)
#+end_src

Enter your system prompt, then press =C-c s=.

<<Using-a-system-prompt-from-Fabric>>
**** 20.4.2 Using a system prompt from Fabric [[#Using-a-system-prompt-from-Fabric][¶]]
:PROPERTIES:
:CUSTOM_ID: using-a-system-prompt-from-fabric
:CLASS: subsection
:END:

#+begin_src example-preformatted
M-x ollama-buddy-fabric-set-system-prompt
#+end_src

or press =C-c f p=.

<<Using-Direct-API-Access>>
*** 20.5 Using Direct API Access [[#Using-Direct-API-Access][¶]]
:PROPERTIES:
:CUSTOM_ID: using-direct-api-access
:CLASS: section
:END:
For direct programmatic access to Ollama:

#+begin_src example-preformatted
(ollama-buddy--make-request "/api/tags" "GET")
#+end_src

Or with a payload:

#+begin_src example-preformatted
(ollama-buddy--make-request "/api/chat" "POST" 
                           (json-encode '((model . "llama3:latest")
                                         (prompt . "Hello"))))
#+end_src

--------------

<<API-Reference>>

Next: [[#FAQ][Frequently Asked Questions]], Previous:
[[#Advanced-Usage][Advanced Usage]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 21 API Reference [[#API-Reference-1][¶]]
:PROPERTIES:
:CUSTOM_ID: API-Reference-1
:CLASS: chapter
:END:
- [[#Interactive-Functions][Interactive Functions]]
- [[#Core-Functions][Core Functions]]
- [[#Customization-Functions][Customization Functions]]

<<Interactive-Functions>>
*** 21.1 Interactive Functions [[#Interactive-Functions][¶]]
:PROPERTIES:
:CUSTOM_ID: interactive-functions
:CLASS: section
:END:
- =ollama-buddy-menu= :: Display the main Ollama Buddy menu.

- =ollama-buddy-transient-menu= :: Display the transient-based menu.

- =ollama-buddy--open-chat= :: Open the chat buffer.

- =ollama-buddy--send-prompt= :: Send the current prompt to the AI.

- =ollama-buddy--swap-model= :: Switch to a different model. With prefix
  argument, select from cloud models.

- =ollama-buddy--swap-model-cloud= :: Switch to an Ollama cloud model.

- =ollama-buddy-cloud-signin= :: Sign in to Ollama cloud services.

- =ollama-buddy-cloud-signout= :: Sign out from Ollama cloud services.

- =ollama-buddy-cloud-status= :: Check Ollama cloud authentication
  status.

- =ollama-buddy-toggle-global-system-prompt= :: Toggle the global system
  prompt on or off.

- =ollama-buddy-unload-model= :: Unload a specific model to free up
  resources.

- =ollama-buddy-unload-all-models= :: Unload all currently running
  models.

- =ollama-buddy-manage-models= :: Display and manage available models.

- =ollama-buddy-pull-model= :: Pull a new model from Ollama Hub.

- =ollama-buddy-import-gguf-file= :: Import a GGUF file to create a
  custom model.

- =ollama-buddy-set-system-prompt= :: Set the current prompt as the
  system prompt.

- =ollama-buddy-reset-system-prompt= :: Reset the system prompt to
  default (none).

- =ollama-buddy-sessions-save= :: Save the current conversation as a
  session.

- =ollama-buddy-sessions-load= :: Load a previously saved session.

- =ollama-buddy-sessions-list= :: Display a list of saved sessions.

- =ollama-buddy-sessions-delete= :: Delete a saved session.

- =ollama-buddy-sessions-new= :: Start a new session.

- =ollama-buddy-toggle-history= :: Toggle conversation history on/off.

- =ollama-buddy-clear-history= :: Clear the conversation history.

- =ollama-buddy-history-edit= :: Display the conversation history.

- =ollama-buddy-roles-switch-role= :: Switch to a different role.

- =ollama-buddy-role-creator-create-new-role= :: Create a new role.

- =ollama-buddy-params-display= :: Display current parameter settings.

- =ollama-buddy-params-edit= :: Edit a specific parameter.

- =ollama-buddy-params-reset= :: Reset all parameters to defaults.

- =ollama-buddy-toggle-params-in-header= :: Toggle display of parameters
  in header.

- =ollama-buddy-toggle-token-display= :: Toggle display of token
  statistics.

- =ollama-buddy-display-token-stats= :: Display token usage statistics
  with graphs and recent interactions.

- =ollama-buddy-fabric-setup= :: Set up Fabric pattern integration.

- =ollama-buddy-fabric-sync-patterns= :: Sync with the latest Fabric
  patterns.

- =ollama-buddy-fabric-list-patterns= :: List available Fabric patterns.

- =ollama-buddy-fabric-send= :: Apply a Fabric pattern to selected text.

- =ollama-buddy-toggle-markdown-conversion= :: Toggle Markdown to Org
  conversion.

- =ollama-buddy-toggle-streaming= :: Toggle streaming mode for
  responses.

- =ollama-buddy-toggle-reasoning-visibility= :: Toggle visibility of
  reasoning/thinking sections in responses.

- =ollama-buddy-switch-communication-backend= :: Interactively switch
  between network-process and curl backends.

- =ollama-buddy-test-communication-backend= :: Test the current
  communication backend.

- =ollama-buddy-toggle-debug-mode= :: Toggle display of debug
  information.

- =ollama-buddy-set-model-context-size= :: Set the context size for a
  specific model.

- =ollama-buddy-toggle-context-percentage= :: Toggle context percentage
  display in the status bar.

- =ollama-buddy-show-context-info= :: Display detailed context usage
  information.

- =ollama-buddy-set-max-history-length= :: Set the maximum number of
  message pairs to keep in history.

- =ollama-buddy-user-prompts-save= :: Save the current system prompt.

- =ollama-buddy-user-prompts-load= :: Load a previously saved system
  prompt.

- =ollama-buddy-user-prompts-list= :: Display a list of all saved user
  system prompts.

- =ollama-buddy-user-prompts-edit= :: Edit a user system prompt.

- =ollama-buddy-user-prompts-delete= :: Delete a user system prompt.

- =ollama-buddy-user-prompts-create-new= :: Create a new system prompt
  from scratch.

- =ollama-buddy-transient-user-prompts-menu= :: Display the transient
  menu for user system prompts.

<<Core-Functions>>
*** 21.2 Core Functions [[#Core-Functions][¶]]
:PROPERTIES:
:CUSTOM_ID: core-functions
:CLASS: section
:END:
- =ollama-buddy--send= :: Send a prompt to Ollama.

- =ollama-buddy--make-request= :: Make a generic request to the Ollama
  API.

- =ollama-buddy--get-models= :: Get a list of available models.

- =ollama-buddy--get-valid-model= :: Get a valid model with fallback
  handling.

- =ollama-buddy--add-to-history= :: Add a message to the conversation
  history.

- =ollama-buddy--get-history-for-request= :: Get history for the current
  request.

- =ollama-buddy--prepare-prompt-area= :: Prepare the prompt area in the
  buffer.

- =ollama-buddy--update-status= :: Update the status display.

<<Customization-Functions>>
*** 21.3 Customization Functions [[#Customization-Functions][¶]]
:PROPERTIES:
:CUSTOM_ID: customization-functions
:CLASS: section
:END:
- =ollama-buddy-update-command-with-params= :: Update a command
  definition with new properties and parameters.

- =ollama-buddy-update-menu-entry= :: Update a menu entry's properties.

- =ollama-buddy-add-model-to-menu-entry= :: Associate a specific model
  with a menu entry.

- =ollama-buddy-add-parameters-to-command= :: Add specific parameters to
  a command definition.

--------------

<<FAQ>>

Next: [[#Troubleshooting][Troubleshooting]], Previous:
[[#API-Reference][API Reference]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 22 Frequently Asked Questions [[#Frequently-Asked-Questions][¶]]
:PROPERTIES:
:CUSTOM_ID: Frequently-Asked-Questions
:CLASS: chapter
:END:
- [[#General-Questions][General Questions]]
- [[#Usage-Questions][Usage Questions]]
- [[#Troubleshooting-2][Troubleshooting]]

<<General-Questions>>
*** 22.1 General Questions [[#General-Questions][¶]]
:PROPERTIES:
:CUSTOM_ID: general-questions
:CLASS: section
:END:
- [[#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f][What
  is the difference between Ollama Buddy and other AI assistants?]]
- [[#Does-Ollama-Buddy-require-an-internet-connection_003f][Does Ollama
  Buddy require an internet connection?]]
- [[#How-do-I-use-Ollama-cloud-models_003f][How do I use Ollama cloud
  models?]]
- [[#Which-models-work-best-with-Ollama-Buddy_003f][Which models work
  best with Ollama Buddy?]]
- [[#How-much-RAM-do-I-need_003f][How much RAM do I need?]]

<<What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f>>
**** 22.1.1 What is the difference between Ollama Buddy and other AI assistants? [[#What-is-the-difference-between-Ollama-Buddy-and-other-AI-assistants_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-is-the-difference-between-ollama-buddy-and-other-ai-assistants
:CLASS: subsection
:END:
Ollama Buddy integrates with Ollama to run LLMs locally, offering
privacy, customization, and seamless Emacs integration without relying
on external API services.

<<Does-Ollama-Buddy-require-an-internet-connection_003f>>
**** 22.1.2 Does Ollama Buddy require an internet connection? [[#Does-Ollama-Buddy-require-an-internet-connection_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: does-ollama-buddy-require-an-internet-connection
:CLASS: subsection
:END:
For local Ollama models, no internet connection is required after
pulling them. Internet is needed for:

- Pulling new models from Ollama
- Using Ollama cloud models
- Using remote providers (OpenAI, Claude, Gemini, Grok, Codestral)
- Syncing Fabric patterns or Awesome ChatGPT Prompts

<<How-do-I-use-Ollama-cloud-models_003f>>
**** 22.1.3 How do I use Ollama cloud models? [[#How-do-I-use-Ollama-cloud-models_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-i-use-ollama-cloud-models
:CLASS: subsection
:END:
Cloud models require authentication:

1. Run =M-x ollama-buddy-cloud-signin= to sign in
2. Select cloud models with =C-u C-c m=
3. The =☁= symbol in the header indicates cloud model usage

<<Which-models-work-best-with-Ollama-Buddy_003f>>
**** 22.1.4 Which models work best with Ollama Buddy? [[#Which-models-work-best-with-Ollama-Buddy_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: which-models-work-best-with-ollama-buddy
:CLASS: subsection
:END:
Most models supported by Ollama work well. Popular choices include:

- llama3:latest - Good general purpose assistant
- codellama:latest - Excellent for code-related tasks
- mistral:latest - Good balance of performance and quality
- phi:latest - Smaller model that works well on limited hardware

<<How-much-RAM-do-I-need_003f>>
**** 22.1.5 How much RAM do I need? [[#How-much-RAM-do-I-need_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-much-ram-do-i-need
:CLASS: subsection
:END:
It depends on the model:

- Small models (7B) - 8GB minimum, 16GB recommended
- Medium models (13B) - 16GB minimum, 24GB+ recommended
- Large models (34B+) - 32GB+ recommended

Quantized models (e.g., Q4_K_M variants) require less RAM.

<<Usage-Questions>>
*** 22.2 Usage Questions [[#Usage-Questions][¶]]
:PROPERTIES:
:CUSTOM_ID: usage-questions
:CLASS: section
:END:
- [[#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f][How do I
  cancel a request that's taking too long?]]
- [[#How-can-I-save-my-conversations_003f][How can I save my
  conversations?]]
- [[#Can-I-use-multiple-models-in-the-same-conversation_003f][Can I use
  multiple models in the same conversation?]]
- [[#How-do-I-clear-the-conversation-history_003f][How do I clear the
  conversation history?]]
- [[#How-can-I-create-a-custom-command_003f][How can I create a custom
  command?]]
- [[#How-can-I-manage-context-windows_003f][How can I manage context
  windows?]]
- [[#What-happens-when-I-exceed-the-context-limit_003f][What happens
  when I exceed the context limit?]]
- [[#How-do-I-create-effective-system-prompts_003f][How do I create
  effective system prompts?]]
- [[#Where-are-my-system-prompts-stored_003f][Where are my system
  prompts stored?]]

<<How-do-I-cancel-a-request-that_0027s-taking-too-long_003f>>
**** 22.2.1 How do I cancel a request that's taking too long? [[#How-do-I-cancel-a-request-that_0027s-taking-too-long_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-i-cancel-a-request-thats-taking-too-long
:CLASS: subsection
:END:
Press =C-c k= in the chat buffer or select "Kill Request" from the menu.

<<How-can-I-save-my-conversations_003f>>
**** 22.2.2 How can I save my conversations? [[#How-can-I-save-my-conversations_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-can-i-save-my-conversations
:CLASS: subsection
:END:
Use =C-c S= to save the current session, giving it a name. You can
restore it later with =C-c L=.

<<Can-I-use-multiple-models-in-the-same-conversation_003f>>
**** 22.2.3 Can I use multiple models in the same conversation? [[#Can-I-use-multiple-models-in-the-same-conversation_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: can-i-use-multiple-models-in-the-same-conversation
:CLASS: subsection
:END:
Yes, you can switch models at any time with =C-c m=. Each model
maintains its own conversation history.

<<How-do-I-clear-the-conversation-history_003f>>
**** 22.2.4 How do I clear the conversation history? [[#How-do-I-clear-the-conversation-history_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-i-clear-the-conversation-history
:CLASS: subsection
:END:
Press =C-c X= to clear history, or =C-c N= to start a completely new
session.

<<How-can-I-create-a-custom-command_003f>>
**** 22.2.5 How can I create a custom command? [[#How-can-I-create-a-custom-command_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-can-i-create-a-custom-command
:CLASS: subsection
:END:
The easiest way is through the role creator: press =C-c E= and follow
the prompts to create commands with specific prompts, models, and
parameters.

<<How-can-I-manage-context-windows_003f>>
**** 22.2.6 How can I manage context windows? [[#How-can-I-manage-context-windows_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-can-i-manage-context-windows
:CLASS: subsection
:END:
Ollama Buddy provides several options:

- Enable context monitoring with
  =(setq ollama-buddy-show-context-percentage t)=
- Use C-c C to check current context usage
- Limit history length with C-c Y
- Set model-specific context sizes with C-c $

<<What-happens-when-I-exceed-the-context-limit_003f>>
**** 22.2.7 What happens when I exceed the context limit? [[#What-happens-when-I-exceed-the-context-limit_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-happens-when-i-exceed-the-context-limit
:CLASS: subsection
:END:
When context monitoring is enabled:

- You'll get a warning when approaching the limit (85-100%)
- You'll get an error dialog at or above 100%
- You can choose to proceed anyway or modify your content

<<How-do-I-create-effective-system-prompts_003f>>
**** 22.2.8 How do I create effective system prompts? [[#How-do-I-create-effective-system-prompts_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-i-create-effective-system-prompts
:CLASS: subsection
:END:
Effective system prompts typically include:

- Clear role definition for the AI
- Specific guidelines for response format and style
- Examples of desired output (when applicable)
- Constraints or limitations to observe

Start simple, test the response, and refine iteratively.

<<Where-are-my-system-prompts-stored_003f>>
**** 22.2.9 Where are my system prompts stored? [[#Where-are-my-system-prompts-stored_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: where-are-my-system-prompts-stored
:CLASS: subsection
:END:
System prompts are stored as .org files in the directory specified by
‘ollama-buddy-user-prompts-directory‘, which defaults to
‘~/.emacs.d/ollama-buddy-user-prompts/‘.

<<Troubleshooting-2>>
*** 22.3 Troubleshooting [[#Troubleshooting-2][¶]]
:PROPERTIES:
:CUSTOM_ID: troubleshooting-1
:CLASS: section
:END:
- [[#Ollama-Buddy-shows-_0022OFFLINE_0022-status][Ollama Buddy shows
  "OFFLINE" status]]
- [[#Responses-are-slow-or-the-model-seems-to-hang][Responses are slow
  or the model seems to hang]]
- [[#Getting-_0022error-parsing-model_0022-when-pulling-a-model][Getting
  "error parsing model" when pulling a model]]
- [[#Model-responses-are-low-quality-or-truncated][Model responses are
  low quality or truncated]]
- [[#How-do-system-prompts-differ-from-regular-prompts_003f][How do
  system prompts differ from regular prompts?]]
- [[#What-is-the-global-system-prompt_003f][What is the global system
  prompt?]]
- [[#How-do-I-use-image-analysis_002fvision-features_003f][How do I use
  image analysis/vision features?]]
- [[#Can-I-share-system-prompts-between-different-installations_003f][Can
  I share system prompts between different installations?]]

<<Ollama-Buddy-shows-_0022OFFLINE_0022-status>>
**** 22.3.1 Ollama Buddy shows "OFFLINE" status [[#Ollama-Buddy-shows-_0022OFFLINE_0022-status][¶]]
:PROPERTIES:
:CUSTOM_ID: ollama-buddy-shows-offline-status
:CLASS: subsection
:END:
Ensure that:

- Ollama is installed and running
- The hostname and port are correctly configured (=ollama-buddy-host=
  and =ollama-buddy-port=)
- Your firewall isn't blocking connections

<<Responses-are-slow-or-the-model-seems-to-hang>>
**** 22.3.2 Responses are slow or the model seems to hang [[#Responses-are-slow-or-the-model-seems-to-hang][¶]]
:PROPERTIES:
:CUSTOM_ID: responses-are-slow-or-the-model-seems-to-hang
:CLASS: subsection
:END:
Try:

- Using a smaller model
- Adjusting the =num_ctx= parameter to a smaller value
- Setting =low_vram= to =t= if you have limited GPU memory
- Checking CPU/RAM usage to ensure your system isn't overloaded

<<Getting-_0022error-parsing-model_0022-when-pulling-a-model>>
**** 22.3.3 Getting "error parsing model" when pulling a model [[#Getting-_0022error-parsing-model_0022-when-pulling-a-model][¶]]
:PROPERTIES:
:CUSTOM_ID: getting-error-parsing-model-when-pulling-a-model
:CLASS: subsection
:END:
This usually means:

- The model name is incorrect
- The model is not available in the Ollama repository
- You have network connectivity issues

<<Model-responses-are-low-quality-or-truncated>>
**** 22.3.4 Model responses are low quality or truncated [[#Model-responses-are-low-quality-or-truncated][¶]]
:PROPERTIES:
:CUSTOM_ID: model-responses-are-low-quality-or-truncated
:CLASS: subsection
:END:
Try:

- Increasing the =temperature= parameter for more creative responses
- Increasing =num_predict= for longer responses
- Using a more capable model
- Providing clearer instructions in your prompt

<<How-do-system-prompts-differ-from-regular-prompts_003f>>
**** 22.3.5 How do system prompts differ from regular prompts? [[#How-do-system-prompts-differ-from-regular-prompts_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-system-prompts-differ-from-regular-prompts
:CLASS: subsection
:END:
System prompts provide overall instructions to the AI about its role and
how to respond, while regular prompts are your specific questions or
requests. System prompts persist across the conversation, affecting all
responses, while regular prompts are one-time interactions.

<<What-is-the-global-system-prompt_003f>>
**** 22.3.6 What is the global system prompt? [[#What-is-the-global-system-prompt_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: what-is-the-global-system-prompt
:CLASS: subsection
:END:
The global system prompt is a baseline prompt automatically prepended to
all requests. It's useful for consistent formatting instructions across
all models. Toggle it with
=M-x ollama-buddy-toggle-global-system-prompt=.

<<How-do-I-use-image-analysis_002fvision-features_003f>>
**** 22.3.7 How do I use image analysis/vision features? [[#How-do-I-use-image-analysis_002fvision-features_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: how-do-i-use-image-analysisvision-features
:CLASS: subsection
:END:
1. Select a vision-capable model (e.g., gemma3:4b, llama3.2:3b)
2. Include an image path in your prompt
3. The image will be automatically detected and encoded

Supported formats: PNG, JPG, JPEG, WebP, GIF.

<<Can-I-share-system-prompts-between-different-installations_003f>>
**** 22.3.8 Can I share system prompts between different installations? [[#Can-I-share-system-prompts-between-different-installations_003f][¶]]
:PROPERTIES:
:CUSTOM_ID: can-i-share-system-prompts-between-different-installations
:CLASS: subsection
:END:
Yes, the user system prompts are stored as plain text .org files in your
‘ollama-buddy-user-prompts-directory‘. You can copy these files to share
them with colleagues or between different machines.

--------------

<<Troubleshooting>>

Next: [[#Contributing][Contributing]], Previous: [[#FAQ][Frequently
Asked Questions]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 23 Troubleshooting [[#Troubleshooting-3][¶]]
:PROPERTIES:
:CUSTOM_ID: Troubleshooting-3
:CLASS: chapter
:END:
- [[#Common-Issues][Common Issues]]
- [[#Debugging][Debugging]]

<<Common-Issues>>
*** 23.1 Common Issues [[#Common-Issues][¶]]
:PROPERTIES:
:CUSTOM_ID: common-issues
:CLASS: section
:END:
- [[#Connection-Problems][Connection Problems]]
- [[#Model-Problems][Model Problems]]
- [[#Interface-Issues][Interface Issues]]

<<Connection-Problems>>
**** 23.1.1 Connection Problems [[#Connection-Problems][¶]]
:PROPERTIES:
:CUSTOM_ID: connection-problems
:CLASS: subsection
:END:
- Symptom: Unable to connect to Ollama server :: - Check if Ollama is
    running with =ps aux | grep ollama=
  - Verify host and port settings (=ollama-buddy-host= and
    =ollama-buddy-port=)
  - Try connecting to Ollama directly:
    =curl http://localhost:11434/api/tags=
- Symptom: Connection breaks during long responses :: - This can happen
    with very large responses
  - Try setting a lower =num_predict= value
  - Check if your OS has any network timeout settings

<<Model-Problems>>
**** 23.1.2 Model Problems [[#Model-Problems][¶]]
:PROPERTIES:
:CUSTOM_ID: model-problems
:CLASS: subsection
:END:
- Symptom: Model loads but gives poor responses :: - Try a different
    model
  - Adjust parameters (increase temperature for more creativity)
  - Provide clearer or more detailed prompts
  - Check if the model is appropriate for your task
- Symptom: Model fails to load or crashes :: - Check system memory usage
  - Try a smaller quantized model
  - Adjust =num_ctx= to a smaller value
  - Set =low_vram= to =t= if using GPU

<<Interface-Issues>>
**** 23.1.3 Interface Issues [[#Interface-Issues][¶]]
:PROPERTIES:
:CUSTOM_ID: interface-issues
:CLASS: subsection
:END:
- Symptom: Chat buffer becomes unresponsive :: - Cancel any running
    requests with =C-c k=
  - Check if Emacs is using high CPU
  - Try disabling token statistics display
  - Close and reopen the chat buffer
- Symptom: Markdown conversion issues :: - Toggle markdown conversion
    off with =C-c C-o=
  - Check if the response contains complex formatting
  - Try editing the history to fix formatting issues

<<Debugging>>
*** 23.2 Debugging [[#Debugging][¶]]
:PROPERTIES:
:CUSTOM_ID: debugging
:CLASS: section
:END:
- [[#Enable-Debug-Mode][Enable Debug Mode]]
- [[#Check-Logs][Check Logs]]
- [[#Report-Issues][Report Issues]]

<<Enable-Debug-Mode>>
**** 23.2.1 Enable Debug Mode [[#Enable-Debug-Mode][¶]]
:PROPERTIES:
:CUSTOM_ID: enable-debug-mode
:CLASS: subsection
:END:
To get more information about what's happening:

#+begin_src example-preformatted
M-x ollama-buddy-toggle-debug-mode
#+end_src

This opens a debug buffer showing raw JSON communication with Ollama.

<<Check-Logs>>
**** 23.2.2 Check Logs [[#Check-Logs][¶]]
:PROPERTIES:
:CUSTOM_ID: check-logs
:CLASS: subsection
:END:
Ollama logs can be useful for troubleshooting:

#+begin_src example-preformatted
tail -f ~/.ollama/logs/ollama.log
#+end_src

<<Report-Issues>>
**** 23.2.3 Report Issues [[#Report-Issues][¶]]
:PROPERTIES:
:CUSTOM_ID: report-issues
:CLASS: subsection
:END:
If you encounter a bug:

1. Enable debug mode
2. Reproduce the issue
3. Copy the debug output
4. Report the issue on GitHub with:
   - Emacs version
   - Ollama version
   - Model used
   - Debug output
   - Steps to reproduce

--------------

<<Contributing>>

Next: [[#Index][Index]], Previous:
[[#Troubleshooting][Troubleshooting]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** 24 Contributing [[#Contributing-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Contributing-1
:CLASS: chapter
:END:
- [[#Getting-Started][Getting Started]]
- [[#Development-Setup][Development Setup]]
- [[#Coding-Guidelines][Coding Guidelines]]
- [[#Testing][Testing]]
- [[#Feature-Requests-and-Bug-Reports][Feature Requests and Bug
  Reports]]

<<Getting-Started>>
*** 24.1 Getting Started [[#Getting-Started][¶]]
:PROPERTIES:
:CUSTOM_ID: getting-started
:CLASS: section
:END:
Ollama Buddy is an open-source project, and contributions are welcome!

1. Fork the repository:
   [[https://github.com/captainflasmr/ollama-buddy]]
2. Clone your fork:
   =git clone https://github.com/YOUR-USERNAME/ollama-buddy.git=
3. Create a branch: =git checkout -b my-feature-branch=
4. Make your changes
5. Test thoroughly
6. Commit with a clear message
7. Push to your fork
8. Create a pull request

<<Development-Setup>>
*** 24.2 Development Setup [[#Development-Setup][¶]]
:PROPERTIES:
:CUSTOM_ID: development-setup
:CLASS: section
:END:
- [[#Required-Tools][Required Tools]]
- [[#Recommended-Packages][Recommended Packages]]

<<Required-Tools>>
**** 24.2.1 Required Tools [[#Required-Tools][¶]]
:PROPERTIES:
:CUSTOM_ID: required-tools
:CLASS: subsection
:END:
- Emacs 28.1+
- Ollama installed and running
- Git

<<Recommended-Packages>>
**** 24.2.2 Recommended Packages [[#Recommended-Packages][¶]]
:PROPERTIES:
:CUSTOM_ID: recommended-packages
:CLASS: subsection
:END:
- package-lint
- flycheck
- elisp-lint

<<Coding-Guidelines>>
*** 24.3 Coding Guidelines [[#Coding-Guidelines][¶]]
:PROPERTIES:
:CUSTOM_ID: coding-guidelines
:CLASS: section
:END:
- Follow Emacs Lisp conventions
- Use two spaces for indentation
- Add documentation strings to functions
- Keep line length under 80 characters
- Use prefix =ollama-buddy--= for internal functions
- Use prefix =ollama-buddy-= for public functions

<<Testing>>
*** 24.4 Testing [[#Testing][¶]]
:PROPERTIES:
:CUSTOM_ID: testing
:CLASS: section
:END:
- [[#Run-Existing-Tests][Run Existing Tests]]
- [[#Adding-New-Tests][Adding New Tests]]

<<Run-Existing-Tests>>
**** 24.4.1 Run Existing Tests [[#Run-Existing-Tests][¶]]
:PROPERTIES:
:CUSTOM_ID: run-existing-tests
:CLASS: subsection
:END:
The package includes comprehensive tests:

#+begin_src example-preformatted
M-x ollama-buddy-run-tests
M-x ollama-buddy-integration-run-tests
M-x ollama-buddy-fabric-run-tests
M-x ollama-buddy-parameter-run-tests
#+end_src

<<Adding-New-Tests>>
**** 24.4.2 Adding New Tests [[#Adding-New-Tests][¶]]
:PROPERTIES:
:CUSTOM_ID: adding-new-tests
:CLASS: subsection
:END:
When adding features, please also add tests:

- Unit tests for individual functions
- Integration tests for API interactions
- Parameter tests for parameter handling

<<Feature-Requests-and-Bug-Reports>>
*** 24.5 Feature Requests and Bug Reports [[#Feature-Requests-and-Bug-Reports][¶]]
:PROPERTIES:
:CUSTOM_ID: feature-requests-and-bug-reports
:CLASS: section
:END:
- Use GitHub Issues for bug reports and feature requests
- Provide clear steps to reproduce bugs
- For feature requests, explain the use case

- [[#User-System-Prompts-Issues][User System Prompts Issues]]

<<User-System-Prompts-Issues>>
**** 24.5.1 User System Prompts Issues [[#User-System-Prompts-Issues][¶]]
:PROPERTIES:
:CUSTOM_ID: user-system-prompts-issues
:CLASS: subsection
:END:
- Symptom: Cannot save system prompts :: - Check if
    ‘ollama-buddy-user-prompts-directory‘ exists and is writable
  - Ensure you have set a system prompt before trying to save it
  - Check for any error messages in the minibuffer or *Messages* buffer
- Symptom: Prompts not appearing in the load menu :: - Verify that
    prompts are saved in the correct format
    (category__title__system.org)
  - Check if the prompts directory contains files with proper formatting
  - Try refreshing the prompts cache with ‘M-x
    ollama-buddy-user-prompts--refresh-cache‘

--------------

<<Index>>

Previous: [[#Contributing][Contributing]], Up: [[#Top][Ollama Buddy]]  
[[[#SEC_Contents][Contents]]][[[#Index][Index]]]

** Index [[#Index-1][¶]]
:PROPERTIES:
:CUSTOM_ID: Index-1
:CLASS: unnumbered
:END:
