#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+startup: showall

#+attr_org: :width 300px
#+attr_html: :width 50%
[[file:img/ollama-buddy-youtube-banner_001.jpg]]

* Ollama Buddy

An Emacs package for interacting with local LLMs via [[https://ollama.ai/][Ollama]], with support for remote providers (OpenAI, Claude, Gemini, Grok, Copilot, Codestral, DeepSeek, OpenRouter) and Ollama Cloud models. Requires Emacs 28.1+.

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

That's it. Start =ollama serve=, open Emacs, press =C-c o=, select =[o]= to open chat, and go.

https://www.youtube.com/@OllamaBuddyforEmacs

** In-buffer replacement demo

Demonstrating in-buffer in-line modification, now we are out of the chat buffer!

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_037.gif]]

** Tool Calling Demo

LLMs can invoke Emacs functions: read/write files, execute shell commands, search buffers, and more.

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_034.gif]]

** Advanced Tool Calling Demo

Demonstrating advanced ollama tool calling with multiple tools working together.

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_035.gif]]

** Quick Demo

Submit queries, swap models, view token usage and statistics:

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_011.gif]]

* Features

- *Tool Calling* -- LLMs can invoke registered Emacs functions (file ops, shell commands, buffer search, calculations). Safe mode restricts to read-only tools.
- *Multiple Providers* -- Local Ollama, Ollama Cloud, OpenAI, Claude, Gemini, Grok, GitHub Copilot, Codestral, DeepSeek, OpenRouter, and any generic OpenAI-compatible server (LM Studio, llama.cpp, vLLM, Jan…)
- *Web Search* -- Inline =@search(query)= syntax for real-time web search via Ollama's API
- *Interactive Menus* -- Transient popup menus with role-based command grouping
- *Roles & Presets* -- Switchable command configurations (developer, writer, tutor, documenter, custom); each command carries a =:destination= hint (=chat= or =in-buffer=) so rewrites go to the right place automatically
- *Sessions & History* -- Save/load conversations, prompt history navigation
- *Thinking Block Folding* -- Reasoning model output (=<think>= tags, DeepSeek API) rendered as collapsible org headings; =C-c V= toggles all blocks
- *File Attachments* -- Attach files and images (vision models) to conversations
- *Model Management* -- Pull, delete, copy models; categorized recommendations; cloud model support
- *Multishot* -- Send the same prompt to multiple models simultaneously
- *Parameters* -- Full control over temperature, top_k, top_p, and all Ollama API options
- *Fabric Patterns* -- Integration with [[https://github.com/danielmiessler/fabric][Fabric]] prompt patterns
- *Token Tracking* -- Real-time token usage statistics with graphs
- *RAG* -- Index local documents and source code; retrieve relevant context via semantic search; inline =@rag(query)= syntax; configurable embedding backend (Ollama =/api/embed= or any OpenAI-compatible =/v1/embeddings= service, e.g. LM Studio)
- *In-Buffer Replace* -- Stream LLM output directly into the source buffer, replacing the selected region in-place; accept/reject confirmation; inline diff view (=C-c d=) with word-level smerge highlighting; automatic code-fence stripping
- *Lightweight* -- No external dependencies; a minified 200-line version (=ollama-buddy-mini=) is also available

* What's New (v3.0.0)

Generic OpenAI-compatible chat provider — connect ollama-buddy to *any* server that speaks the OpenAI chat-completions API without needing a dedicated module.

- *=ollama-buddy-openai-compat=* (new module, prefix =l:=) -- works out of the box with LM Studio (default =http://localhost:1234=), llama.cpp, vLLM, Jan, or any other local/remote OpenAI-API server. Key settings:
  - =ollama-buddy-openai-compat-base-url= -- server address
  - =ollama-buddy-openai-compat-provider-name= -- display name in the UI
  - =ollama-buddy-openai-compat-api-key= -- optional Bearer token (leave empty for local servers)
  - Models are auto-discovered via =GET /v1/models= on load; run =M-x ollama-buddy-openai-compat-refresh-models= to re-discover after switching models at runtime.
  - The =l= indicator appears in the status line when the module is active.

* Recent Changes

| Version | Summary                                                                                                                                                                                                                   |
|---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| *2.9.2* | =:destination= property for command definitions — per-command routing to =chat= or =in-buffer=, overriding the global toggle; all built-in commands and presets pre-annotated.                                            |
| *2.9.1* | Welcome screen tips — =ollama-buddy-tips.el= shows a random usage tip on every fresh chat buffer; disable with =(setq ollama-buddy-show-tips nil)=.                                                                       |
| *2.9*   | Configurable RAG embedding backend — =ollama-buddy-rag-embedding-base-url= and =ollama-buddy-rag-embedding-api-style= let you point RAG at any =/v1/embeddings= server (e.g. LM Studio) instead of Ollama's =/api/embed=. |
| *2.8.1* | In-buffer replace polish: inline diff view (=C-c d=, word-level smerge highlighting), automatic clean-output tone, code-fence stripping, =C-c W= toggle.                                                                  |
| *2.8.0* | Initial in-buffer replace mode — stream rewrites directly into the source buffer; =C-c C-c= accept / =C-c C-k= reject.                                                                                                    |

See [[file:CHANGELOG.org]] for full history.

* Installation

** MELPA Simple

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** With Remote Providers

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  ;; add other providers similarly …
  :config
  (require 'ollama-buddy-openai nil t)
  ;; other cloud providers …
  )
#+end_src

** With a Local OpenAI-Compatible Server (LM Studio, llama.cpp, vLLM…)

No API key needed for local servers:

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (require 'ollama-buddy-openai-compat)
  ;; point at your server — LM Studio default shown here
  (setq ollama-buddy-openai-compat-base-url "http://localhost:1234")
  (setq ollama-buddy-openai-compat-provider-name "LM Studio"))
#+end_src

Optionally also route RAG embeddings through the same server:

#+begin_src elisp
(setq ollama-buddy-rag-embedding-base-url "http://localhost:1234")
(setq ollama-buddy-rag-embedding-api-style 'openai)
(setq ollama-buddy-rag-embedding-model "nomic-embed-text")
#+end_src

** Manual

#+begin_src shell
git clone https://github.com/captainflasmr/ollama-buddy.git
#+end_src

#+begin_src elisp
(add-to-list 'load-path "path/to/ollama-buddy")
(require 'ollama-buddy)
(global-set-key (kbd "C-c o") #'ollama-buddy-role-transient-menu)
(global-set-key (kbd "C-c O") #'ollama-buddy-transient-menu-wrapper)
#+end_src

** Manual Setup with Presets and User Prompts

To fully utilize role-based presets and custom system prompts, you’ll need to copy two important directories into your local Emacs configuration:

1. *=ollama-buddy-presets=* – Contains pre-defined role presets (e.g., developer, writer, tutor) as =.el= files.
2. *=ollama-buddy-user-prompts=* – Where your *saved* system prompts are stored (created automatically when using =M-x ollama-buddy-user-prompts-save=).

*** Step-by-Step Setup

Run the following shell commands in your terminal:

#+begin_src bash
cd ~/.emacs.d

# extract out the ollama-buddy-presets and ollama-buddy-user-prompts
curl -sL https://api.github.com/repos/captainflasmr/ollama-buddy/tarball/main | \
    tar -xzf - --transform='s|[^/]*/||' \
        --wildcards \
        '*/ollama-buddy-presets/*' \
        '*/ollama-buddy-user-prompts/*'
#+end_src

* My current configuration

#+begin_src elisp
(use-package ollama-buddy
  :load-path "~/source/repos/ollama-buddy"
  :demand t
  :bind
  ;; ("C-c o" . ollama-buddy-menu)
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  
  ;; overall default model
  (setq ollama-buddy-default-model "glm-4.7:cloud")
  
  ;; key setup
  (setq ollama-buddy-openai-api-key
        (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (setq ollama-buddy-claude-api-key
        (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (setq ollama-buddy-gemini-api-key
        (auth-source-pick-first-password :host "ollama-buddy-gemini" :user "apikey"))
  (setq ollama-buddy-grok-api-key
        (auth-source-pick-first-password :host "ollama-buddy-grok" :user "apikey"))
  (setq ollama-buddy-codestral-api-key
        (auth-source-pick-first-password :host "ollama-buddy-codestral" :user "apikey"))
  (setq ollama-buddy-openrouter-api-key
        (auth-source-pick-first-password :host "ollama-buddy-openrouter" :user "apikey"))
  (setq ollama-buddy-deepseek-api-key
        (auth-source-pick-first-password :host "ollama-buddy-deepseek" :user "apikey"))
  (setq ollama-buddy-cloud-api-key
        (auth-source-pick-first-password :host "ollama-buddy-cloud" :user "apikey"))
  (setq ollama-buddy-web-search-api-key
        (auth-source-pick-first-password :host "ollama-buddy-web-search" :user "apikey"))

  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  (require 'ollama-buddy-gemini nil t)
  (require 'ollama-buddy-grok nil t)
  (require 'ollama-buddy-codestral nil t)
  (require 'ollama-buddy-copilot nil t)
  (require 'ollama-buddy-openrouter nil t)
  (require 'ollama-buddy-deepseek nil t)

  ;; other
  (setq ollama-buddy-full-welcome-enabled nil)

  (require 'ollama-buddy-openai-compat)
  ;; LM Studio default — this is the built-in default so no change needed
  (setq ollama-buddy-openai-compat-base-url "http://localhost:1234")
  (setq ollama-buddy-openai-compat-provider-name "LM Studio")
  ;; No API key needed for LM Studio
  (setq ollama-buddy-openai-compat-api-key "")
  
  ;; Use LM Studio's /v1/embeddings endpoint instead of Ollama's /api/embed
  (setq ollama-buddy-rag-embedding-base-url "http://localhost:1234")
  (setq ollama-buddy-rag-embedding-api-style 'openai)
  (setq ollama-buddy-rag-embedding-model "text-embedding-nomic-embed-text-v1.5")
  
  ;; setup default custom menu for preferred models
  (ollama-buddy-update-menu-entry 'refactor-code     :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'git-commit        :model "glm-4.7:cloud")
  (ollama-buddy-update-menu-entry 'describe-code     :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'dictionary-lookup :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'synonym           :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'proofread         :model "minimax-m2.1:cloud")
  ;; (ollama-buddy-update-menu-entry 'refactor-code     :model "tinyllama:1.1b")
  ;; (ollama-buddy-update-menu-entry 'git-commit        :model "tinyllama:1.1b")
  ;; (ollama-buddy-update-menu-entry 'describe-code     :model "tinyllama:1.1b")
  ;; (ollama-buddy-update-menu-entry 'dictionary-lookup :model "tinyllama:1.1b")
  ;; (ollama-buddy-update-menu-entry 'synonym           :model "tinyllama:1.1b")
  ;; (ollama-buddy-update-menu-entry 'proofread         :model "tinyllama:1.1b")

  ;; dired integration
  (with-eval-after-load 'dired
    (define-key dired-mode-map (kbd "C-c C-a") #'ollama-buddy-dired-attach-marked-files)))
#+end_src

* Key Bindings (Chat Buffer)

| Key       | Action                        |
|-----------+-------------------------------|
| C-c C-c   | Send prompt                   |
| C-c RET   | Send prompt (alt)             |
| C-c C-k   | Cancel request                |
| C-c m     | Change model                  |
| C-c M     | Manage models                 |
| C-c O     | Main transient menu           |
| M-p / M-n | Browse prompt history         |
| C-c a     | Attach file                   |
| C-c 0     | Clear attachments             |
| C-c r     | RAG transient menu            |
| C-c C-s   | Show system prompt            |
| C-c C-r   | Reset system prompt           |
| C-c ~     | Select response tone          |
| C-c v     | Set keep-alive duration       |
| C-c !     | Toggle airplane mode          |
| C-c e     | Switch backend (network/curl) |

* Screenshots & Demos

See the [[file:README-full.org][full README]] for all 30+ demos with descriptions, or browse the [[https://www.youtube.com/@OllamaBuddyforEmacs][YouTube channel]].

* ollama API Support Tables
** Core API Endpoints

| API Endpoint              | Method | Purpose                            | Supported | Notes                                          |
|---------------------------+--------+------------------------------------+-----------+------------------------------------------------|
| =/api/chat=               | POST   | Send chat messages to a model      | Yes       | Core feature with streaming, tools, vision     |
| =/api/generate=           | POST   | Generate text without chat context | No        | Not planned - chat covers all use cases        |
| =/api/tags=               | GET    | List available local models        | Yes       | Used for model management                      |
| =/api/show=               | POST   | Show model information             | Yes       | Model info display                             |
| =/api/pull=               | POST   | Pull a model from Ollama library   | Yes       | Async pull with progress                       |
| =/api/push=               | POST   | Push a model to the Ollama library | No        | Not planned                                    |
| =/api/delete=             | DELETE | Delete a model                     | Yes       | Model management                               |
| =/api/copy=               | POST   | Copy a model                       | Yes       | Model management                               |
| =/api/create=             | POST   | Create a model from Modelfile/GGUF | Yes       | GGUF import supported                          |
| =/api/ps=                 | GET    | List running models                | Yes       | Shows loaded models in management buffer       |
| =/api/embed=              | POST   | Generate embeddings from text      | Yes       | RAG module via =ollama-buddy-rag.el= (v2.5.0+) |
| =/api/version=            | GET    | Get Ollama version                 | Yes       | Displayed on Model Management page             |
| =HEAD /api/blobs/:digest= | HEAD   | Check if blob exists               | No        | Not needed for current functionality           |
| =POST /api/blobs/:digest= | POST   | Push a blob                        | No        | Not needed for current functionality           |
| Cancel requests           | Custom | Terminate ongoing request          | Yes       | =C-c C-k= in chat buffer                       |

** Chat API Parameters (=/api/chat=)

| Parameter    | Supported | Notes                                                            |
|--------------+-----------+------------------------------------------------------------------|
| =model=      | Yes       | Per-command or global via =ollama-buddy-default-model=           |
| =messages=   | Yes       | Full conversation history support                                |
| =tools=      | Yes       | Tool calling via =ollama-buddy-tools.el= (v2.0.0+)               |
| =think=      | Yes       | Reasoning model support with visibility toggle                   |
| =format=     | Partial   | JSON mode not yet exposed                                        |
| =options=    | Yes       | Full implementation of all model parameters                      |
| =stream=     | Yes       | Toggleable with =ollama-buddy-toggle-streaming=                  |
| =keep_alive= | Yes       | Set via =ollama-buddy-set-keepalive= or =ollama-buddy-keepalive= |

** Generate API Parameters (=/api/generate=)

| Parameter    | Supported | Notes                                                   |
|--------------+-----------+---------------------------------------------------------|
| =model=      | N/A       | Generate endpoint not used                              |
| =prompt=     | N/A       | Chat endpoint used instead                              |
| =suffix=     | N/A       | Could be added via =ollama-buddy-set-suffix=            |
| =images=     | Yes*      | Vision supported via chat endpoint                      |
| =format=     | No        | Not currently exposed                                   |
| =options=    | Yes       | Full parameter support                                  |
| =system=     | Yes       | =ollama-buddy-set-system-prompt=                        |
| =template=   | No        | Uses model defaults                                     |
| =stream=     | Yes       | Streaming enabled by default                            |
| =raw=        | No        | Not currently needed                                    |
| =keep_alive= | N/A       | Generate endpoint not used (chat endpoint used instead) |
| =context=    | N/A       | Deprecated - using messages array instead               |

** Model Options Parameters

| Parameter Group         | Parameters                                                                                     | Supported |
|-------------------------+------------------------------------------------------------------------------------------------+-----------|
| *Temperature Controls*  | =temperature=, =top_k=, =top_p=, =min_p=, =typical_p=                                          | Yes       |
| *Repetition Controls*   | =repeat_last_n=, =repeat_penalty=, =presence_penalty=, =frequency_penalty=                     | Yes       |
| *Advanced Sampling*     | =mirostat=, =mirostat_tau=, =mirostat_eta=, =penalize_newline=, =stop=                         | Yes       |
| *Resource Management*   | =num_keep=, =seed=, =num_predict=, =num_ctx=, =num_batch=                                      | Yes       |
| *Hardware Optimization* | =numa=, =num_gpu=, =main_gpu=, =low_vram=, =vocab_only=, =use_mmap=, =use_mlock=, =num_thread= | Yes       |

** Additional API Features

| Feature                     | Supported | Notes                                               |
|-----------------------------+-----------+-----------------------------------------------------|
| Streaming responses         | Yes       | Default mode, toggleable                            |
| Tool calling                | Yes       | Built-in tools + custom tool registration (v2.0.0+) |
| Vision/Images               | Yes       | Base64 image encoding for multimodal models         |
| Structured outputs (JSON)   | No        | Not yet exposed                                     |
| Reproducible outputs (seed) | Yes       | Via parameters                                      |
| Load/Unload models          | Yes       | Via model management buffer                         |
| Cloud models                | Yes       | Via =ollama-buddy-cloud-models= (v1.1+)             |
| Thinking/Reasoning models   | Yes       | =think= parameter with visibility toggle            |

* Roadmap

** Future Considerations

*** Structured Outputs (JSON Schema)

The =format= parameter supports JSON schema for structured responses.

*Implementation considerations:*
- Define output schemas per command
- Integration with org-mode tables
- Structured data extraction workflows

*** External LLMs to use tooling

In this case I might not ever do this, I am always mindful of burning through tokens.

*** Anthropic Messages API Compatibility

Ollama v0.14+ implements =/v1/messages= endpoint compatible with Anthropic's API format.

*Implementation considerations:*
- Option to route Claude-format requests through local Ollama
- Could simplify =ollama-buddy-claude.el= or provide fallback
- Support for tool_use and tool_result content blocks

*** Image Generation Support

Ollama v0.14+ supports local image generation with models like Z-Image Turbo (6B) and FLUX.2 Klein.

*Implementation considerations:*
- Detect image generation models
- Handle image output (save to file, display in buffer)
- Support generation parameters (width, height, steps, seed)
- Integration with org-mode inline images

* Documentation

- [[file:docs/ollama-buddy.org][Manual]] -- Full reference manual
- [[file:README-full.org][Full README]] -- Detailed README with all demos, tutorials, transient menu guide, and role setup
- [[file:README-features.org][Feature History]] -- Detailed feature descriptions as they were added
- [[file:CHANGELOG.org][Changelog]] -- Version-by-version changes
