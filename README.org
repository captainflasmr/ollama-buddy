#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+startup: showall

#+attr_org: :width 300px
#+attr_html: :width 50%
[[file:img/ollama-buddy-youtube-banner_001.jpg]]

* Ollama Buddy

An Emacs package for interacting with local LLMs via [[https://ollama.ai/][Ollama]], with support for remote providers (OpenAI, Claude, Gemini, Grok, Copilot, Codestral) and Ollama Cloud models. Requires Emacs 28.1+.

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

That's it. Start =ollama serve=, open Emacs, press =C-c o=, select =[o]= to open chat, and go.

https://www.youtube.com/@OllamaBuddyforEmacs

** Tool Calling Demo

LLMs can invoke Emacs functions: read/write files, execute shell commands, search buffers, and more.

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_034.gif]]

** Quick Demo

Submit queries, swap models, view token usage and statistics:

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_011.gif]]

* Features

- *Tool Calling* -- LLMs can invoke registered Emacs functions (file ops, shell commands, buffer search, calculations). Safe mode restricts to read-only tools.
- *Multiple Providers* -- Local Ollama, Ollama Cloud, OpenAI, Claude, Gemini, Grok, GitHub Copilot, Codestral
- *Web Search* -- Inline =@search(query)= syntax for real-time web search via Ollama's API
- *Interactive Menus* -- Transient popup menus with role-based command grouping
- *Roles & Presets* -- Switchable command configurations (developer, writer, tutor, documenter, custom)
- *Sessions & History* -- Save/load conversations, prompt history navigation
- *File Attachments* -- Attach files and images (vision models) to conversations
- *Model Management* -- Pull, delete, copy models; categorized recommendations; cloud model support
- *Multishot* -- Send the same prompt to multiple models simultaneously
- *Parameters* -- Full control over temperature, top_k, top_p, and all Ollama API options
- *Fabric Patterns* -- Integration with [[https://github.com/danielmiessler/fabric][Fabric]] prompt patterns
- *Token Tracking* -- Real-time token usage statistics with graphs
- *RAG* -- Index local documents and source code, retrieve relevant context for conversations via semantic search. Inline =@rag(query)= syntax for automatic context attachment.
- *Lightweight* -- No external dependencies; a minified 200-line version (=ollama-buddy-mini=) is also available

* What's New (2.5.2)

- *PDF Indexing* -- RAG can now index PDF files via =pdftotext= (from =poppler-utils=). Automatically detected; gracefully skipped if not installed.
- *Selection Status* -- Role transient menu shows current selection info (chars, lines) to confirm text is selected before commands
- *Session Naming* -- Improved session names via stop-word filtering: removes common words, keeps up to 5 key terms
- *Autosave Transcript* -- Chat buffer auto-saved to =~autosave.org= after each response; deleted on proper session save
- *Inline @rag()* -- Use =@rag(query)= in prompts to automatically search and attach RAG context, like =@search()= for web search

See [[file:CHANGELOG.org]] for full history.

* Installation

** MELPA

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** With Remote Providers

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  (require 'ollama-buddy-gemini nil t)
  (require 'ollama-buddy-copilot nil t))
#+end_src

** Manual

#+begin_src shell
git clone https://github.com/captainflasmr/ollama-buddy.git
#+end_src

#+begin_src elisp
(add-to-list 'load-path "path/to/ollama-buddy")
(require 'ollama-buddy)
(global-set-key (kbd "C-c o") #'ollama-buddy-role-transient-menu)
(global-set-key (kbd "C-c O") #'ollama-buddy-transient-menu-wrapper)
#+end_src

* Key Bindings (Chat Buffer)

| Key       | Action                        |
|-----------+-------------------------------|
| C-c C-c   | Send prompt                   |
| C-c RET   | Send prompt (alt)             |
| C-c C-k   | Cancel request                |
| C-c m     | Change model                  |
| C-u C-c m | Select cloud model            |
| C-c M     | Manage models                 |
| C-c O     | Main transient menu           |
| M-p / M-n | Browse prompt history         |
| C-c a     | Attach file                   |
| C-c 0     | Clear attachments             |
| C-c r     | RAG transient menu            |
| C-c C-s   | Show system prompt            |
| C-c C-r   | Reset system prompt           |
| C-c ~     | Select response tone          |
| C-c e     | Switch backend (network/curl) |

* Screenshots & Demos

See the [[file:README-full.org][full README]] for all 30+ demos with descriptions, or browse the [[https://www.youtube.com/@OllamaBuddyforEmacs][YouTube channel]].

* ollama API Support Tables
** Core API Endpoints

| API Endpoint              | Method | Purpose                            | Supported | Notes                                      |
|---------------------------+--------+------------------------------------+-----------+--------------------------------------------|
| =/api/chat=               | POST   | Send chat messages to a model      | Yes       | Core feature with streaming, tools, vision |
| =/api/generate=           | POST   | Generate text without chat context | No        | Not planned - chat covers all use cases    |
| =/api/tags=               | GET    | List available local models        | Yes       | Used for model management                  |
| =/api/show=               | POST   | Show model information             | Yes       | Model info display                         |
| =/api/pull=               | POST   | Pull a model from Ollama library   | Yes       | Async pull with progress                   |
| =/api/push=               | POST   | Push a model to the Ollama library | No        | Not planned                                |
| =/api/delete=             | DELETE | Delete a model                     | Yes       | Model management                           |
| =/api/copy=               | POST   | Copy a model                       | Yes       | Model management                           |
| =/api/create=             | POST   | Create a model from Modelfile/GGUF | Yes       | GGUF import supported                      |
| =/api/ps=                 | GET    | List running models                | Yes       | Shows loaded models in management buffer   |
| =/api/embed=              | POST   | Generate embeddings from text      | Yes       | RAG module via =ollama-buddy-rag.el= (v2.5.0+) |
| =/api/version=            | GET    | Get Ollama version                 | No        | Not currently needed                       |
| =HEAD /api/blobs/:digest= | HEAD   | Check if blob exists               | No        | Not needed for current functionality       |
| =POST /api/blobs/:digest= | POST   | Push a blob                        | No        | Not needed for current functionality       |
| Cancel requests           | Custom | Terminate ongoing request          | Yes       | =C-c C-k= in chat buffer                   |

** Chat API Parameters (=/api/chat=)

| Parameter    | Supported | Notes                                                    |
|--------------+-----------+----------------------------------------------------------|
| =model=      | Yes       | Per-command or global via =ollama-buddy-default-model=   |
| =messages=   | Yes       | Full conversation history support                        |
| =tools=      | Yes       | Tool calling via =ollama-buddy-tools.el= (v2.0.0+)       |
| =think=      | Yes       | Reasoning model support with visibility toggle           |
| =format=     | Partial   | JSON mode not yet exposed                                |
| =options=    | Yes       | Full implementation of all model parameters              |
| =stream=     | Yes       | Toggleable with =ollama-buddy-toggle-streaming=          |
| =keep_alive= | No        | Uses Ollama default (5m)                                 |

** Generate API Parameters (=/api/generate=)

| Parameter    | Supported | Notes                                                    |
|--------------+-----------+----------------------------------------------------------|
| =model=      | N/A       | Generate endpoint not used                               |
| =prompt=     | N/A       | Chat endpoint used instead                               |
| =suffix=     | N/A       | Could be added via =ollama-buddy-set-suffix=             |
| =images=     | Yes*      | Vision supported via chat endpoint                       |
| =format=     | No        | Not currently exposed                                    |
| =options=    | Yes       | Full parameter support                                   |
| =system=     | Yes       | =ollama-buddy-set-system-prompt=                         |
| =template=   | No        | Uses model defaults                                      |
| =stream=     | Yes       | Streaming enabled by default                             |
| =raw=        | No        | Not currently needed                                     |
| =keep_alive= | No        | Uses Ollama default                                      |
| =context=    | N/A       | Deprecated - using messages array instead                |

** Model Options Parameters

| Parameter Group         | Parameters                                                                                     | Supported |
|-------------------------+------------------------------------------------------------------------------------------------+-----------|
| *Temperature Controls*  | =temperature=, =top_k=, =top_p=, =min_p=, =typical_p=                                          | Yes       |
| *Repetition Controls*   | =repeat_last_n=, =repeat_penalty=, =presence_penalty=, =frequency_penalty=                     | Yes       |
| *Advanced Sampling*     | =mirostat=, =mirostat_tau=, =mirostat_eta=, =penalize_newline=, =stop=                         | Yes       |
| *Resource Management*   | =num_keep=, =seed=, =num_predict=, =num_ctx=, =num_batch=                                      | Yes       |
| *Hardware Optimization* | =numa=, =num_gpu=, =main_gpu=, =low_vram=, =vocab_only=, =use_mmap=, =use_mlock=, =num_thread= | Yes       |

** Additional API Features

| Feature                      | Supported | Notes                                                  |
|------------------------------+-----------+--------------------------------------------------------|
| Streaming responses          | Yes       | Default mode, toggleable                               |
| Tool calling                 | Yes       | Built-in tools + custom tool registration (v2.0.0+)    |
| Vision/Images                | Yes       | Base64 image encoding for multimodal models            |
| Structured outputs (JSON)    | No        | Not yet exposed                                        |
| Reproducible outputs (seed)  | Yes       | Via parameters                                         |
| Load/Unload models           | Yes       | Via model management buffer                            |
| Cloud models                 | Yes       | Via =ollama-buddy-cloud-models= (v1.1+)                |
| Thinking/Reasoning models    | Yes       | =think= parameter with visibility toggle               |

* Roadmap

** In Progress

*** GitHub Copilot Integration Improvements

=ollama-buddy-copilot.el= provides Copilot Chat support:
- OAuth device flow authentication (no API key)
- Supports multiple backend models (GPT-4o, Claude, Gemini via Copilot)
- =p:= prefix for Copilot models

*Still TODO:*
- Better token refresh handling
- Copilot-specific features (code suggestions, etc.)

** Future Considerations

*** Image Generation Support

Ollama v0.14+ supports local image generation with models like Z-Image Turbo (6B) and FLUX.2 Klein.

*Implementation considerations:*
- Detect image generation models
- Handle image output (save to file, display in buffer)
- Support generation parameters (width, height, steps, seed)
- Integration with org-mode inline images

*** Anthropic Messages API Compatibility

Ollama v0.14+ implements =/v1/messages= endpoint compatible with Anthropic's API format.

*Implementation considerations:*
- Option to route Claude-format requests through local Ollama
- Could simplify =ollama-buddy-claude.el= or provide fallback
- Support for tool_use and tool_result content blocks

*** @@html:<s>@@Embeddings API@@html:</s>@@ (Done -- v2.5.0)

Implemented in =ollama-buddy-rag.el=. See the RAG feature above.

*** Keep-Alive Parameter

Control how long models stay loaded in memory.

*Implementation considerations:*
- =ollama-buddy-param-keep-alive= customization
- Presets for different usage patterns
- Display loaded models and their timeout status in model management

*** Structured Outputs (JSON Schema)

The =format= parameter supports JSON schema for structured responses.

*Implementation considerations:*
- Define output schemas per command
- Integration with org-mode tables
- Structured data extraction workflows

* Documentation

- [[file:docs/ollama-buddy.org][Manual]] -- Full reference manual
- [[file:README-full.org][Full README]] -- Detailed README with all demos, tutorials, transient menu guide, and role setup
- [[file:README-features.org][Feature History]] -- Detailed feature descriptions as they were added
- [[file:CHANGELOG.org][Changelog]] -- Version-by-version changes
