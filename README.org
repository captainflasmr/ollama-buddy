#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+todo: TODO DOING | DONE
#+startup: showall

#+attr_org: :width 300px
#+attr_html: :width 50%
[[file:img/ollama-buddy-youtube-banner_001.jpg]]

* Welcome to OLLAMA BUDDY

Want to quickly access local running AI through Emacs and Ollama? This package provides a friendly Emacs interface for interacting with Ollama models and offers a convenient way to integrate to Ollama's local LLM capabilities directly into your Emacs workflow with little or no configuration required!

/"The name is just something a little bit fun and it seems to always remind me of the "bathroom buddy" from the film Gremlins (although hopefully this will work better than that seemed to!)"/

I have a youtube channel where where I am looking to regularly post videos showcasing the capabilities of the package. Check it out here:

https://www.youtube.com/@OllamaBuddyforEmacs

See [[file:docs/ollama-buddy.org]] for the manual!

You can install with literally no config, but I would suggest binding to both menus.

  #+begin_src elisp
   (use-package ollama-buddy
     :bind
     ("C-c o" . ollama-buddy-menu)
     ("C-c O" . ollama-buddy-transient-menu-wrapper))
  #+end_src

A simple text information screen will be presented on the first opening of the chat (by selecting "o" from any of the menus), its just a bit of fun, but I wanted a quick start tutorial/assistant type of feel, here is what will be presented:

#+begin_src
,#+TITLE: Ollama Buddy Chat

,* Welcome to OLLAMA BUDDY

,#+begin_example
 ___ _ _      n _ n      ___       _   _ _ _
|   | | |__._|o(Y)o|__._| . |_ _ _| |_| | | |
| | | | | .  | 1.1 | .  | . | | | . | . |__ |
|___|_|_|__/_|_|_|_|__/_|___|___|___|___|___|
,#+end_example

- Ask me anything!       C-c C-c
- Main transient menu    C-c O
- Detailed info          C-c A

,* *tinyllama:latest >> PROMPT:* <put prompt in here>
#+end_src

Press =C-c A= to open the =*Ollama Buddy Info*= buffer which shows:
- All installed models with Select/Info/Pull/Copy/Delete actions
- Models available to pull
- Complete commands reference

You will, of course, need to have =ollama serve= running, but technically, that is all you need! Emacs and =ollama-buddy= will handle the rest!

The example above has some models already pulled, but if you don't have any initially, the Recommended Models section will be fully populated with suggestions. Just click, or move your point over, and press return, and ollama-buddy via Ollama will pull that model for you asynchronously!

* Quick Demo Video

** 011 Real-time tracking and display

Submit some queries, with different models:

PROMPT>> why is the sky blue?
PROMPT>> why is the grass green?
PROMPT>> why is emacs so great?

and show the token usage and rate being displayed in the chat buffer.

Open the Token Usage stats from the menu

Open the Token Usage graph

Toggle on =ollama-buddy-toggle-token-display=

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_011.gif]]

See *Screenshots / Demos* section for more videos/demonstration.

* Whats New

See full list in :

[[file:CHANGELOG.org]]

** <2026-01-30 Thu> *1.1.1*

Simplified chat interface

- Default welcome screen now shows only essential commands: Ask me anything (C-c C-c), Main transient menu (C-c O), and Detailed info (C-c A)
- Full interface info (model list with actions, all commands reference) now opens in a separate =*Ollama Buddy Info*= buffer via C-c A
- Cleaner, less cluttered initial experience while keeping all functionality accessible

** <2026-01-28 Tue> *1.1*

Added Ollama Cloud Models support

- Cloud models (running on ollama.com infrastructure) now work seamlessly
- =ollama-buddy-cloud-signin= to automatically open browser for authentication
- Cloud models are proxied through the local Ollama server which handles authentication
- Use =C-u C-c m= or transient menu "Model > Cloud" to select cloud models
- Status line shows ☁ indicator when using a cloud model
- Available cloud models include: qwen3-coder:480b-cloud, deepseek-v3.1:671b-cloud, gpt-oss:120b-cloud, minimax-m2.1:cloud, and more

To use cloud models:
1. Sign in once with =M-x ollama-buddy-cloud-signin= (opens browser automatically)
2. Select a cloud model with =C-u C-c m=
3. Use normally - the local Ollama server handles cloud authentication

#+begin_src elisp
;; Cloud models work with no additional configuration
;; Just sign in once via the transient menu: Cloud Auth > Sign In
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** <2025-12-11 Thu> *1.0.2*

Added Mistral Codestral support

- Added ollama-buddy-codestral.el for Mistral Codestral API support
- Implemented model handler registration for seamless switching between local Ollama and cloud-based Codestral models
- Supports async streaming responses with real-time token counting
- Follows the same integration pattern as other remote AI providers (OpenAI, Claude, Gemini, Grok)
- Uses "s:" prefix to identify Codestral models in the model list

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-codestral-api-key
   (auth-source-pick-first-password :host "ollama-buddy-codestral" :user "apikey"))
  :config
  (require 'ollama-buddy-codestral nil t))
#+end_src

* Features

See [[file:README-features.org]] for a deeper delve into each feature as it was added.

- *Minimal Setup*
  
If desired, the following will get you going! (of course, have =ollama= running with some models loaded).
    
  #+begin_src elisp
   (use-package ollama-buddy
     :ensure t
     :bind
     ("C-c o" . ollama-buddy-menu)
     ("C-c O" . ollama-buddy-transient-menu-wrapper))
  #+end_src

- *Interactive Command Menu*
  
  - Quick-access menu with single-key commands (=M-x ollama-buddy-menu=)
  - Quickly define your own menu using =defcustom= with dynamic adaptable menu
  - Menu Presets easily definable in text files
  - Quickly switch between LLM models with no configuration required
  - Send text from any Emacs buffer
  - Each model is uniquely colored to enhance visual feedback
  - Switch between basic and advanced interface levels (=C-c A=)

- *Role and Session Management*

  - Create, switch, save and manage role-specific command menu configurations
  - Create, switch, save and manage sessions to provide chat/model based context
  - Prompt history/context with the ability to clear, turn on and off and save as part of a session file
  - Edit conversation history with intuitive keybindings

- *Smart Model Management*

  - Models can be assigned to individual menu commands
  - Intelligent model fallback
  - Real-time model availability monitoring
  - Easy model switching during sessions
  - Ollama Cloud models support (qwen3-coder:480b, deepseek-v3.1:671b, gpt-oss, minimax-m2.1, etc.)

- *Parameter Control*

  - Comprehensive parameter management for all Ollama API options
  - Temperature control for adjusting AI creativity
  - Command-specific parameter customization for optimized interactions
  - Visual parameter interface showing active and modified values

- *System Prompt Support*

  - Set persistent system prompts for guiding AI responses
  - Command-specific system prompts for tailored interactions
  - Visual indicators for active system prompts

- *AI Operations*
  
  - Code refactoring with context awareness
  - Automatic git commit message generation
  - Code explanation and documentation
  - Text operations (proofreading, conciseness, dictionary lookups)
  - Custom prompt support for flexibility

- *Conversation Tools*

  - Conversation history tracking and navigation
  - Real-time token usage tracking and statistics
  - Multi-model comparison (send prompts to multiple models)
  - Markdown to Org conversion for better formatting
  - Export conversations in various formats

- *Lightweight*
  
  - Single package file
  - A minified version =ollama-buddy-mini= (200 lines) is available
  - No external dependencies (curl not used)

* Transient Menu

Ollama Buddy now includes a transient-based menu system to improve usability and streamline interactions. Yes, I originally stated that I would never do it, but I think it compliments my crafted simple textual menu and the fact that I have now defaulted the main chat interface to a simple menu.

This can give the user more options for configuration, they can use the chat in advanced mode where the keybindings are presented in situ, or a more minimal basic setup where the transient menu can be activated.  For my use-package definition I current have the following set up, with the two styles of menus sitting alongside each other :

  #+begin_src elisp
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu)
  #+end_src

The new menu provides an organized interface for accessing the assistant’s core functions, including chat, model management, roles, and Fabric patterns. This post provides an overview of the features available in the Ollama Buddy transient menus.

Yes that's right also =fabric= patterns!, I have decided to add in auto syncing of the patterns directory in https://github.com/danielmiessler/fabric

Simply I pull the patterns directory which contain prompt guidance for a range of different topics and then push them through a completing read to set the =ollama-buddy= system prompt, so a special set of curated prompts can now be applied right in the =ollama-buddy= chat!

Anyways, here is a description of the transient menu system.

** What is the Transient Menu?

The transient menu in Ollama Buddy leverages Emacs' =transient.el= package (the same technology behind Magit's popular interface) to create a hierarchical, discoverable menu system. This approach transforms the user experience from memorizing numerous keybindings to navigating through logical groups of commands with clear descriptions.

** Accessing the Menu

The main transient menu can be accessed with the keybinding =C-c O= when in an Ollama Buddy chat buffer. You can also call it via =M-x ollama-buddy-transient-menu= from anywhere in Emacs.

** What the Menu Looks Like

When called, the main transient menu appears at the bottom of your Emacs frame, organized into logical sections with descriptive prefixes. Here's what you'll see:

#+begin_src 
|o(Y)o| Ollama Buddy
[Chat]             [Prompts]            [Model]               [Roles & Patterns]
o  Open Chat       l  Send Region       W  Manage Models      R  Switch Roles
O  Commands        s  Set System Prompt m  Switch Model       E  Create New Role
RET Send Prompt    C-s Show System      v  View Model Status  D  Open Roles Directory
h  Help/Menu       r  Reset System      i  Show Model Info    f  Fabric Patterns
k  Kill/Cancel     b  Ollama Buddy Menu M  Multishot          t  OpenAI Integration
x  Toggle Streaming

[Display Options]          [History]              [Sessions]             [Parameters]
A  Toggle Interface Level  H  Toggle History      N  New Session         P  Edit Parameter
B  Toggle Debug Mode       X  Clear History       L  Load Session        G  Display Parameters
T  Toggle Token Display    J  Edit History        S  Save Session        I  Parameter Help
u  Token Stats                                    Q  List Sessions       K  Reset Parameters
U  Token Usage Graph                              Z  Delete Session      F  Toggle Params in Header
C-o Toggle Markdown->Org                                                 p  Parameter Profiles
c  Toggle Model Colors
V  Toggle reasoning visibility                                           
#+end_src

This visual layout makes it easy to discover and access the full range of Ollama Buddy's functionality. Let's explore each section in detail.

** Menu Sections Explained

*** Chat Section

This section contains the core interaction commands:

- *Open Chat (o)*: Opens the Ollama Buddy chat buffer
- *Commands (O)*: Opens a submenu with specialized commands
- *Send Prompt (RET)*: Sends the current prompt to the model
- *Help/Menu (h)*: Displays the help assistant with usage tips
- *Kill/Cancel Request (k)*: Cancels the current ongoing request
- *Toggle Streaming (x)*: Toggle streaming on and off only for ollama models

*** Prompts Section

These commands help you manage and send prompts:

- *Send Region (l)*: Sends the selected region as a prompt
- *Set System Prompt (s)*: Sets the current prompt as a system prompt
- *Show System Prompt (C-s)*: Displays the current system prompt
- *Reset System Prompt (r)*: Resets the system prompt to default
- *Ollama Buddy Menu (b)*: Opens the classic custom menu interface

*** Model Section

Commands for model management:

- *Manage Models*: Info, Pull, Copy, Delete for each model, plus Import GGUF Model and Pull any model from ollama library
- *Switch Model (m)*: Changes the active LLM
- *View Model Status (v)*: Shows status of all available models
- *Show Model Info (i)*: Displays detailed information about the current model
- *Multishot (M)*: Sends the same prompt to multiple models

*** Roles & Patterns Section

These commands help manage roles and use fabric patterns:

- *Switch Roles (R)*: Switch to a different predefined role
- *Create New Role (E)*: Create a new role interactively
- *Open Roles Directory (D)*: Open the directory containing role definitions
- *Fabric Patterns (f)*: Opens the submenu for Fabric patterns
- *OpenAI Integration (t)*: Opens menu specifically for OpenAI/ChatGPT

When you select the Fabric Patterns option, you'll see a submenu like this:

#+begin_src 
Fabric Patterns (42 available, last synced: 2025-03-18 14:30)
[Actions]             [Sync]              [Categories]          [Navigation]
s  Send with Pattern  S  Sync Latest      u  Universal Patterns q  Back to Main Menu
p  Set as System      P  Populate Cache   c  Code Patterns
l  List All Patterns  I  Initial Setup    w  Writing Patterns
v  View Pattern Details                   a  Analysis Patterns
#+end_src

*** Display Options Section

Commands to customize the display:

- *Toggle Interface Level (A)*: Switch between basic and advanced interfaces
- *Toggle Debug Mode (B)*: Enable/disable JSON debug information
- *Toggle Token Display (T)*: Show/hide token usage statistics
- *Display Token Stats (U)*: Show detailed token usage information
- *Toggle Markdown->Org (C-o)*: Enable/disable conversion to Org format
- *Toggle Model Colors (c)*: Enable/disable model-specific colors
- *Toggle reasoning visibility (V)*: Hide/show reasoning/thinking sections
- *Token Usage Graph (g)*: Display a visual graph of token usage

*** History Section

Commands for managing conversation history:

- *Toggle History (H)*: Enable/disable conversation history
- *Clear History (X)*: Clear the current history
- *Edit History (J)*: Edit the history in a buffer

*** Sessions Section

Commands for session management:

- *New Session (N)*: Start a new session
- *Load Session (L)*: Load a saved session
- *Save Session (S)*: Save the current session
- *List Sessions (Q)*: List all available sessions
- *Delete Session (Z)*: Delete a saved session

*** Parameters Section

Commands for managing model parameters:

- *Edit Parameter (P)*: Opens a submenu to edit specific parameters
- *Display Parameters (G)*: Show current parameter settings
- *Parameter Help (I)*: Display help information about parameters
- *Reset Parameters (K)*: Reset parameters to defaults
- *Toggle Params in Header (F)*: Show/hide parameters in header
- *Parameter Profiles (p)*: Opens the parameter profiles submenu

When you select the Edit Parameter option, you'll see a comprehensive submenu of all available parameters:

#+begin_src 
Parameters
[Generation]                [More Generation]          [Mirostat]
t  Temperature              f  Frequency Penalty       M  Mirostat Mode
k  Top K                    s  Presence Penalty        T  Mirostat Tau
p  Top P                    n  Repeat Last N           E  Mirostat Eta
m  Min P                    x  Stop Sequences
y  Typical P                l  Penalize Newline
r  Repeat Penalty

[Resource]                  [More Resource]            [Memory]
c  Num Ctx                  P  Num Predict             m  Use MMAP
b  Num Batch                S  Seed                    L  Use MLOCK
g  Num GPU                  N  NUMA                    C  Num Thread
G  Main GPU                 V  Low VRAM
K  Num Keep                 o  Vocab Only

[Profiles]                  [Actions]
d  Default Profile          D  Display All
a  Creative Profile         R  Reset All
e  Precise Profile          H  Help
A  All Profiles             F  Toggle Display in Header
                            q  Back to Main Menu
#+end_src

** Parameter Profiles

Ollama Buddy includes predefined parameter profiles that can be applied with a single command. When you select "Parameter Profiles" from the main menu, you'll see:

#+begin_src 
Parameter Profiles
Current modified parameters: temperature, top_k, top_p
[Available Profiles]
d  Default
c  Creative
p  Precise

[Actions]
q  Back to Main Menu
#+end_src

** Commands Submenu

The Commands submenu provides quick access to specialized operations:

#+begin_src 
Ollama Buddy Commands
[Code Operations]       [Language Operations]    [Pattern-based]         [Custom]
r  Refactor Code        l  Dictionary Lookup     f  Fabric Patterns      C  Custom Prompt
d  Describe Code        s  Synonym Lookup        u  Universal Patterns   m  Minibuffer Prompt
g  Git Commit Message   p  Proofread Text        c  Code Patterns

[Actions]
q  Back to Main Menu
#+end_src

** Direct Keybindings

For experienced users who prefer direct keybindings, all transient menu functions can also be accessed through keybindings with the prefix of your choice (or =C-c O= when in the chat minibuffer) followed by the key shown in the menu. For example:

- =C-c O s= - Set system prompt
- =C-c O m= - Switch model
- =C-c O P= - Open parameter menu

** Customization

The transient menu can be customized by modifying the =transient-define-prefix= definitions in the package. You can add, remove, or rearrange commands to suit your workflow.

* Screenshots / Demos

Note that all the demos are in real time.

** Ollama Buddy - youtube #emacs #ollama

Also these videos will all be uploaded to a youtube channel:

https://www.youtube.com/@OllamaBuddyforEmacs

*** description

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

*** Display setup

 (setq font-general "Source Code Pro 15")

 theme : doom-oceanic-next

** 001 Ollama Buddy - First Steps #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

- Starting with model : llama3.2:1b
- Show menu activation C-c o =ollama-buddy-menu=
- [o] Open chat buffer
- PROMPT:: why is the sky blue?
- C-c C-c to send from chat buffer
- From this demo README file, select the following text and send to chat buffer:
  What is the capital of France?

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_001.gif]]

** 002 Ollama Buddy - Swap Models #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

- C-c m to swap to differing models from chat buffer
- Select models from the intro message
- Swap models from the transient menu
- PROMPT:: how many fingers do I have?

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_002.gif]]

** 003 Ollama Buddy - From Other Buffers #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

the quick brown fox jumps over the lazy dog
- Select individual words for dictionary menu items
- Select whole sentence and convert to uppercase

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_003.gif]]

** 004 Coding - Ollama Buddy - Writing a Hello World! program with increasingly advanced models #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

- PROMPT:: can you write a hello world program in Ada?
- switch models to the following and check differing output:
  - tinyllama:latest
  - qwen2.5-coder:3b
  - qwen2.5-coder:7b

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_004.gif]]

** 005 006 Ollama Buddy - Roald Dahl to Buffy! using a custom menu - just for fun! #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

In fairy-tales, witches always wear silly black hats and black coats, and they ride on broomsticks. But this is not a fairy-tale. This is about REAL WITCHES.

Lets change roles into a Buffy preset and have some fun!

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

- *Cordelia burn*

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_005.gif]]

- *Giles... yawn!*

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_006.gif]]

** 007 Ollama Buddy - Multishot #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

- PROMPT:: how many fingers do I have?
- send to multiple models, any difference in output?
- Also they are now available in the equivalent letter named registers, so lets pull those registers!

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_007.gif]]  

** 008 Ollama Buddy - Role Switching #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Show the changing of roles and how they affect the menu and hence the commands available.

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_008.gif]]  

** 009 Ollama Buddy - Prompt History #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Enter a few queries to test the system, then navigate back through your previous inputs, switch models, and resubmit to compare results.

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_009.gif]]  

** 012 Ollama Buddy - Sessions/History and recall #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

- Ask:
What is the capital of France?
and of Italy?
- Turn off history
and of Germany?
- Turn on history
and of Germany?
- Save Session
- Restart Emacs
- Load Session
and of Sweden?

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_012.gif]]

** 015 Ollama Buddy - System prompt support #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Set the system message to:
You must always respond in a single sentence.
Now ask the following:
Tell me why Emacs is so great!
Tell me about black holes
clear the system message and ask again, the responses should now be more verbose!!

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_015.gif]]

** 016 Ollama Buddy - Same prompt to 10 models (multishot) #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

PROMPT: What is the capital of France?
Multishot to abcdefghij

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_016.gif]]

** 017 Ollama Buddy - Lets look at some usage statistics #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Display Token Stats
Display Token Usage Graph

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_017.gif]]

** 018 Ollama Buddy - Awesome ChatGPT Prompting Pt1 #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Select a passage from 20,000 and push

as a poet
as a gaslighter
as a drunk person

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_018.gif]]

** 019 Ollama Buddy - First Steps into User System Prompt Management #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

switch to use qwen2.5-coder:3b

Select User System Prompts

List, and move down through the buffer

Set as system prompt/load

Select Elisp Debugging Guide

Show the system prompt in a buffer

Run checking overy my/rsync in Emacs-DIYer

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_019.gif]]

** 020 Ollama Buddy - Attaching files to the chat buffer #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

switch to use qwen2.5-coder:3b

Attach the ollama-buddy Makefile

can you tell me about this file?

Attach another file, curl-tests.sh

Show attachments

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_020.gif]]

** 021 Ollama Buddy - Using a vision model to extract some text #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

switch to use gemma3:4b

Open the image with the text

Copy the link, paste into the chat buffer and then lets see if the text is correctly extracted!

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_021.gif]]

** 023 Ollama Buddy - Accessing cloud models! #emacs #ollama

Demonstrating the Emacs package ollama-buddy, providing a convenient way to integrate Ollama's local LLM capabilities.

Note: that the user has to be already signed in to ollama.com so the authentication is set up through ollama running locally

switch to *minimax-m2.1:cloud using the universal argument

type in some text

lets see a response!

switch to gpt-oss:120b-cloud using the transient menu

type in some text

lets see a response!

https://melpa.org/#/ollama-buddy
https://github.com/captainflasmr/ollama-buddy

#emacs #ollama

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_023.gif]]

* Installation

** Prerequisites

- [[https://ollama.ai/][Ollama]] installed and running locally
- Emacs 26.1 or later

** MELPA

#+begin_src emacs-lisp
   (use-package ollama-buddy
     :ensure t
     :bind
     ("C-c o" . ollama-buddy-menu)
     ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** Manual Installation

Clone this repository:

#+begin_src shell
git clone https://github.com/captainflasmr/ollama-buddy.git
#+end_src

*** init.el

With the option to add your own user keybinding for the =ollama-buddy-menu=

#+begin_src emacs-lisp
(add-to-list 'load-path "path/to/ollama-buddy")
(require 'ollama-buddy)
(global-set-key (kbd "C-c o") #'ollama-buddy-menu)
(global-set-key (kbd "C-c O") #'ollama-buddy-transient-menu-wrapper)
#+end_src

OR

#+begin_src elisp
 (use-package ollama-buddy
   :load-path "path/to/ollama-buddy"
   :bind
   ("C-c o" . ollama-buddy-menu)
   ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

- Usage

1. Start your Ollama server locally with =ollama serve=
2. In Emacs, =M-x ollama-buddy-menu= or a user defined keybinding =C-c o= to open up the ollama buddy menu
3. Select [o] to open and jump to the chat buffer
4. Read the AI assistants greeting and off you go!

** Key Bindings in Chat Buffer

| Key             | Action                                  |
|-----------------+-----------------------------------------|
| C-c C-c         | Send prompt                             |
| C-c k           | Cancel request                          |
| C-c m           | Change model                            |
| C-c h           | Show help                               |
| C-c i           | Show model info                         |
| C-c U           | Show token statistics                   |
| C-c U           | Show token graph                        |
| C-c V           | Toggle reasoning visibility             |
| C-c C-s         | Show system prompt                      |
| C-u C-c C-c     | Set system prompt                       |
| C-u C-u C-c C-c | Clear system prompt                     |
| M-p / M-n       | Browse prompt history                   |
| C-c n / C-c p   | Navigate between prompts                |
| C-c N           | New session                             |
| C-c L           | Load session                            |
| C-c S           | Save session                            |
| C-c Y           | List sessions                           |
| C-c W           | Delete session                          |
| C-c H           | Toggle history                          |
| C-c X           | Clear history                           |
| C-c E           | Edit history                            |
| C-c l           | Prompt to multiple models               |
| C-c P           | Edit parameters                         |
| C-c G           | Show parameters                         |
| C-c I           | Show parameter help                     |
| C-c K           | Reset parameters                        |
| C-c D           | Toggle JSON debug mode                  |
| C-c T           | Toggle token display                    |
| C-c Z           | Toggle parameters                       |
| C-c C-o         | Toggle Markdown/Org format              |
| C-c A           | Toggle interface level (basic/advanced) |

** Default Menu Items

The default menu offers the following menu items:

| Key | Action                      | Description                 |
|-----+-----------------------------+-----------------------------|
| o   | Open chat buffer            | Open chat buffer            |
| m   | Swap model                  | Swap model                  |
| v   | View model status           | View model status           |
| l   | Send region                 | Send region                 |
| R   | Switch roles                | Switch roles                |
| N   | Create new role             | Create new role             |
| D   | Open roles directory        | Open roles directory        |
| r   | Refactor code               | Refactor code               |
| g   | Git commit message          | Git commit message          |
| c   | Describe code               | Describe code               |
| d   | Dictionary Lookup           | Dictionary Lookup           |
| n   | Word synonym                | Word synonym                |
| p   | Proofread text              | Proofread text              |
| e   | Custom prompt               | Custom prompt               |
| i   | Minibuffer Prompt           | Minibuffer Prompt           |
| K   | Delete session              | Delete session              |
| q   | Quit                        | Quit                        |

* Tutorial: Quickly Setting Up a Specialized Menu

Lets quickly get started by adding that little command you frequently use on text, lets add it to the ollama buddy menu!

First, lets clarify some concepts.

** Understanding Commands, Roles, and Prompts in ollama-buddy

*** Commands

In ollama-buddy, a "command" is essentially a predefined action that appears as an option in the menu. Each command has:

- A key (the button you press to invoke it)
- A description (what shows up in the menu)
- An action (what happens when you select it)
- Optional settings like prompts, model selection, etc.

*** Roles

A "role" is a collection of commands. Think of it as a preset configuration or profile that defines which commands are available in your menu and how they behave.

*** The Different Types of Prompts

There are two types of prompts that might be causing confusion:

1. *Prompt* (=:prompt= property): This is the instruction or question that gets prepended to your selected text. For example, if your prompt is "Fill in the missing Turkish letters in:" and your selected text is "Merhaba nasilsin", the LLM will receive "Fill in the missing Turkish letters in: Merhaba nasilsin".

2. *System Prompt* (=:system= property): This is a behind-the-scenes instruction that guides the LLM's overall behavior but isn't directly part of your query. It's like telling the AI "Here's how I want you to approach this task" before you give it the actual task.

** Creating a Simple Turkish Letter Correction Command

Let's set up a simple, consistent command for filling in missing Turkish letters, there are two main ways.

The first will add a menu item to the existing menu, and the second will define a new role, which will be a collection of menu items. In the example, I have added only a single menu item, but you can add as many as you like. If you want, you can set up an entire menu system in the Role file that could be just Turkey specific.

*** Option 1: Adding to existing commands:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (setq ollama-buddy-default-model "o:tinyllama:latest")
  (add-to-list 'ollama-buddy-command-definitions
             '(turkish-letters
               :key ?t ; Press 't' in the menu to select this
               :description "Fix Turkish Letters" ; What shows in the menu
               :model "o:deepseek-r1:7b" ; Choose an appropriate model
               :prompt "Fill in the missing Turkish letters in the following text:"
               :system "You are an expert in Turkish language. Your task is to correctly add any missing Turkish-specific letters (ç, ğ, ı, ö, ş, ü) to text that may be missing them. Only correct the spelling by adding proper Turkish letters - do not change any words or add any commentary. Return only the corrected text."
               :action (lambda () (ollama-buddy--send-with-command 'turkish-letters)))))
#+end_src

1. Add the code block above to your Emacs configuration (tweaking the use-package command as desired for your local configuration and setting the :model you wish to use) 
2. Restart Emacs or evaluate the code
3. Select some Turkish text in any buffer
4. Press =C-c o= to open the ollama-buddy menu
5. Press =t= to run your "Fix Turkish Letters" command

*** Option 2: Creating a dedicated role:

1. Create the file =~/.emacs.d/ollama-buddy-presets/ollama-buddy--preset__turkish.el= with the code block below:

#+begin_src elisp
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands (always include these)
    (open-chat
     :key ?o
     :description "Open chat buffer"
     :action ollama-buddy--open-chat)
    
    (show-models
     :key ?v
     :description "View model status"
     :action ollama-buddy-show-model-status)
    
    (switch-role
     :key ?R
     :description "Switch roles"
     :action ollama-buddy-roles-switch-role)
    
    (create-role
     :key ?E
     :description "Create new role"
     :action ollama-buddy-role-creator-create-new-role)
    
    (open-roles-directory
     :key ?D
     :description "Open roles directory"
     :action ollama-buddy-roles-open-directory)
    
    (swap-model
     :key ?m
     :description "Swap model"
     :action ollama-buddy--swap-model)
    
    (help
     :key ?h
     :description "Help"
     :action ollama-buddy--menu-help-assistant)
    
    ;; Your custom Turkish command
    (turkish-letters
     :key ?t
     :description "Fix Turkish Letters"
     :model "o:deepseek-r1:7b"
     :prompt "Fill in the missing Turkish letters in the following text:"
     :system "You are an expert in Turkish language. Your task is to correctly add any missing Turkish-specific letters (ç, ğ, ı, ö, ş, ü) to text that may be missing them. Only correct the spelling by adding proper Turkish letters - do not change any words or add any commentary. Return only the corrected text."
     :action (lambda () (ollama-buddy--send-with-command 'turkish-letters)))
    
    (send-region
     :key ?l
     :description "Send region"
     :action (lambda ()
               (let* ((selected-text (when (use-region-p)
                                       (buffer-substring-no-properties
                                        (region-beginning) (region-end)))))
                 (when (not selected-text)
                   (user-error "This command requires selected text"))
                 
                 (ollama-buddy--open-chat)
                 (insert selected-text))))
    ))
#+end_src

2. In Emacs, press =C-c o= to open the ollama-buddy menu
3. Press =R= to switch roles
4. Select "turkish" from the list
5. Now your menu will be simplified and focused on Turkish helpers
6. Select some Turkish text, open the menu again (=C-c o=), and press =t=

Let's break down what's happening in the command definition:

#+begin_src elisp
(turkish-letters
 :key ?t ; The key to press in the menu
 :description "Fix Turkish Letters" ; What appears in the menu
 :model "o:deepseek-r1:7b"
 :prompt "Fill in the missing Turkish letters in the following text:" ; Instruction sent to the LLM
 :system "You are an expert in Turkish language..."  ; Background instruction for the LLM
 :action (lambda () (ollama-buddy--send-with-command 'turkish-letters)))  ; Function to execute
#+end_src

- =:prompt= is what gets added before your selected text. It tells the AI what to do with your text.
- =:system= gives the AI context about its role and specific behavioral guidelines. This helps ensure it gives exactly the type of response you want.

The prompt is for the specific task, while the system message shapes the AI's overall approach. Together, they ensure consistent, specialized behavior.

* Tutorial: Setting Up Roles and Custom Menus

Roles in Ollama Buddy allow you to create different configurations of commands and models for specific use cases. This tutorial will guide you through setting up roles, creating custom menus, and effectively using them in your workflow.

** Understanding Roles

A role is essentially a preset configuration that defines:
- Which commands are available in your menu
- What models to use for specific commands
- What prompts and system messages to use
- Any special parameters for optimization

For example, you might have a "programmer" role focused on coding tasks and a "writer" role with writing-focused commands.

** Understanding Role File Naming Convention

The *file naming convention* is critical to understand how roles, preset files, and menu configurations work together:

- *Required filename format*: =ollama-buddy--preset__ROLE-NAME.el=
  - The double underscore =__= separates the prefix from your role name
  - The role name portion becomes the identifier shown when switching roles
  - Example: =ollama-buddy--preset__programmer.el= creates a role named "programmer"

This naming convention is how Ollama Buddy discovers and identifies role files in your roles directory. When you run =ollama-buddy-roles-switch-role=, the system:

1. Scans the =ollama-buddy-roles-directory= for files matching the pattern
2. Extracts the role name from each filename (the part after =__=)
3. Presents these names in the role selection interface
4. When selected, loads the corresponding file which redefines =ollama-buddy-command-definitions=
5. This redefinition immediately changes the available commands in your Ollama Buddy menu

The relationship chain works like this:
#+begin_src 
ollama-buddy--preset__ROLE-NAME.el → Defines ollama-buddy-command-definitions → Controls menu content
#+end_src

When creating roles using the interactive role creator (=C-c E=), this naming convention is automatically handled for you. When creating roles manually, you must follow this pattern for Ollama Buddy to recognize your role files correctly.

You can locate your roles directory with:
#+begin_src elisp
;; Check where your roles are stored
(message ollama-buddy-roles-directory)

;; Or open the directory directly
M-x ollama-buddy-roles-open-directory
#+end_src

By default, this is set to =~/.emacs.d/ollama-buddy-presets/=, but you can customize it:
#+begin_src elisp
(setq ollama-buddy-roles-directory "/your/custom/path/to/presets")
#+end_src

** Creating Custom Roles

There are two ways to create custom roles:

*** 1. Using the Interactive Role Creator

The most user-friendly approach:

1. Press =C-c E= or run =M-x ollama-buddy-role-creator-create-new-role=
2. Enter a name for your role (e.g., "programmer")
3. For each command you want to add:
   - Specify a command name (e.g., "refactor-code")
   - Choose a key shortcut for the menu
   - Add a description
   - Optionally specify a model
   - Optionally add prompt prefixes and system messages

*** 2. Creating Role Files Manually

For more advanced customization, create role files manually:

1. Create a file named =ollama-buddy--preset__your-role-name.el= in your =ollama-buddy-roles-directory=
2. Structure your file like this:

#+begin_src elisp
;; ollama-buddy preset for role: programmer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands - always include these
    (open-chat
     :key ?o
     :description "Open chat buffer"
     :action ollama-buddy--open-chat)
    
    (show-models
     :key ?v
     :description "View model status"
     :action ollama-buddy-show-model-status)
    
    (switch-role
     :key ?R
     :description "Switch roles"
     :action ollama-buddy-roles-switch-role)
    
    (create-role
     :key ?E
     :description "Create new role"
     :action ollama-buddy-role-creator-create-new-role)
    
    (open-roles-directory
     :key ?D
     :description "Open roles directory"
     :action ollama-buddy-roles-open-directory)
    
    ;; Custom commands for this role
    (refactor-code
     :key ?r
     :description "Refactor code"
     :model "codellama:7b"
     :prompt "Refactor this code to improve readability and efficiency:"
     :system "You are an expert software engineer who improves code quality while maintaining functionality."
     :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))
    
    (explain-code
     :key ?e
     :description "Explain code"
     :model "deepseek-r1:7b"
     :prompt "Explain what this code does in detail:"
     :system "You are a programming teacher who explains code clearly and thoroughly."
     :action (lambda () (ollama-buddy--send-with-command 'explain-code)))
    
    (git-commit
     :key ?g
     :description "Git commit message"
     :prompt "Write a concise git commit message for these changes:"
     :system "You are a version control expert who creates clear, concise commit messages."
     :action (lambda () (ollama-buddy--send-with-command 'git-commit)))
    ))
#+end_src

** Switching Between Roles

To switch between roles:

1. Press =C-c R= or run =M-x ollama-buddy-roles-switch-role=
2. Select a role from the completion list
3. The menu will update with the commands defined in that role

You can also switch roles from within the Ollama Buddy menu by pressing 'R'.

** Advanced Customization Techniques

*** Command-Specific Models

Assign specific models to commands for optimal performance:

#+begin_src elisp
(ollama-buddy-add-model-to-menu-entry 'refactor-code "codellama:7b")
#+end_src

*** Command-Specific Parameters

Optimize parameters for specific commands:

#+begin_src elisp
(ollama-buddy-add-parameters-to-command 'refactor-code
  'temperature 0.2
  'top_p 0.7
  'repeat_penalty 1.3)
#+end_src

*** Creating New Commands

Add entirely new commands to your menu:

#+begin_src elisp
(ollama-buddy-update-menu-entry 'my-new-command
  :key ?z
  :description "My new awesome command"
  :prompt "Here is what I want you to do:"
  :system "You are an expert system specialized in this task."
  :action (lambda () (ollama-buddy--send-with-command 'my-new-command)))
#+end_src

** Example: Creating a "Writer" Role

Here's a complete example of setting up a writing-focused role:

#+begin_src elisp
;; ollama-buddy preset for role: writer
(require 'ollama-buddy)

(setq ollama-buddy-command-definitions
  '(
    ;; Standard commands
    (open-chat
     :key ?o
     :description "Open chat buffer"
     :action ollama-buddy--open-chat)
    
    (show-models
     :key ?v
     :description "View model status"
     :action ollama-buddy-show-model-status)
    
    (switch-role
     :key ?R
     :description "Switch roles"
     :action ollama-buddy-roles-switch-role)
    
    (create-role
     :key ?E
     :description "Create new role"
     :action ollama-buddy-role-creator-create-new-role)
    
    (open-roles-directory
     :key ?D
     :description "Open roles directory"
     :action ollama-buddy-roles-open-directory)
    
    ;; Writing-focused commands
    (summarize
     :key ?s
     :description "Summarize text"
     :prompt "Summarize the following text in a concise manner:"
     :system "You are an expert at extracting the key points from any text."
     :action (lambda () (ollama-buddy--send-with-command 'summarize)))
    
    (proofread
     :key ?p
     :description "Proofread text"
     :model "deepseek-r1:7b"
     :prompt "Proofread the following text and correct any errors:"
     :system "You are a professional editor who identifies and corrects grammar, spelling, and style errors."
     :action (lambda () (ollama-buddy--send-with-command 'proofread)))
    
    (rewrite
     :key ?r
     :description "Rewrite text"
     :prompt "Rewrite the following text to improve clarity and flow:"
     :system "You are a skilled writer who can improve any text while preserving its meaning."
     :action (lambda () (ollama-buddy--send-with-command 'rewrite)))
    
    (brainstorm
     :key ?b
     :description "Brainstorm ideas"
     :model "llama3.2:3b"
     :prompt "Generate creative ideas related to the following topic:"
     :parameters ((temperature . 1.0) (top_p . 0.95))
     :action (lambda () (ollama-buddy--send-with-command 'brainstorm)))
    ))
#+end_src

Save this as =ollama-buddy--preset__writer.el= in your =ollama-buddy-roles-directory=.

** Tips for Effective Role Usage

1. *Group related commands*: Create roles around specific workflows or tasks
2. *Match models to tasks*: Use lightweight models for simple tasks and more powerful models for complex ones
3. *Customize system prompts*: Craft specific system prompts to guide the model for each command
4. *Use the roles directory*: Press =C-c D= to quickly access and manage your role files
5. *Create specialized roles*: Consider roles for programming, writing, translation, or domain-specific knowledge

* Core Features in Detail

** Chat Interface

The ollama-buddy chat buffer is powered by =org-mode=, providing enhanced readability and structure. Conversations automatically format user prompts and AI responses with org-mode headings, making them easier to navigate and utilize.

Benefits include:
- Outlining and heading navigation
- Org export capabilities
- Source code fontification

Toggle Markdown to Org conversion with =C-c C-o=.

** Interface Levels

You can choose between two interface levels depending on your preference:

- *Basic Interface*: Shows minimal commands for new users
- *Advanced Interface*: Shows all available commands and features

Set your preferred interface level:

#+begin_src elisp
(setq ollama-buddy-interface-level 'basic)  ; or 'advanced
#+end_src

By default, the menu is set to Basic. Switch between levels during your session with =C-c A=.

** Conversation History

Ollama Buddy maintains context between your interactions by:

- Tracking conversation history between prompts and responses
- Sending previous messages to Ollama for improved contextual responses
- Displaying a history counter in the status line showing conversation length
- Providing configurable history length limits to control memory usage

Control this feature with:

#+begin_src elisp
;; Enable/disable conversation history (default: t)
(setq ollama-buddy-history-enabled t)

;; Set maximum conversation pairs to remember (default: 10)
(setq ollama-buddy-max-history-length 10)

;; Show/hide the history counter in the header line (default: t)
(setq ollama-buddy-show-history-indicator t)
#+end_src

History-related commands:
- =C-c H=: Toggle history tracking on/off
- =C-c X=: Clear the current conversation history
- =C-c E=: Edit conversation history (universal argument to edit specific model)

** Parameter Management

Comprehensive parameter management gives you complete control over your Ollama model's behavior through API options:

- *All Parameters* - Set any custom option for the Ollama LLM at runtime
- *Smart Parameter Management*: Only modified parameters are sent to Ollama, preserving defaults
- *Visual Parameter Interface*: Clear display showing which parameters are active with highlighting

Access parameter management through keyboard shortcuts:
- =C-c P= - Edit a parameter
- =C-c G= - Display current parameters
- =C-c I= - Show parameter help
- =C-c K= - Reset parameters to defaults

** Command-Specific Parameters

You can define specific parameter sets for each command in the menu, enabling optimization for particular use cases:

#+begin_src elisp
;; Update properties and parameters at once
(ollama-buddy-update-command-with-params 'describe-code
 :model "codellama:latest"
 :parameters '((temperature . 0.3) (top_p . 0.8)))
#+end_src

This feature is particularly useful for:
1. *Code-related tasks*: Lower temperature for more deterministic code generation
2. *Creative writing*: Higher temperature for more varied and creative outputs
3. *Technical explanations*: Balanced settings for clear, accurate explanations
4. *Summarization tasks*: Custom parameters to control verbosity and focus

** System Prompt Support

Ollama Buddy supports system prompts, allowing you to set and manage system-level instructions for your AI interactions:

- *Set a system prompt*: Use =C-u C-c C-c= to designate any user prompt as a system prompt
- *Clear system prompt*: Use =C-u C-u C-c C-c= to clear the system prompt
- *View system prompt*: Use =C-c C-s= to display the current system prompt

System prompts remain active across user queries, providing better control over conversation context. The status bar displays an "S" indicator when a system prompt is active.

You can also define system prompts per command:

#+begin_src elisp
(ollama-buddy-update-menu-entry
 'refactor-code
 :model "qwen2.5-coder:7b"
 :system "You are an expert software engineer who improves code and only mainly using the principles exhibited by Ada")
#+end_src

** Token Usage Tracking

Real-time token tracking helps you monitor usage and performance:

- Track token counts, rates, and usage history
- Display token usage statistics (=C-c t= or menu option)
- Toggle token stats display after responses
- Real-time updates via timer

** Multi-model Comparison

With the multishot mode, you can send a prompt to multiple models in sequence and compare their responses:

- Models are assigned letters for quick selection (e.g., =(a) mistral=, =(b) gemini=)
- Use =C-c M= to initiate a multishot sequence
- Status updates track progress during multishot execution

To use multishot mode:
1. =C-c M= to start a multishot session
2. Type a sequence of model letters (e.g., =abc= to use models a, b, and c)
3. The selected models process the prompt one by one

** Role-Based Presets

Roles in Ollama Buddy are essentially profiles tailored to specific tasks:

- Store custom roles in =ollama-buddy-roles-directory= (default: =~/.emacs.d/ollama-buddy-presets/=)
- Switch between roles with =M-x ollama-buddy-roles-switch-role= or menu option =R=
- Create custom roles with =M-x ollama-buddy-role-creator-create-new-role= or menu option =N=
- Open role directory with =M-x ollama-buddy-roles-open-directory= or menu option =D=

Preconfigured presets available:
- ollama-buddy--preset__buffy.el
- ollama-buddy--preset__default.el
- ollama-buddy--preset__emacs.el
- ollama-buddy--preset__developer.el
- ollama-buddy--preset__janeway.el
- ollama-buddy--preset__translator.el
- ollama-buddy--preset__writer.el

** Session Management

Session management allows you to save conversations and restore them with relevant context:

- *Save session* with =ollama-buddy-sessions-save= or =C-c S=
- *Load session* with =ollama-buddy-sessions-load= or =C-c L=
- *List sessions* with =ollama-buddy-sessions-list= or =C-c Y=
- *Delete session* with =ollama-buddy-sessions-delete= or =C-c K=
- *New session* with =ollama-buddy-sessions-new= or =C-c E=

Sessions preserve model-specific chat history to prevent context contamination across different models.

** Prompt History Support

Prompts are integrated into the Emacs history mechanism and persist across sessions:

- Use =M-p= to navigate prompt history in the chat buffer
- Use =M-p= / =M-n= within the minibuffer to insert previous prompts

* Advanced Customizing the Ollama Buddy Menu System

Ollama Buddy provides a flexible menu system that can be easily customized to match your workflow. The menu is built from =ollama-buddy-command-definitions=, which you can modify or extend in your Emacs configuration.

** Basic Structure

Each menu item is defined using a property list with these key attributes:

#+begin_src elisp
(command-name
 :key ?k              ; Character for menu selection
 :description "desc"  ; Menu item description
 :model "model-name"  ; Specific Ollama model (optional)
 :prompt "prompt"     ; System prompt (optional)
 :system "system"     ; System prompt (optional)
 :parameters ((param . value)) ; Command-specific parameters (optional)
 :action function)    ; Command implementation
#+end_src

** Examples

*** Adding New Commands

You can add new commands to =ollama-buddy-command-definitions= in your config:

#+begin_src elisp
;; Add a single new command
(add-to-list 'ollama-buddy-command-definitions
               '(pirate
                 :key ?i
                 :description "R Matey!"
                 :model "mistral:latest"
                 :prompt "Translate the following as if I was a pirate:"
                 :action (lambda () (ollama-buddy--send-with-command 'pirate))))

;; Incorporate into a use-package
(use-package ollama-buddy
  :load-path "path/to/ollama-buddy"
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  (add-to-list 'ollama-buddy-command-definitions
               '(pirate
                 :key ?i
                 :description "R Matey!"
                 :model "mistral:latest"
                 :prompt "Translate the following as if I was a pirate:"
                 :action (lambda () (ollama-buddy--send-with-command 'pirate))))
  :custom ollama-buddy-default-model "llama:latest")

;; Add multiple commands at once
(setq ollama-buddy-command-definitions
      (append ollama-buddy-command-definitions
              '((summarize
                 :key ?u
                 :description "Summarize text"
                 :model "tinyllama:latest"
                 :prompt "Provide a brief summary:"
                 :action (lambda () 
                          (ollama-buddy--send-with-command 'summarize)))
                (translate-spanish
                 :key ?t
                 :description "Translate to Spanish"
                 :model "mistral:latest"
                 :prompt "Translate this text to Spanish:"
                 :action (lambda () 
                          (ollama-buddy--send-with-command 'translate-spanish))))))
#+end_src

*** Creating a Minimal Setup

You can create a minimal configuration by defining only the commands you need:

#+begin_src elisp
;; Minimal setup with just essential commands
(setq ollama-buddy-command-definitions
      '((send-basic
         :key ?l
         :description "Send Basic Region"
         :action (lambda () (ollama-buddy--send-with-command 'send-basic)))

        (quick-define
         :key ?d
         :description "Define word"
         :model "tinyllama:latest"
         :prompt "Define this word:"
         :action (lambda () 
                  (ollama-buddy--send-with-command 'quick-define)))
        (quit
         :key ?q
         :description "Quit"
         :model nil
         :action (lambda () 
                  (message "Quit Ollama Shell menu.")))))
#+end_src

** Tips for Custom Commands

1. Choose unique keys for menu items
2. Match models to task complexity (small models for quick tasks)
3. Use clear, descriptive names

** Command Properties Reference

| Property     | Description                              | Required |
|--------------+------------------------------------------+----------|
| :key         | Single character for menu selection      | Yes      |
| :description | Menu item description                    | Yes      |
| :model       | Specific Ollama model to use             | No       |
| :prompt      | Static system prompt                     | No       |
| :system      | System-level instruction                 | No       |
| :parameters  | Command-specific parameters              | No       |
| :action      | Function implementing the command        | Yes      |

- Model Selection and Fallback Logic

** Overview

You can associate specific commands defined in the menu with an Ollama LLM to optimize performance for different tasks. For example, if speed is a priority over accuracy, such as when retrieving synonyms, you might use a lightweight model like TinyLlama or a 1B–3B model. On the other hand, for tasks that require higher precision, like code refactoring, a more capable model such as Qwen-Coder 7B can be assigned to the "refactor" command on the buddy menu system.

Since this package enables seamless model switching through Ollama, the buddy menu can present a list of commands, each linked to an appropriate model. All Ollama interactions share the same chat buffer, ensuring that menu selections remain consistent. Additionally, the status bar on the header line and the prompt itself indicate the currently active model.

Ollama Buddy also includes a model selection mechanism with a fallback system to ensure commands execute smoothly, even if the preferred model is unavailable.

** Command-Specific Models

Commands in =ollama-buddy-command-definitions= can specify preferred models using the =:model= property. This allows optimizing different commands for specific models:

#+begin_src elisp
(defcustom ollama-buddy-command-definitions
  '((refactor-code
     :key ?r
     :description "Refactor code"
     :model "qwen-coder:latest"
     :prompt "refactor the following code:")
    (git-commit
     :key ?g
     :description "Git commit message"
     :model "tinyllama:latest"
     :prompt "write a concise git commit message for the following:")
    (send-region
     :key ?l
     :description "Send region"
     :model "llama:latest"))
  ...)
#+end_src

When =:model= is =nil=, the command will use whatever model is currently set as =ollama-buddy-default-model=.

** Fallback Chain

When executing a command, the model selection follows this fallback chain:

1. Command-specific model (=:model= property)
2. Current model (=ollama-buddy-default-model=)
3. User selection from available models

** Configuration Options

*** Setting the Fallback Model

#+begin_src elisp
(setq ollama-buddy-default-model "llama:latest")
#+end_src

** User Interface Feedback

When a fallback occurs, Ollama Buddy provides clear feedback:

- The header line shows which model is being used
- If using a fallback model, an orange warning appears showing both the requested and actual model
- The model status can be viewed using the "View model status" command (v key)

* Customization

** Basic Customization

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  ;; Set default model
  (ollama-buddy-default-model "llama3:latest")
  ;; Set interface level (basic or advanced)
  (ollama-buddy-interface-level 'advanced)
  ;; Enable model colors
  (ollama-buddy-enable-model-colors t)
  ;; Set menu columns
  (ollama-buddy-menu-columns 3))
#+end_src

** Remote Ollama Server

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-host "http://<remote-server>")
  (ollama-buddy-port 11400))
#+end_src

** Ollama Cloud Models

Ollama Cloud models run on ollama.com infrastructure and provide access to large models (like qwen3-coder:480b, deepseek-v3.1:671b) without requiring local hardware capable of running them.

*** Authentication

Cloud models require a one-time sign-in. Use the transient menu (=C-c O=) and select "Cloud Auth > Sign In", or run =M-x ollama-buddy-cloud-signin=. This will automatically open your browser for authentication.

The authentication is handled by the local Ollama server using SSH keys stored in =~/.ollama/=, so no API key configuration is needed in Emacs.

*** Selecting Cloud Models

- Use =C-u C-c m= (universal argument + model switch) to select from cloud models
- Or use the transient menu: "Model > Cloud"
- Cloud models are identified by the =-cloud= suffix (e.g., =minimax-m2.1:cloud=)
- The status line shows a ☁ indicator when using a cloud model

*** Available Cloud Models

The default cloud models list includes:
- =qwen3-coder:480b-cloud= - Large coding model
- =deepseek-v3.1:671b-cloud= - DeepSeek's flagship model
- =gpt-oss:120b-cloud= / =gpt-oss:20b-cloud= - Open source GPT variants
- =glm-4.7:cloud= - GLM model
- =minimax-m2.1:cloud= - MiniMax model

You can customize the list via =ollama-buddy-cloud-models=.

** Command-Specific Models

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (ollama-buddy-add-model-to-menu-entry 'dictionary-lookup "tinyllama:latest")
  (ollama-buddy-add-model-to-menu-entry 'synonym "tinyllama:latest"))
#+end_src

** Command-Specific System Prompts

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (ollama-buddy-update-menu-entry
   'refactor-code
   :model "qwen2.5-coder:7b"
   :system "You are an expert software engineer who improves code using Ada principles"))
#+end_src

** Variables

#+begin_src emacs-lisp :results table :colnames '("Custom variable" "Description") :exports results
  (let ((rows))
    (mapatoms
     (lambda (symbol)
       (when (and (string-match "^ollama-buddy-" (symbol-name symbol))
                  (not (string-match "--" (symbol-name symbol)))
                  (or (custom-variable-p symbol)
                      (boundp symbol)))
         (push `(,symbol
                 ,(car
                   (split-string
                    (or (get (indirect-variable symbol)
                             'variable-documentation)
                        (get symbol 'variable-documentation)
                        "")
                    "\n")))
               rows))))
    rows)
#+end_src

#+RESULTS:
| Custom variable                          | Description                                                                  |
|------------------------------------------+------------------------------------------------------------------------------|
| ollama-buddy-awesome-categorize-prompts  | Whether to categorize prompts based on common keywords.                      |
| ollama-buddy-awesome-prompts-file        | Filename containing the prompts within the repository.                       |
| ollama-buddy-grok-api-endpoint           | Endpoint for Grok chat completions API.                                      |
| ollama-buddy-remote-models               | List of available remote models.                                             |
| ollama-buddy-command-definitions         | Comprehensive command definitions for Ollama Buddy.                          |
| ollama-buddy-enable-model-colors         | Whether to show model colors.                                                |
| ollama-buddy-interface-level             | Level of interface complexity to display.                                    |
| ollama-buddy-show-params-in-header       | Whether to show modified parameters in the header line.                      |
| ollama-buddy-debug-mode                  | When non-nil, show raw JSON messages in a debug buffer.                      |
| ollama-buddy-mode                        | Non-nil if Ollama-Buddy mode is enabled.                                     |
| ollama-buddy-grok-marker-prefix          | Prefix used to identify Grok models in the ollama-buddy interface.           |
| ollama-buddy-reasoning-markers           | List of marker pairs that encapsulate reasoning/thinking sections.           |
| ollama-buddy-awesome-update-on-startup   | Whether to automatically update prompts when Emacs starts.                   |
| ollama-buddy-context-bar-width           | Width of the context progress bar in characters.                             |
| ollama-buddy-sessions-directory          | Directory containing ollama-buddy session files.                             |
| ollama-buddy-history-model-view-mode-map | Keymap for model-specific history viewing mode.                              |
| ollama-buddy-menu-columns                | Number of columns to display in the Ollama Buddy menu.                       |
| ollama-buddy-gemini-api-endpoint         | Endpoint format for Google Gemini API.                                       |
| ollama-buddy-fabric-pattern-categories   | List of pattern categories to focus on when listing patterns.                |
| ollama-buddy-status-update-interval      | Interval in seconds to update the status line with background operations.    |
| ollama-buddy-fabric-update-on-startup    | Whether to automatically update patterns when Emacs starts.                  |
| ollama-buddy-available-models            | List of available models to pull from Ollama Hub.                            |
| ollama-buddy-grok-max-tokens             | Maximum number of tokens to generate in the response.                        |
| ollama-buddy-claude-max-tokens           | Maximum number of tokens to generate in the response.                        |
| ollama-buddy-openai-marker-prefix        | Prefix to indicate that a model is from OpenAI rather than Ollama.           |
| ollama-buddy-history-enabled             | Whether to use conversation history in Ollama requests.                      |
| ollama-buddy-grok-api-key                | API key for accessing Grok services.                                         |
| ollama-buddy-grok-default-model          | Default Grok model to use.                                                   |
| ollama-buddy-claude-marker-prefix        | Prefix used to identify Claude models in the model list.                     |
| ollama-buddy-roles-directory             | Directory containing ollama-buddy role preset files.                         |
| ollama-buddy-history-view-mode-map       | Keymap for history viewing mode.                                             |
| ollama-buddy-fabric-local-dir            | Local directory where Fabric patterns will be stored.                        |
| ollama-buddy-show-history-indicator      | Whether to show the history indicator in the header line.                    |
| ollama-buddy-context-size-thresholds     | Thresholds for context usage warnings.                                       |
| ollama-buddy-mode-map                    | Keymap for ollama-buddy mode.                                                |
| ollama-buddy-marker-prefix               | Prefix used to identify Ollama models in the ollama-buddy interface.         |
| ollama-buddy-fabric-patterns-subdir      | Subdirectory within the Fabric repo containing the patterns.                 |
| ollama-buddy-context-display-type        | How to display context usage in the status bar.                              |
| ollama-buddy-awesome-repo-url            | URL of the Awesome ChatGPT Prompts GitHub repository.                        |
| ollama-buddy-openai-api-endpoint         | Endpoint for OpenAI chat completions API.                                    |
| ollama-buddy-vision-models               | List of models known to support vision capabilities.                         |
| ollama-buddy-params-active               | Currently active values for Ollama API parameters.                           |
| ollama-buddy-openai-default-model        | Default OpenAI model to use.                                                 |
| ollama-buddy-claude-default-model        | Default Claude model to use.                                                 |
| ollama-buddy-params-modified             | Set of parameters that have been explicitly modified by the user.            |
| ollama-buddy-gemini-temperature          | Temperature setting for Gemini requests (0.0-1.0).                           |
| ollama-buddy-current-session-name        | The name of the currently loaded session.                                    |
| ollama-buddy-host                        | Host where Ollama server is running.                                         |
| ollama-buddy-image-formats               | List of regular expressions matching supported image file formats.           |
| ollama-buddy-streaming-enabled           | Whether to use streaming mode for responses.                                 |
| ollama-buddy-max-history-length          | Maximum number of message pairs to keep in conversation history.             |
| ollama-buddy-gemini-default-model        | Default Gemini model to use.                                                 |
| ollama-buddy-default-model               | Default Ollama model to use.                                                 |
| ollama-buddy-claude-temperature          | Temperature setting for Claude requests (0.0-1.0).                           |
| ollama-buddy-vision-enabled              | Whether to enable vision support for models that support it.                 |
| ollama-buddy-awesome-prompt-variable     |                                                                              |
| ollama-buddy-display-token-stats         | Whether to display token usage statistics in responses.                      |
| ollama-buddy-show-context-percentage     | Whether to show context percentage in the status bar.                        |
| ollama-buddy-claude-api-endpoint         | Endpoint for Anthropic Claude API.                                           |
| ollama-buddy-gemini-api-key              | API key for accessing Google Gemini services.                                |
| ollama-buddy-openai-max-tokens           | Maximum number of tokens to generate in the response.                        |
| ollama-buddy-claude-api-key              | API key for accessing Anthropic Claude services.                             |
| ollama-buddy-params-profiles             | Predefined parameter profiles for different usage scenarios.                 |
| ollama-buddy-awesome-local-dir           | Local directory where Awesome ChatGPT Prompts will be stored.                |
| ollama-buddy-params-defaults             | Default values for Ollama API parameters.                                    |
| ollama-buddy-mode-line-segment           | Mode line segment for Ollama Buddy.                                          |
| ollama-buddy-grok-temperature            | Temperature setting for Grok requests (0.0-1.0).                             |
| ollama-buddy-port                        | Port where Ollama server is running.                                         |
| ollama-buddy-default-register            | Default register to store the current response when not in multishot mode.   |
| ollama-buddy-fabric-repo-url             | URL of the Fabric GitHub repository.                                         |
| ollama-buddy-gemini-max-tokens           | Maximum number of tokens to generate in the response.                        |
| ollama-buddy-modelfile-directory         | Directory for storing temporary Modelfiles.                                  |
| ollama-buddy-fallback-context-sizes      | Mapping of model names to their default context sizes.                       |
| ollama-buddy-context-bar-chars           | Characters used to draw the context progress bar.                            |
| ollama-buddy-openai-temperature          | Temperature setting for OpenAI requests (0.0-2.0).                           |
| ollama-buddy-convert-markdown-to-org     | Whether to automatically convert markdown to `org-mode' format in responses. |
| ollama-buddy-mode-hook                   | Hook run after entering or leaving `ollama-buddy-mode'.                      |
| ollama-buddy-gemini-marker-prefix        | Prefix used to identify Gemini models in the ollama-buddy interface.         |
| ollama-buddy-hide-reasoning              | When non-nil, hide reasoning/thinking blocks from the stream output.         |
| ollama-buddy-openai-api-key              | API key for accessing OpenAI services.                                       |

* Interactive functions

#+begin_src emacs-lisp :results table :colnames '("Command" "Description") :exports results
  (let ((rows))
    (mapatoms
     (lambda (symbol)
       (when (and (string-match "^ollama-buddy-" (symbol-name symbol))
                  (not (string-match "--" (symbol-name symbol)))
                  (commandp symbol))  ;; Check if it's an interactive command
         (push `(,symbol
                 ,(car
                   (split-string
                    (or (documentation symbol)
                        "")
                    "\n")))
               rows))))
    rows)
#+end_src

#+RESULTS:
| Command                                   | Description                                                                    |
|-------------------------------------------+--------------------------------------------------------------------------------|
| ollama-buddy-open-info                    | Open the Info manual for the ollama-buddy package.                             |
| ollama-buddy-dired-import-gguf            | Import the GGUF file at point in Dired into Ollama.                            |
| ollama-buddy-toggle-markdown-conversion   | Toggle automatic conversion of markdown to ‘org-mode’ format.                  |
| ollama-buddy-fabric-send                  | Apply a Fabric pattern to the selected text and send to Ollama.                |
| ollama-buddy-reset-system-prompt          | Reset the system prompt to default (none).                                     |
| ollama-buddy-beginning-of-prompt          | Move point to the beginning of the current prompt.                             |
| ollama-buddy-toggle-streaming             | Toggle streaming mode for Ollama responses.                                    |
| ollama-buddy-toggle-reasoning-visibility  | Toggle visibility of reasoning/thinking sections in responses.                 |
| ollama-buddy-history-toggle-edit-mode     | Toggle between viewing and editing modes for history.                          |
| ollama-buddy-mode                         | Minor mode for ollama-buddy keybindings.                                       |
| ollama-buddy-roles-switch-role            | Switch to a different ollama-buddy role.                                       |
| ollama-buddy-pull-model                   | Pull or update MODEL from Ollama Hub asynchronously.                           |
| ollama-buddy-history-edit                 | View and edit the conversation history in a buffer.                            |
| ollama-buddy-previous-history             | Navigate to previous item in prompt history.                                   |
| ollama-buddy-toggle-params-in-header      | Toggle display of modified parameters in the header line.                      |
| ollama-buddy-display-token-graph          | Display a visual graph of token usage statistics.                              |
| ollama-buddy-toggle-token-display         | Toggle display of token statistics after each response.                        |
| ollama-buddy-show-raw-model-info          | Retrieve and display raw JSON information about the current default MODEL.     |
| ollama-buddy-history-cancel               | Cancel editing the history.                                                    |
| ollama-buddy-sessions-load                | Load an Ollama Buddy session with improved org file handling.                  |
| ollama-buddy-transient-menu-wrapper       | Wrapper function for safely loading the Ollama Buddy transient menu.           |
| ollama-buddy-sessions-list                | Display a list of saved sessions.                                              |
| ollama-buddy-toggle-context-percentage    | Toggle display of context percentage in the status bar.                        |
| ollama-buddy-transient-awesome-menu       | Awesome ChatGPT Prompts for ollama-buddy.                                      |
| ollama-buddy-show-context-info            | Show detailed information about context sizes for all models.                  |
| ollama-buddy-fabric-set-system-prompt     | Set the system prompt to a Fabric pattern without sending a request.           |
| ollama-buddy-params-help                  | Display help for Ollama parameters.                                            |
| ollama-buddy-awesome-send                 | Apply an Awesome ChatGPT Prompt to the selected text and send to Ollama.       |
| ollama-buddy-sessions-new                 | Start a new session by clearing history and buffer.                            |
| ollama-buddy-show-system-prompt           | Display the current system prompt in a buffer.                                 |
| ollama-buddy-awesome-show-prompts-menu    | Show a transient menu of Awesome ChatGPT Prompt organized by category.         |
| ollama-buddy-awesome-populate-prompts     | Populate the list of available prompt from the local repository.               |
| ollama-buddy-awesome-set-system-prompt    | Set the system prompt to an Awesome ChatGPT Prompt without sending a request.  |
| ollama-buddy-params-edit                  | Edit a specific parameter PARAM interactively.                                 |
| ollama-buddy-awesome-list-prompts         | Display a list of available Awesome ChatGPT Prompt.                            |
| ollama-buddy-transient-parameter-menu     | Parameter menu for Ollama Buddy.                                               |
| ollama-buddy-toggle-debug-mode            | Toggle display of raw JSON messages in a debug buffer.                         |
| ollama-buddy-awesome-show-prompt          | Display the full content of a prompt with FORMATTED-NAME.                      |
| ollama-buddy-toggle-model-colors          | Toggle the use of model-specific colors in ollama-buddy.                       |
| ollama-buddy-awesome-setup                | Set up the ollama-buddy-awesome package.                                       |
| ollama-buddy-fabric-show-pattern          | Display the full content of a PATTERN.                                         |
| ollama-buddy-fabric-sync-patterns         | Sync the latest patterns from the Fabric GitHub repository.                    |
| ollama-buddy-reset-all-prompts            | Reset both system prompt and suffix to default (none).                         |
| ollama-buddy-fabric-list-patterns         | Display a list of available Fabric patterns with descriptions.                 |
| ollama-buddy-history-search               | Search through the prompt history using a ‘completing-read’ interface.         |
| ollama-buddy-toggle-history               | Toggle conversation history on/off.                                            |
| ollama-buddy-display-token-stats          | Display token usage statistics.                                                |
| ollama-buddy-role-creator-create-new-role | Create a new role interactively.                                               |
| ollama-buddy-roles-open-directory         | Open the ollama-buddy roles directory in Dired.                                |
| ollama-buddy-params-reset                 | Reset all parameters to default values and clear modification tracking.        |
| ollama-buddy-menu                         | Display Ollama Buddy menu with support for prefixed model references.          |
| ollama-buddy-unload-all-models            | Unload all currently running Ollama models to free up resources.               |
| ollama-buddy-history-toggle-edit-model    | Toggle between viewing and editing modes for MODEL history.                    |
| ollama-buddy-transient-menu               | Ollama Buddy main menu.                                                        |
| ollama-buddy-fabric-setup                 | Set up the ollama-buddy-fabric package.                                        |
| ollama-buddy-manage-models                | Update the model management interface to include unload capabilities.          |
| ollama-buddy-history-edit-model           | Edit the conversation history for a specific MODEL.                            |
| ollama-buddy-roles-create-directory       | Create the ollama-buddy roles directory if it doesn’t exist.                   |
| ollama-buddy-history-save-model           | Save the edited history for MODEL back to variable.                            |
| ollama-buddy-toggle-context-display-type  | Toggle between text and bar display for context usage.                         |
| ollama-buddy-set-max-history-length       | Set the maximum number of message pairs to keep in conversation history.       |
| ollama-buddy-reset-suffix                 | Reset the suffix to default (none).                                            |
| ollama-buddy-show-model-status            | Display status of models referenced in command definitions with color coding.  |
| ollama-buddy-next-history                 | Navigate to next item in prompt history.                                       |
| ollama-buddy-clear-history                | Clear the conversation history.                                                |
| ollama-buddy-awesome-sync-prompts         | Sync the latest prompt from the Awesome ChatGPT Prompt GitHub repository.      |
| ollama-buddy-set-model-context-size       | Manually set the context size for MODEL to SIZE.                               |
| ollama-buddy-sessions-delete              | Delete an Ollama Buddy session.                                                |
| ollama-buddy-transient-profile-menu       | Parameter profiles menu for Ollama Buddy.                                      |
| ollama-buddy-set-suffix                   | Set the current prompt as a suffix.                                            |
| ollama-buddy-set-system-prompt            | Set the current prompt as a system prompt.                                     |
| ollama-buddy-history-save                 | Save the edited history back to ‘ollama-buddy--conversation-history-by-model’. |
| ollama-buddy-import-gguf-file             | Import a GGUF file at FILE-PATH into Ollama.                                   |
| ollama-buddy-transient-commands-menu      | Commands menu for Ollama Buddy.                                                |
| ollama-buddy-toggle-interface-level       | Toggle between basic and advanced interface levels.                            |
| ollama-buddy-transient-fabric-menu        | Fabric patterns menu for Ollama Buddy.                                         |
| ollama-buddy-params-display               | Display the current Ollama parameter settings.                                 |
| ollama-buddy-fabric-populate-patterns     | Populate the list of available patterns from the local repository.             |
| ollama-buddy-sessions-save                | Save the current Ollama Buddy session.                                         |

* Interactive functions

All interactive functions available in ollama-buddy are listed in the original README.

* ollama API Support Tables

** Core API

| API Endpoint    | Method | Purpose                            | Supported? | will ollama-buddy support?        |
|-----------------+--------+------------------------------------+------------+-----------------------------------|
| =/api/chat=     | POST   | Send chat messages to a model      | Yes        |                                   |
| =/api/generate= | POST   | Generate text without chat context | No         | Probably not, as chat covers      |
| =/api/delete=   | DELETE | Deletes available models           | Yes        |                                   |
| =/api/tags=     | GET    | List available models              | Yes        |                                   |
| =/api/pull=     | POST   | Pull a model from Ollama library   | Yes        |                                   |
| =/api/push=     | POST   | Push a model to the Ollama library | No         | No                                |
| =/api/create=   | POST   | Create a model from a Modelfile    | Yes        |                                   |
| =/api/show=     | POST   | Show model information             | Yes        |                                   |
| =/api/ps=       | GET    | List running models                | Yes        |                                   |
| =/api/copy=     | POST   | Copy a model                       | Yes        |                                   |
| =/api/embed=    | POST   | Generate embeddings from text      | No         | Probably when I can figure it out |
| =/api/version=  | GET    | Get Ollama version                 | Yes        |                                   |
| Cancel requests | Custom | Terminate ongoing request          | Yes        |                                   |

** Core Params

| Parameter    | Supported | Notes                                            | will ollama-buddy support? |
|--------------+-----------+--------------------------------------------------+----------------------------|
| =model=      | Yes       | Can be specified per command or globally         |                            |
| =prompt=     | Yes       | Core feature with extensive prompt handling      |                            |
| =suffix=     | Yes/No    | Can be set with =ollama-buddy-set-suffix=        |                            |
| =images=     | No        |                                                  | Unlikely, this is Emacs!   |
| =format=     | No        |                                                  | Maybe                      |
| =options=    | Yes       | Full implementation of all model parameters      |                            |
| =system=     | Yes       | Can be set with =ollama-buddy-set-system-prompt= |                            |
| =template=   | No        |                                                  | Maybe                      |
| =stream=     | Yes       | Toggleable with =ollama-buddy-toggle-streaming=  |                            |
| =raw=        | No        |                                                  | Might as well              |
| =keep_alive= | No        | Uses default Ollama behavior                     | Yes                        |

** Model Options Parameters 

| Parameter Group         | Parameters                                                                                     | Supported |
|-------------------------+------------------------------------------------------------------------------------------------+-----------|
| *Temperature Controls*  | =temperature=, =top_k=, =top_p=, =min_p=, =typical_p=                                          | Yes       |
| *Repetition Controls*   | =repeat_last_n=, =repeat_penalty=, =presence_penalty=, =frequency_penalty=                     | Yes       |
| *Advanced Sampling*     | =mirostat=, =mirostat_tau=, =mirostat_eta=, =penalize_newline=, =stop=                         | Yes       |
| *Resource Management*   | =num_keep=, =seed=, =num_predict=, =num_ctx=, =num_batch=                                      | Yes       |
| *Hardware Optimization* | =numa=, =num_gpu=, =main_gpu=, =low_vram=, =vocab_only=, =use_mmap=, =use_mlock=, =num_thread= | Yes       |

* Bugs                                                                 :bugs:

** TODO GGUF to be asynchronous

** TODO Better attachment signalling

** TODO Modifying a parameter internal defaults would flag as not modified

** TODO No streaming with hiding reasoning corrupts output

** TODO Pushing hundreds or even thousands of lines to the chat buffer takes a while

** TODO Check tags caching

** TODO keybindings in ollama chat buffer are reserved

** DOING Kill online AI process with cancel request

** DONE Claude request and connection can sometimes take time and it blocks!

** DONE allow switch to remote LLMs even when ollama not running

** DONE Vision image with spaces not processed as image

** DONE Save system prompts as part of session

** DONE chat ollama names to include prefix

** DONE Still some encoding issues on returns from online AI

** DONE Conversation history to display with Claude/ChatGPT

** DONE I think some hard coded Claude models currently don't work

** DONE register contents to be converted to org, currently it is in markdown

** DONE =C-a= beginning-of-line to jump to prompt start like comint

** DONE As shell generally C-n to end of list to clear prompt history

** DONE Add Texinfo to MELPA recipe

** DONE System prompt not cleared on new session

** DONE Remove fabric from Commands transient menu

** DONE External AI to copy to register

** DONE First request with no chat buffer fails

* Roadmap                                                           :roadmap:

** Ollama API Improvements (Jan 2026)

Recent Ollama releases (v0.12-v0.15) have introduced significant new features. The following roadmap items represent opportunities to integrate these capabilities into ollama-buddy.

*** DOING Cloud Models Support

Ollama v0.12+ supports cloud-hosted models via =ollama.com= infrastructure, enabling access to larger models (e.g., =deepseek-v3.1:671b-cloud=, =qwen3-coder:480b-cloud=) without local hardware requirements.

*Implemented:*
- =ollama-buddy-cloud-models= customization for available cloud models
- =C-u C-c m= to select from cloud models (prefix argument to swap-model)
- Transient menu "Model > Cloud" option (=c= key)
- Cloud indicator (☁) in status line when using cloud model
- =ollama-buddy--cloud-model-p= function to detect cloud models
- Validation accepts cloud models without requiring local availability
- =ollama-buddy-cloud-signin= - runs =ollama signin= to authenticate (opens browser)
- =ollama-buddy-cloud-signout= - runs =ollama signout= to sign out
- =ollama-buddy-cloud-status= - checks cloud authentication status
- Transient menu "Cloud Auth" section with Sign In (=2=), Sign Out (=3=), Auth Status (=4=)
- =ollama-buddy-ollama-executable= customization for ollama CLI path

*Still TODO:*
- Handle potential network latency differences
- Auto-fetch available cloud models from API (if endpoint exists)

*References:* [[https://ollama.com/blog/cloud-models][Ollama Cloud Models Blog]]

*** TODO Tool Calling / Function Calling

Ollama now supports tool calling through the =/api/chat= endpoint with a =tools= parameter. Models like Llama 3.1, Mistral Nemo, and Command-R+ can invoke external functions.

*Implementation considerations:*
- Define tool schema format in elisp
- Parse =tool_calls= response field
- Execute tool functions and return results via =tool= role messages
- Potential integration with Emacs functions (file operations, shell commands, org-mode)

*References:* [[https://ollama.com/blog/tool-support][Ollama Tool Support Blog]]

*** TODO Web Search API Integration

Ollama provides a web search API at =https://ollama.com/api/web_search= requiring an API key. Could enable grounded responses with current information.

*Implementation considerations:*
- Add =ollama-buddy-web-search-api-key= customization
- Implement =ollama-buddy-web-search= function
- Option to automatically augment prompts with search results
- Display source URLs in responses

*References:* [[https://ollama.com/blog/web-search][Ollama Web Search Blog]]

*** TODO Image Generation Support

Ollama v0.14+ supports local image generation with models like Z-Image Turbo (6B) and FLUX.2 Klein. Currently macOS only, Windows/Linux planned.

*Implementation considerations:*
- Detect image generation models (=x/= prefix)
- Handle image output (save to file, display in buffer)
- Support generation parameters (width, height, steps, seed)
- Integration with org-mode inline images

*References:* [[https://ollama.com/blog/image-generation][Ollama Image Generation Blog]]

*** TODO Anthropic Messages API Compatibility

Ollama v0.14+ implements =/v1/messages= endpoint compatible with Anthropic's API format. This could simplify the existing =ollama-buddy-claude.el= module or enable new workflows.

*Implementation considerations:*
- Option to route Claude-format requests through local Ollama
- Support for tool_use and tool_result content blocks
- Extended thinking parameter support
- Token counting endpoint (=/v1/messages/count_tokens=)

*References:* [[https://docs.ollama.com/api/anthropic-compatibility][Ollama Anthropic Compatibility Docs]]

*** TODO Enhanced Token Metrics from API Response

The API now returns detailed timing and token metrics in responses:
- =prompt_eval_count= - input token count
- =eval_count= - output token count
- =prompt_eval_duration= - prompt processing time (nanoseconds)
- =eval_duration= - generation time (nanoseconds)
- =total_duration= - total execution time
- =load_duration= - model loading time

*Implementation considerations:*
- Parse and display all timing metrics
- Calculate and show tokens/second rate
- Track model load times separately
- Historical performance comparison per model

*** TODO Think Parameter for Reasoning Models

The API now supports a =think= parameter to enable thinking/reasoning mode for compatible models.

*Implementation considerations:*
- Add =ollama-buddy-param-think= customization
- Toggle thinking mode from transient menu
- Distinguish thinking output from final response in display

*** TODO Improved Context Window Tracking

Verify current implementation uses the returned context information correctly. The API provides =context= field for continuation and models report their context window size.

*Implementation considerations:*
- Validate context percentage calculation against actual API values
- Warn when approaching context limits
- Option to automatically truncate/summarize conversation

*** TODO Keep-Alive Parameter

The API supports =keep_alive= parameter (default 5m) to control how long models stay loaded in memory.

*Implementation considerations:*
- Add =ollama-buddy-param-keep-alive= customization
- Presets for different usage patterns (quick queries vs long sessions)
- Display currently loaded models and their timeout status

*** TODO Model Scheduling Improvements

Ollama v0.11+ introduced improved model scheduling that reduces OOM crashes and maximizes GPU utilization when running multiple models.

*Implementation considerations:*
- Leverage for multishot mode improvements
- Better handling of concurrent model requests
- Status display for GPU memory utilization

* Alternative LLM based packages

To the best of my knowledge, there are currently a few Emacs packages related to Ollama, though the ecosystem is still relatively young:

1. *llm.el* (by Andrew Hyatt)
   - A more general LLM interface package that supports Ollama as one of its backends
   - GitHub: https://github.com/ahyatt/llm
   - Provides a more abstracted approach to interacting with language models
   - Supports multiple backends including Ollama, OpenAI, and others

2. *gptel* (by Karthik Chikmagalur)
   - While primarily designed for ChatGPT and other online services, it has experimental Ollama support
   - GitHub: https://github.com/karthink/gptel
   - Offers a more integrated chat buffer experience
   - Has some basic Ollama integration, though it's not the primary focus

3. *chatgpt-shell* (by xenodium)
   - Primarily designed for ChatGPT, but has some exploration of local model support
   - GitHub: https://github.com/xenodium/chatgpt-shell
   - Not specifically Ollama-focused, but interesting for comparison

4. *ellama* (by s-kostyaev)
   - A comprehensive Emacs package for interacting with local LLMs through Ollama
   - GitHub: https://github.com/s-kostyaev/ellama
   - Features deep org-mode integration and extensive prompt templates
   - Offers streaming responses and structured interaction patterns
   - More complex but feature-rich approach to local LLM integration

* Alternative package comparison

Let's compare ollama-buddy to the existing solutions:

1. *llm.el*
   
   - *Pros*:
     
     - Provides a generic LLM interface
     - Supports multiple backends
     - More abstracted and potentially more extensible
       
   =ollama-buddy= is more:
   
   - Directly focused on Ollama
   - Lightweight and Ollama-native
   - Provides a more interactive, menu-driven approach
   - Simpler to set up for Ollama specifically

2. *gptel*
   
   - *Pros*:
     
     - Sophisticated chat buffer interface
     - Active development
     - Good overall UX
       
   =ollama-buddy= differentiates by:
   
   - Being purpose-built for Ollama
   - Offering a more flexible, function-oriented approach
   - Providing a quick, lightweight interaction model
   - Having a minimal, focused design

3. *chatgpt-shell*
   
   - *Pros*:
     
     - Mature shell-based interaction model
     - Rich interaction capabilities
       
   =ollama-buddy= stands out by:
   
   - Being specifically designed for Ollama
   - Offering a simpler, more direct interaction model
   - Providing a quick menu-based interface
   - Having minimal dependencies

4. *ellama*
   
   - *Pros*:
     - Tight integration with Emacs org-mode
     - Extensive built-in prompt templates
     - Support for streaming responses
     - Good documentation and examples

   =ollama-buddy= differs by:
   - Having a simpler, more streamlined setup process
   - Providing a more lightweight, menu-driven interface
   - Focusing on quick, direct interactions from any buffer
   - Having minimal dependencies and configuration requirements

* Issues

Report issues on the [[https://github.com/captainflasmr/ollama-buddy/issues][GitHub Issues page]]

** DONE Error when model numbers is more than alphabet letters #15

If the local ollama server has more than 26 models the create-intro-message function produce an error due to the fact that alphabet letter are just 26.

** DONE Not possible to pass path to image? #14

in the ollama cli when using models with vision support (ex. ollama run gemma3:4b) you can say something like "extract the text from this image: /home/user/Downloads/screenshot.png") then the ollama cli will detect this valid path and pass it to the gemma model as a file parameter somehow and the model will then be able to analize the actual file, when using ollama-buddy this is not possible

** DONE Role stuff is difficult to understand #13

I am simply trying to add a new role which should by default add the missing turkish letters to any of the paragraph of text I send to it. However, ollama-buddy's Role stuff is quite confusing to me. How do I set it up? I see no explanation about what I am doing when I try to add a role like that.

** DONE Getting constant lisp nesting exceeds errors in latest versions #12

I just updated ollama-buddy's source to its latest version, here's my use-package config:

#+begin_src elisp
(use-package ollama-buddy
  :if (file-directory-p "/home/user/.local/gits/emacs/ollama-buddy")
  :vc (:url "file:///home/user/.local/gits/emacs/ollama-buddy"
			:rev "870233e5d83da133b42fa7e9f57fcd6909f74502")
  :config
  (setq ollama-buddy-current-model "gemma3:12b-it-qat")
  (setq ollama-buddy-menu-columns 4)
  (setq ollama-buddy-host "localhost")
  (setq ollama-buddy-port 11434)
  (setq ollama-buddy-connection-check-interval 1))
#+end_src

After byte-compiling, I cannot use ollama-buddy. When I send a prompt to my ollama backend, I constantly get Debugger entered--Lisp error: (excessive-lisp-nesting 1601). Older versions of ollama-buddy have been working without such an error.

** DONE stopping the thinking text output #11 #2

As for context size, all I want the model to report it's max size it can handle, what was the context size received, what it considered as input context, what it dropped and output context size for each call. Visually, all I was want is red, green amber colouring to know where it cut off(red), was ok(green) or slightly exceeded the context size(amber). whatever the model reports on the metrics(if at all, it does track that on a per API call)

** DONE stopping the thinking text output #11

Thanks for the package, I'm slowly getting into the grove of using it regularly.

I have a laptop without graphics cards for LLM work. I get about 2-3 tokens/sec. The newer models seem to be doing reasoning/thinking text output that are huge and takes up time. Is there a way to toggle it off during the session or through config on a per model basis. Either as a binary switch or a config parameter.

While at it, how do I set the context length to larger sizes than the default one and how do I check if it's respected? On a per model basis, when you report the token stats, would you consider plugging in info about temp, top-k, context length that was active during the session?

* Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Open a pull request

* License

[[https://opensource.org/licenses/MIT][MIT License]]

* Acknowledgments

- [[https://ollama.ai/][Ollama]] for making local LLM inference accessible
- Emacs community for continuous inspiration
