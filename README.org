#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+startup: showall

#+attr_org: :width 300px
#+attr_html: :width 50%
[[file:img/ollama-buddy-youtube-banner_001.jpg]]

* Ollama Buddy

An Emacs package for interacting with local LLMs via [[https://ollama.ai/][Ollama]], with support for remote providers (OpenAI, Claude, Gemini, Grok, Copilot, Codestral, DeepSeek, OpenRouter) and Ollama Cloud models. Requires Emacs 28.1+.

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

That's it. Start =ollama serve=, open Emacs, press =C-c o=, select =[o]= to open chat, and go.

https://www.youtube.com/@OllamaBuddyforEmacs

** Tool Calling Demo

LLMs can invoke Emacs functions: read/write files, execute shell commands, search buffers, and more.

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_034.gif]]

** Advanced Tool Calling Demo

Demonstrating advanced ollama tool calling with multiple tools working together.

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_035.gif]]

** Quick Demo

Submit queries, swap models, view token usage and statistics:

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_011.gif]]

* Features

- *Tool Calling* -- LLMs can invoke registered Emacs functions (file ops, shell commands, buffer search, calculations). Safe mode restricts to read-only tools.
- *Multiple Providers* -- Local Ollama, Ollama Cloud, OpenAI, Claude, Gemini, Grok, GitHub Copilot, Codestral, DeepSeek, OpenRouter
- *Web Search* -- Inline =@search(query)= syntax for real-time web search via Ollama's API
- *Interactive Menus* -- Transient popup menus with role-based command grouping
- *Roles & Presets* -- Switchable command configurations (developer, writer, tutor, documenter, custom)
- *Sessions & History* -- Save/load conversations, prompt history navigation
- *File Attachments* -- Attach files and images (vision models) to conversations
- *Model Management* -- Pull, delete, copy models; categorized recommendations; cloud model support
- *Multishot* -- Send the same prompt to multiple models simultaneously
- *Parameters* -- Full control over temperature, top_k, top_p, and all Ollama API options
- *Fabric Patterns* -- Integration with [[https://github.com/danielmiessler/fabric][Fabric]] prompt patterns
- *Token Tracking* -- Real-time token usage statistics with graphs
- *RAG* -- Index local documents and source code, retrieve relevant context for conversations via semantic search. Inline =@rag(query)= syntax for automatic context attachment.
- *Lightweight* -- No external dependencies; a minified 200-line version (=ollama-buddy-mini=) is also available

* What's New (2.7.1)

- *RET to set system prompt in org list buffers* -- Pressing =RET= on a prompt/pattern heading in the =*User System Prompts*=, =*Awesome ChatGPT Prompts List*=, or =*Fabric Patterns List*= browse buffers instantly sets it as the current system prompt, with a hint comment at the top of each buffer.

* What's New (2.7.0)

- *Airplane Mode* -- Toggle =C-c != to completely isolate the package from the internet. While active, all external LLM providers (OpenAI, Claude, Gemini, Grok, Copilot, Codestral, DeepSeek, OpenRouter) and Ollama cloud models are blocked. Only local Ollama models are available. Web search is also disabled. The =✈= indicator appears in the header line, and attempting to send to an internet-requiring model shows a minibuffer message. Toggle via =C-c !=, the transient menu Actions section, or =M-x ollama-buddy-toggle-airplane-mode=.

* What's New (2.6.0)

- *Remote provider refactor* -- All remote provider modules now share common infrastructure via =ollama-buddy-remote.el=, eliminating ~1400 lines of duplicated code. Adding a new OpenAI-compatible provider now requires ~100 lines instead of ~350.
- *DeepSeek provider* -- New =ollama-buddy-deepseek.el= module (prefix: =d:=) with support for =deepseek-chat= and =deepseek-reasoner= models.
- *OpenRouter provider* -- New =ollama-buddy-openrouter.el= module (prefix: =r:=) providing access to 400+ models from all major providers through a single API, with optional model filtering.

See [[file:CHANGELOG.org]] for full history.

* Installation

** MELPA Simple

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** With Remote Providers

#+begin_src elisp
(use-package ollama-buddy
  :ensure t
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  ;; e.t.c
  :config
  (require 'ollama-buddy-openai nil t)
  ;; e.t.c)
#+end_src

** Manual

#+begin_src shell
git clone https://github.com/captainflasmr/ollama-buddy.git
#+end_src

#+begin_src elisp
(add-to-list 'load-path "path/to/ollama-buddy")
(require 'ollama-buddy)
(global-set-key (kbd "C-c o") #'ollama-buddy-role-transient-menu)
(global-set-key (kbd "C-c O") #'ollama-buddy-transient-menu-wrapper)
#+end_src

** Manual Setup with Presets and User Prompts

To fully utilize role-based presets and custom system prompts, you’ll need to copy two important directories into your local Emacs configuration:

1. *=ollama-buddy-presets=* – Contains pre-defined role presets (e.g., developer, writer, tutor) as =.el= files.
2. *=ollama-buddy-user-prompts=* – Where your *saved* system prompts are stored (created automatically when using =M-x ollama-buddy-user-prompts-save=).

*** Step-by-Step Setup

Run the following shell commands in your terminal:

#+begin_src bash
cd ~/.emacs.d

# extract out the ollama-buddy-presets and ollama-buddy-user-prompts
curl -sL https://api.github.com/repos/captainflasmr/ollama-buddy/tarball/main | \
    tar -xzf - --transform='s|[^/]*/||' \
        --wildcards \
        '*/ollama-buddy-presets/*' \
        '*/ollama-buddy-user-prompts/*'
#+end_src

* My current configuration

#+begin_src elisp
(use-package ollama-buddy
  :demand t
  :bind
  ;; ("C-c o" . ollama-buddy-menu)
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  
  ;; overall default model
  (setq ollama-buddy-default-model "glm-4.7:cloud")
  
  ;; key setup
  (setq ollama-buddy-openai-api-key
        (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (setq ollama-buddy-claude-api-key
        (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (setq ollama-buddy-gemini-api-key
        (auth-source-pick-first-password :host "ollama-buddy-gemini" :user "apikey"))
  (setq ollama-buddy-grok-api-key
        (auth-source-pick-first-password :host "ollama-buddy-grok" :user "apikey"))
  (setq ollama-buddy-codestral-api-key
        (auth-source-pick-first-password :host "ollama-buddy-codestral" :user "apikey"))
  (setq ollama-buddy-openrouter-api-key
        (auth-source-pick-first-password :host "ollama-buddy-openrouter" :user "apikey"))
  (setq ollama-buddy-deepseek-api-key
        (auth-source-pick-first-password :host "ollama-buddy-deepseek" :user "apikey"))
  (setq ollama-buddy-cloud-api-key
        (auth-source-pick-first-password :host "ollama-buddy-cloud" :user "apikey"))
  (setq ollama-buddy-web-search-api-key
        (auth-source-pick-first-password :host "ollama-buddy-web-search" :user "apikey"))

  ;; (require 'ollama-buddy-openai nil t)
  ;; (require 'ollama-buddy-claude nil t)
  ;; (require 'ollama-buddy-gemini nil t)
  ;; (require 'ollama-buddy-grok nil t)
  ;; (require 'ollama-buddy-codestral nil t)
  ;; (require 'ollama-buddy-copilot nil t)
  ;; (require 'ollama-buddy-openrouter nil t)
  ;; (require 'ollama-buddy-deepseek nil t)

  ;; other
  (setq ollama-buddy-full-welcome-enabled nil)

  ;; setup default custom menu for preferred models
  (ollama-buddy-update-menu-entry 'refactor-code     :model "qwen3-coder:480b-cloud")
  (ollama-buddy-update-menu-entry 'git-commit        :model "glm-4.7:cloud")
  (ollama-buddy-update-menu-entry 'describe-code     :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'dictionary-lookup :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'synonym           :model "minimax-m2.1:cloud")
  (ollama-buddy-update-menu-entry 'proofread         :model "deepseek-v3.1:671b-cloud")

  ;; dired integration
  (with-eval-after-load 'dired
    (define-key dired-mode-map (kbd "C-c C-a") #'ollama-buddy-dired-attach-marked-files)))
#+end_src

* Key Bindings (Chat Buffer)

| Key       | Action                        |
|-----------+-------------------------------|
| C-c C-c   | Send prompt                   |
| C-c RET   | Send prompt (alt)             |
| C-c C-k   | Cancel request                |
| C-c m     | Change model                  |
| C-c M     | Manage models                 |
| C-c O     | Main transient menu           |
| M-p / M-n | Browse prompt history         |
| C-c a     | Attach file                   |
| C-c 0     | Clear attachments             |
| C-c r     | RAG transient menu            |
| C-c C-s   | Show system prompt            |
| C-c C-r   | Reset system prompt           |
| C-c ~     | Select response tone          |
| C-c v     | Set keep-alive duration       |
| C-c !     | Toggle airplane mode          |
| C-c e     | Switch backend (network/curl) |

* Screenshots & Demos

See the [[file:README-full.org][full README]] for all 30+ demos with descriptions, or browse the [[https://www.youtube.com/@OllamaBuddyforEmacs][YouTube channel]].

* ollama API Support Tables
** Core API Endpoints

| API Endpoint              | Method | Purpose                            | Supported | Notes                                          |
|---------------------------+--------+------------------------------------+-----------+------------------------------------------------|
| =/api/chat=               | POST   | Send chat messages to a model      | Yes       | Core feature with streaming, tools, vision     |
| =/api/generate=           | POST   | Generate text without chat context | No        | Not planned - chat covers all use cases        |
| =/api/tags=               | GET    | List available local models        | Yes       | Used for model management                      |
| =/api/show=               | POST   | Show model information             | Yes       | Model info display                             |
| =/api/pull=               | POST   | Pull a model from Ollama library   | Yes       | Async pull with progress                       |
| =/api/push=               | POST   | Push a model to the Ollama library | No        | Not planned                                    |
| =/api/delete=             | DELETE | Delete a model                     | Yes       | Model management                               |
| =/api/copy=               | POST   | Copy a model                       | Yes       | Model management                               |
| =/api/create=             | POST   | Create a model from Modelfile/GGUF | Yes       | GGUF import supported                          |
| =/api/ps=                 | GET    | List running models                | Yes       | Shows loaded models in management buffer       |
| =/api/embed=              | POST   | Generate embeddings from text      | Yes       | RAG module via =ollama-buddy-rag.el= (v2.5.0+) |
| =/api/version=            | GET    | Get Ollama version                 | Yes       | Displayed on Model Management page             |
| =HEAD /api/blobs/:digest= | HEAD   | Check if blob exists               | No        | Not needed for current functionality           |
| =POST /api/blobs/:digest= | POST   | Push a blob                        | No        | Not needed for current functionality           |
| Cancel requests           | Custom | Terminate ongoing request          | Yes       | =C-c C-k= in chat buffer                       |

** Chat API Parameters (=/api/chat=)

| Parameter    | Supported | Notes                                                            |
|--------------+-----------+------------------------------------------------------------------|
| =model=      | Yes       | Per-command or global via =ollama-buddy-default-model=           |
| =messages=   | Yes       | Full conversation history support                                |
| =tools=      | Yes       | Tool calling via =ollama-buddy-tools.el= (v2.0.0+)               |
| =think=      | Yes       | Reasoning model support with visibility toggle                   |
| =format=     | Partial   | JSON mode not yet exposed                                        |
| =options=    | Yes       | Full implementation of all model parameters                      |
| =stream=     | Yes       | Toggleable with =ollama-buddy-toggle-streaming=                  |
| =keep_alive= | Yes       | Set via =ollama-buddy-set-keepalive= or =ollama-buddy-keepalive= |

** Generate API Parameters (=/api/generate=)

| Parameter    | Supported | Notes                                                   |
|--------------+-----------+---------------------------------------------------------|
| =model=      | N/A       | Generate endpoint not used                              |
| =prompt=     | N/A       | Chat endpoint used instead                              |
| =suffix=     | N/A       | Could be added via =ollama-buddy-set-suffix=            |
| =images=     | Yes*      | Vision supported via chat endpoint                      |
| =format=     | No        | Not currently exposed                                   |
| =options=    | Yes       | Full parameter support                                  |
| =system=     | Yes       | =ollama-buddy-set-system-prompt=                        |
| =template=   | No        | Uses model defaults                                     |
| =stream=     | Yes       | Streaming enabled by default                            |
| =raw=        | No        | Not currently needed                                    |
| =keep_alive= | N/A       | Generate endpoint not used (chat endpoint used instead) |
| =context=    | N/A       | Deprecated - using messages array instead               |

** Model Options Parameters

| Parameter Group         | Parameters                                                                                     | Supported |
|-------------------------+------------------------------------------------------------------------------------------------+-----------|
| *Temperature Controls*  | =temperature=, =top_k=, =top_p=, =min_p=, =typical_p=                                          | Yes       |
| *Repetition Controls*   | =repeat_last_n=, =repeat_penalty=, =presence_penalty=, =frequency_penalty=                     | Yes       |
| *Advanced Sampling*     | =mirostat=, =mirostat_tau=, =mirostat_eta=, =penalize_newline=, =stop=                         | Yes       |
| *Resource Management*   | =num_keep=, =seed=, =num_predict=, =num_ctx=, =num_batch=                                      | Yes       |
| *Hardware Optimization* | =numa=, =num_gpu=, =main_gpu=, =low_vram=, =vocab_only=, =use_mmap=, =use_mlock=, =num_thread= | Yes       |

** Additional API Features

| Feature                     | Supported | Notes                                               |
|-----------------------------+-----------+-----------------------------------------------------|
| Streaming responses         | Yes       | Default mode, toggleable                            |
| Tool calling                | Yes       | Built-in tools + custom tool registration (v2.0.0+) |
| Vision/Images               | Yes       | Base64 image encoding for multimodal models         |
| Structured outputs (JSON)   | No        | Not yet exposed                                     |
| Reproducible outputs (seed) | Yes       | Via parameters                                      |
| Load/Unload models          | Yes       | Via model management buffer                         |
| Cloud models                | Yes       | Via =ollama-buddy-cloud-models= (v1.1+)             |
| Thinking/Reasoning models   | Yes       | =think= parameter with visibility toggle            |

* Roadmap

** Future Considerations

*** Structured Outputs (JSON Schema)

The =format= parameter supports JSON schema for structured responses.

*Implementation considerations:*
- Define output schemas per command
- Integration with org-mode tables
- Structured data extraction workflows

*** External LLMs to use tooling

In this case I might not ever do this, I am always mindful of burning through tokens.

*** Anthropic Messages API Compatibility

Ollama v0.14+ implements =/v1/messages= endpoint compatible with Anthropic's API format.

*Implementation considerations:*
- Option to route Claude-format requests through local Ollama
- Could simplify =ollama-buddy-claude.el= or provide fallback
- Support for tool_use and tool_result content blocks

*** Image Generation Support

Ollama v0.14+ supports local image generation with models like Z-Image Turbo (6B) and FLUX.2 Klein.

*Implementation considerations:*
- Detect image generation models
- Handle image output (save to file, display in buffer)
- Support generation parameters (width, height, steps, seed)
- Integration with org-mode inline images

* Documentation

- [[file:docs/ollama-buddy.org][Manual]] -- Full reference manual
- [[file:README-full.org][Full README]] -- Detailed README with all demos, tutorials, transient menu guide, and role setup
- [[file:README-features.org][Feature History]] -- Detailed feature descriptions as they were added
- [[file:CHANGELOG.org][Changelog]] -- Version-by-version changes
