#+title: Change Log for ollama-buddy
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+todo: TODO DOING | DONE
#+startup: showall

This document contains the release notes for each tagged commit on the
project's main git repository: [[https://github.com/captainflasmr/ollama-buddy

* Versions

** <2026-02-02 Sun> *1.2.1*

Added attachment count indicator to header-line

- Header-line now displays ðŸ“ŽN (paperclip + count) when files are attached
- Provides constant visual reminder of attachment context before sending
- Indicator appears in bold near the beginning of the status line
- Empty when no attachments are present

** <2026-02-01 Sat> *1.2*

Added GitHub Copilot integration

- Added =ollama-buddy-copilot.el= for GitHub Copilot Chat API support
- Implemented GitHub OAuth device flow authentication (no API key needed)
- =M-x ollama-buddy-copilot-login= opens browser for secure GitHub authentication
- =M-x ollama-buddy-copilot-logout= removes saved authentication
- =M-x ollama-buddy-copilot-status= checks authentication status
- OAuth token persisted to =~/.emacs.d/ollama-buddy-copilot-token.json=
- Uses "p:" prefix to identify Copilot models
- Supports OpenAI (gpt-4.1, gpt-4o, gpt-5), Anthropic (claude-sonnet-4, claude-opus-4.5), Google (gemini-2.5-pro, gemini-3-pro), and xAI (grok-code-fast-1) models
- Supports conversation history and system prompts
- Header line shows "p" indicator when Copilot provider is loaded

To use GitHub Copilot:
1. Ensure you have an active GitHub Copilot subscription
2. Run =M-x ollama-buddy-copilot-login=
3. Enter the code shown in your browser at github.com/login/device
4. Select a Copilot model with =C-c m= (e.g., p:gpt-4o)

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (require 'ollama-buddy-copilot nil t))
#+end_src

** <2026-01-31 Fri> *1.1.6*

Removed model status buffer

- Removed =ollama-buddy-show-model-status= function and =*Ollama Model Status*= buffer
- Removed =C-c v= keybinding and transient menu entry
- Model information is available in the model management buffer (=C-c W=)

** <2026-01-31 Fri> *1.1.5*

Added global system prompt for consistent response formatting

- Added =ollama-buddy-global-system-prompt= customization with sensible default instructing models to avoid markdown tables and use plain prose
- Added =ollama-buddy-global-system-prompt-enabled= toggle (default: t) to enable/disable the global prompt
- Global prompt is prepended to session-specific system prompts, providing baseline formatting across all models and providers
- Added =ollama-buddy-toggle-global-system-prompt= command to quickly toggle the feature
- Added toggle to transient menu under "Display Toggle" (key: 9)
- Works with all providers: Ollama, OpenAI, Claude, Gemini, Grok, and Codestral

This feature helps ensure consistent response formatting across different models, avoiding issues like unexpected markdown tables in responses.

Consolidated model management

- Moved "Recommended Models (Pull)" section into =C-c W= model management buffer
- Removed =C-c A= (Detailed Info) keybinding and =ollama-buddy-show-interface-info= function
- Updated welcome screen to reference =C-c W= for model management

** <2026-01-31 Fri> *1.1.4*

Header-line and keybinding improvements

- Added =C-c RET= keybinding to send prompts (matches gptel convention)
- Simplified header-line: removed backend indicator (N/C)
- Changed header-line "Markdown" indicator to "MD" for compactness
- Fixed markdown to org heading conversion: MD headings now offset by 2 levels so H1 becomes org level 3, properly nesting under the =** [MODEL] RESPONSE= heading

** <2026-01-31 Fri> *1.1.3*

Improved chat UX and simplified codebase

- Added =ollama-buddy-auto-scroll= customization option (default: nil)
- When nil, cursor stays in place during streaming output, allowing manual scrolling
- When t, buffer auto-scrolls to follow new output (previous behavior)
- Added =ollama-buddy-pulse-response= option (default: t) to flash/pulse the response text when streaming completes, providing visual feedback similar to gptel
- Removed model name coloring feature (hash-based highlighting)
- Removed =ollama-buddy-highlight-models-enabled=, =ollama-buddy-toggle-model-highlighting=
- Simplifies codebase and may improve org-mode buffer performance

** <2026-01-30 Thu> *1.1.2*

Simplified chat interface

- Streamlined welcome screen showing only essential commands
- Full interface info (models, commands) moved to separate =*Ollama Buddy Info*= buffer via =C-c A=
- Model selection now shows counts: =C-c m (10 ollama / 4 online)= and =C-u C-c m (7 cloud)=
- Header line displays bold prefix characters for loaded external providers: =acgks=
- Welcome screen lists enabled online LLM providers (e.g., "Online: a: OpenAI | c: Claude")

** <2026-01-28 Tue> *1.1*

Added Ollama Cloud Models support

- Cloud models (running on ollama.com infrastructure) now work seamlessly
- =ollama-buddy-cloud-signin= to automatically open browser for authentication
- Cloud models are proxied through the local Ollama server which handles authentication
- Use =C-u C-c m= or transient menu "Model > Cloud" to select cloud models
- Status line shows â˜ indicator when using a cloud model
- Available cloud models include: qwen3-coder:480b-cloud, deepseek-v3.1:671b-cloud, gpt-oss:120b-cloud, minimax-m2.1:cloud, and more

To use cloud models:
1. Sign in once with =M-x ollama-buddy-cloud-signin= (opens browser automatically)
2. Select a cloud model with =C-u C-c m=
3. Use normally - the local Ollama server handles cloud authentication

#+begin_src elisp
;; Cloud models work with no additional configuration
;; Just sign in once via the transient menu: Cloud Auth > Sign In
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** <2025-12-11 Thu> *1.0.2*

Added Mistral Codestral integration

- Added ollama-buddy-codestral.el for Mistral Codestral API support
- Implemented model handler registration for seamless switching between local Ollama and cloud-based Codestral models
- Supports async streaming responses with real-time token counting
- Follows the same integration pattern as other remote AI providers (OpenAI, Claude, Gemini, Grok)
- Uses "s:" prefix to identify Codestral models in the model list

** <2025-06-21> *0.13.1*

Refactored content register processing to be more efficient and added a new Emacs package brainstorming prompt file.

** <2025-06-15> *0.13.0*

Added curl communication backend with fallback support

- Added ollama-buddy-curl.el as separate backend implementation
- Implemented backend dispatcher system in ollama-buddy-core.el
- Updated all async functions to use backend dispatcher
- Added curl backend validation and testing functions
- Maintained full compatibility with existing network process backend

** <2025-05-31> *0.12.1*

Optimized Unicode escape function to fix blocking with large file attachments

- Fixed UI blocking when sending large attached files
- Used temp buffer with delete-char/insert instead of repeated concat calls
- Reduced processing time from minutes to milliseconds for large payloads

** <2025-05-22> *0.12.0*

Full system prompt in the status bar replaced with a more meaningful simple role title

- Added system prompt metadata tracking with title, source, and timestamp registry
- Implemented automatic title extraction and unified completing-read interface
- Enhanced fabric/awesome prompt integration with proper metadata handling
- Improved transient menu organization and org-mode formatting with folding
- Added system prompt history display and better error handling for empty files
- Transient menu has been simplified and reorganised

** <2025-05-21> *0.11.1*

Quite a bit of refactoring to generally make this project more maintainable and I have added a starter kit of user prompts.

- Color System Reworking
  - Removed all model color-related functions and variables
  - Removed dependency on =color.el=
  - Replaced with =highlight-regexp= and hashing to =^font-lock= faces, so now using a more native built-in solutions for model colouring rather than shoe-horning in overlays.

- UI Improvements
  - Simplified the display system by leveraging Org mode
  - Added org-mode styling for output buffers
  - Added =org-hide-emphasis-markers= and =org-hide-leading-stars= settings
  - Changed formatting to use Org markup instead of text properties
  - Converted plain text headers to proper Org headings
  - Replaced color properties with Org emphasis (bold)

- History Management Updates
  - Streamlined history editing functionality
  - Improved model-specific history editing
  - Refactored history display and navigation

- System Prompts
  - Added library of system prompts in these categories:
    - analysis (3 prompts)
    - coding (5 prompts)
    - creative (3 prompts)
    - documentation (3 prompts)
    - emacs (10 prompts)
    - general (3 prompts)
    - technical (3 prompts)
    - writing (3 prompts)

** <2025-05-19> *0.11.0*

Added user system prompts management

- You can now save, load and manage system prompts
- Created new transient menu for user system prompts (C-c s)
- Organized prompts by categories with org-mode format storage
- Supported prompt editing, listing, creation and deletion
- Updated key bindings to integrate with existing functionality
- Added prompts directory customization with defaults
  
** <2025-05-14> *0.10.0*

Added file attachment system for including documents in conversations

- Added file attachment support with configurable file size limits (10MB default) and supported file types
- Implemented session persistence for attachments in save/load functionality  
- Added attachment context inclusion in prompts with proper token counting
- Created comprehensive attachment management commands:
  - Attach files to conversations
  - Show current attachments in dedicated buffer
  - Detach specific files
  - Clear all attachments
- Added Dired integration for bulk file attachment
- Included attachment menu in transient interface (C-c A)
- Updated help text to document new attachment keybindings
- Enhanced context calculation to include attachment token usage

** <2025-05-12> *0.9.50*

Added context size management and monitoring

- Added configurable context sizes for popular models (llama3.2, mistral, qwen, etc.)
- Implemented real-time context usage display in status bar
- Can display in text or bar display types
- Added context size thresholds with visual warnings
- Added interactive commands for context management:
  - =ollama-buddy-show-context-info=: View all model context sizes
  - =ollama-buddy-set-model-context-size=: Manually configure model context
  - =ollama-buddy-toggle-context-percentage=: Toggle context display
- Implemented context size validation before sending prompts
- Added token estimation and breakdown (history/system/current prompt)
- Added keybindings: C-c $ (set context), C-c % (toggle display), C-c C (show info)
- Updated status bar to show current/max context with fontification

** <2025-05-05> *0.9.44*

- Sorted model names alphabetically in intro message
- Removed multishot writing to register name letters

** <2025-05-05> *0.9.43*

Fix model reference error exceeding 26 models #15

Update =ollama-buddy= to handle more than 26 models by using prefixed combinations for model references beyond 'z'. This prevents errors in =create-intro-message= when the local server hosts a large number of models.

** <2025-05-03> *0.9.42*

Added the following to recommended models:

- qwen3:0.6b
- qwen3:1.7b
- qwen3:4b
- qwen3:8b

and fixed pull model

** <2025-05-02> *0.9.41*

Refactored model prefixing again so that when using only ollama models no prefix is applied and is only applied when online LLMs are selected (for example claude, chatGPT e.t.c)

I think this makes more sense and is cleaner for I suspect the majority who may use this package are probably more interested in just using ollama models and the prefix will probably be a bit confusing.

This could be a bit of a breaking change once again I'm afraid for those ollama users that have switched and are now familiar with prefixing "o:", sorry!

** <2025-05-02> *0.9.40*

Added vision support for those ollama models that can support it!

** <2025-04-29> *0.9.38*

Added model unloading functionality to free system resources

- Add unload capability for individual models via the model management UI
- Create keyboard shortcut (C-c C-u) for quick unloading of all models
- Display running model count and unload buttons in model management buffer
  
** <2025-04-25> *0.9.37*

- Display modified parameters in token stats

** <2025-04-25> *0.9.36*

Added Reasoning/Thinking section visibility toggle functionality

** <2025-04-21> *0.9.35*

Added Grok support

** <2025-04-20> *0.9.33*

Fixed utf-8 encoding stream response issues from remote LLMs.

** <2025-04-19> *0.9.32*

Finished the remote LLM decoupling process, meaning that the core =ollama-buddy= logic is now not dependent on any remote LLM, and each remote LLM package is self-contained and functions as a unique extension.

** <2025-04-18> *0.9.31*

Refactored model prefixing logic and cleaned up

- Standardized model prefixing by introducing distinct prefixes for Ollama (=o:=), OpenAI (=a:=), Claude (=c:=), and Gemini (=g:=) models.
- Centralized functions to get full model names with prefixes across different model types.
- Removed redundant and unused variables related to model management.

Note that there may be some breaking changes here especially regarding session recall as all models will now have a prefix to uniquely identify their type.  For =ollama= recall, just edit the session files to prepend the ollama prefix of "o:"

** <2025-04-17> *0.9.30*

Added Gemini integration!

As with the Claude and ChatGPT integration, you will need to add something similar to them in your configuration. I currently have the following set up to enable access to the remote LLMs:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (ollama-buddy-claude-api-key
   (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (ollama-buddy-gemini-api-key
   (auth-source-pick-first-password :host "ollama-buddy-gemini" :user "apikey"))
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  (require 'ollama-buddy-gemini nil t))
#+end_src

Also with the previous update all the latest model names will be pulled, so there should be a full comprehensive list for each of the main remote AI LLMs!

** <2025-04-17> *0.9.23*

Refactored history and model management for remote LLMs

- Now pulling in latest model list for remote LLMs (so now ChatGPT 4.1 is available!)
- Removed redundant history and model management functions from =ollama-buddy-claude.el= and =ollama-buddy-openai.el=. Replaced them with shared implementations to streamline code and reduce duplication

** <2025-04-15> *0.9.22*

Enhanced session management

- Refactored =ollama-buddy-sessions-save= to autogenerate session names using timestamp and model.
- Improved session saving/loading by integrating org file handling.
- Updated mode line to display current session name dynamically.

** <2025-04-11> *0.9.21*

Add history edit/view toggle features, so effectively merging the former history display into the history edit functionality.

** <2025-04-04> *0.9.20*

- Added =ollama-buddy-awesome.el= to integrate Awesome ChatGPT Prompts.
  
** <2025-04-01> *0.9.17*

- Added link to =ollama-buddy= info manual from the chat buffer and transient menu as MELPA has now picked it up and installed it!

** <2025-03-28> *0.9.16*

- Added =ollama-buddy-fix-encoding-issues= to handle text encoding problems.
- Refactored and streamline fabric pattern description handling.
- Removed unused fabric pattern categories to enhance maintainability.

** <2025-03-28> *0.9.15*

- Implement asynchronous operations for model management
  - Introduce non-blocking API requests for fetching, copying, and deleting models
- Add caching mechanisms to improve efficiency
  - Cache model data to reduce redundant API calls
  - Manage cache expiration with timestamps and time-to-live settings
- Update status line to reflect ongoing background operations
- Ensure smooth user interaction by minimizing wait times and enhancing performance

** <2025-03-26> *0.9.13*

- Added automatic writing of last response to a register
- Added M-r to search through prompt history

** <2025-03-25> *0.9.12*

- Added experimental Claude AI support!
- removed curl and replaced with url.el for online AI integration

** <2025-03-24> *0.9.11*

Added the ability to toggle streaming on and off

- Added customization option to enable/disable streaming mode
- Implemented toggle function with keybindings (C-c x) and transient menu option
- Added streaming status indicator in the modeline

** <2025-03-22> *0.9.10*

Added experimental OpenAI support!

** <2025-03-22> *0.9.9.5*

Added texinfo documentation for future automatic installation through MELPA and created an Emacs manual.

** <2025-03-20> *0.9.9*

Intro message with model management options (select, pull, delete) and option for recommended models to pull

- Enhance model management and selection features
- Display models available for download but not yet pulled

** <2025-03-19> *0.9.8*

Added model management interface to pull and delete models

- Introduced `ollama-buddy-manage-models` to list and manage models.
- Added actions for selecting, pulling, stopping, and deleting models.

** <2025-03-19> *0.9.7*

- Added GGUF file import and Dired integration

** <2025-03-18> *0.9.6*

- Added a transient menu containing all commands currently presented in the chat buffer
- Added fabric prompting support, see https://github.com/danielmiessler/fabric
- Moved the presets to the top level so they will be present in the package folder
  
** <2025-03-17> *0.9.5*

Added conversation history editing

- Added functions to edit conversation history (=ollama-buddy-history-edit=, =ollama-buddy-history-save=, etc.).
- Updated =ollama-buddy-display-history= to support history editing.
- Added keybinding =C-c E= for history editing.

** <2025-03-17> *0.9.1*

New simple basic interface is available.

** <2025-03-17> *0.9.0*

Added command-specific parameter customization

- Added :parameters property to command definitions for granular control
- Implemented functions to apply and restore parameter settings
- Added example configuration to refactor-code command

** <2025-03-16> *0.8.5*

Added system prompt support for commands

- Introduced `:system` field to command definitions.
- Added `ollama-buddy-show-system-prompt` to view active system prompt.
- Updated UI elements to reflect system prompt status.

** <2025-03-14> *0.8.0*

Added system prompt support

- Added =ollama-buddy--current-system-prompt= variable to track system prompts
- Updated prompt area rendering to distinguish system prompts
- Modified request payload to include system prompt when set 
- Enhanced status bar to display system prompt indicator
- Improved help menu with system prompt keybindings
  
** <2025-03-13> *0.7.4*

Added model info command, update keybindings

- Added `ollama-buddy-show-raw-model-info` to fetch and display raw JSON details 
  of the current model in the chat buffer.
- Updated keybindings:
  - `C-c i` now triggers model info display.
  - `C-c h` mapped to help assistant.
  - Improved shortcut descriptions in quick tips section.
- Removed unused help assistant entry from menu.
- Changed minibuffer-prompt key from `?i` to `?b`.

** <2025-03-12> *0.7.3*

Added function to associate models with menu commands

- Added =ollama-buddy-add-model-to-menu-entry= autoload function
- Enabled dynamic modification of command-model associations

** <2025-03-12> *0.7.2*

Added menu model colours back in and removed some redundant code

** <2025-03-11> *0.7.1*

Added debug mode to display raw JSON messages in a debug buffer

- Created new debug buffer to show raw JSON messages from Ollama API
- Added toggle function to enable/disable debug mode (ollama-buddy-toggle-debug-mode)
- Modified stream filter to log and pretty-print incoming JSON messages
- Added keybinding C-c D to toggle debug mode
- Updated documentation in welcome message

** <2025-03-11> *0.7.0*

Added comprehensive Ollama parameter management

- Added customization for all Ollama option API parameters with defaults
- Only send modified parameters to preserve Ollama defaults
- Display active parameters with visual indicators for modified values
- Add keybindings and help system for parameter management
- Remove redundant temperature controls in favor of unified parameters

** <2025-03-10> *0.6.1*

Refactored prompt handling so each org header line should now always have a prompt for better export

- Added functionality to properly handle prompt text when showing/replacing prompts
- Extracted inline lambdas in menu actions into named functions
- Added fallback for when no default model is set

** <2025-03-08> *0.6.0*

Chat buffer now in org-mode

- Enabled =org-mode= in chat buffer for better text structure
- Implemented =ollama-buddy--md-to-org-convert-region= for Markdown to Org conversion
- Turn org conversion on and off
- Updated keybindings =C-c C-o= to toggle Markdown to Org conversion

** <2025-03-07> *0.5.1*

Added temperature control

- Implemented temperature control parameter
- Added menu commands for setting (T), resetting (0)
- Added keybindings (C-c t/T/0) for quick temperature adjustments
- Updated header line and prompt displays to show current temperature
- Included temperature info in welcome screen with usage guidance

** <2025-03-06> *0.5.0*

Implemented session management, so you can now save your conversations and bring them back with the relevant context and chat history!

- Chat history is now maintained separately for each model
- Added session new/load/save/delete/list functionality
- A switch in context can now be achieved by any of the following methods:
  - Loading a previous session
  - Creating a new session
  - Clearing history on the current session
  - Toggling history on and off

** <2025-03-04> *0.4.1*

Added a sparse version of =ollama-buddy= called =ollama-buddy-mini=, see the github repository for the elisp file and a description in =README-mini.org=

** <2025-03-03> *0.4.0*

Added conversation history support and navigation functions

- Implemented conversation history tracking between prompts and responses
- Added configurable history length limits and visual indicators
- Created navigation functions to move between prompts/responses in buffer

** <2025-03-02> *0.3.1*

Enhanced model colour contrast with themes, allowing =ollama-buddy-enable-model-colors= to be enabled by default.

** <2025-03-01> *0.3.0*

Added real-time token usage tracking and display

- Introduce variables to track token counts, rates, and usage history
- Implement real-time token rate updates with a timer
- Add a function to display token usage statistics in a dedicated buffer
- Allow toggling of token stats display after responses
- Integrate token tracking into response processing and status updates
- Ensure cleanup of timers and tracking variables on completion or cancellation

** <2025-02-28> *0.2.4*

Added model-specific color highlighting (experimental)

- Introduce `ollama-buddy-enable-model-colors` (default: nil) to toggle model-based color highlighting.
- Assign consistent colors to models based on string hashing.
- Apply colors to model names in the menu, status, headers, and responses.
- Add `ollama-buddy-toggle-model-colors` command to toggle this feature.

This feature improves UI clarity, making it easier to visually distinguish models.

** <2025-02-28> *0.2.3*

Added Prompt History Support

- Prompts are now integrated into the Emacs history mechanism which means they persist across sessions.  
- Use =M-p= to navigate prompt history, and =M-p= / =M-n= within the minibuffer to insert previous prompts.  

** <2025-02-27> *0.2.2*

Added support for role-based presets

- Introduced `ollama-buddy-roles-directory` for storing role preset files.
- Implemented interactive functions to manage roles:
  - `ollama-buddy-roles-switch-role`
  - `ollama-buddy-role-creator-create-new-role`
  - `ollama-buddy-roles-open-directory`
- Added ability to create and switch between role-specific commands.
- Updated menu commands to include role management options.

** <2025-02-26> *0.2.1*

added multishot execution with model selection

- Assign letters to models for quick selection
- Implement multishot mode for sequential requests to multiple models
- Store responses per model in registers named after assigned letters
- Display multishot progress in status
- Bind `C-c C-l` to trigger multishot prompt

** <2025-02-19> *0.2.0*

Improved prompt handling in chat buffer and simplified setup

- Chat buffer now more prompt based rather than ad-hoc using C-c C-c to send and C-c C-k to cancel
- Connection monitor now optional, ollama status visibility now maintained by strategic status checks simplifying setup.
- Can now change models from chat buffer using C-c C-m
- Updated intro message with ascii logo
- Suggested default "C-c o" for =ollama-buddy-menu=
- defcustom ollama-buddy-command-definitions now will work in the customization interface.

** <2025-02-13>

Models can be assigned to individual commands

- Set menu :model property to associate a command with a model
- Introduce `ollama-buddy-fallback-model` for automatic fallback if the specified model is unavailable.
- Improve `ollama-buddy--update-status-overlay` to indicate model substitution.
- Expand `ollama-buddy-menu` with structured command definitions using properties for improved flexibility.
- Add `ollama-buddy-show-model-status` to display available and used models.
- Refactor command execution flow to ensure model selection is handled dynamically.

** <2025-02-12>

- =ollama-buddy= updated in preparation for MELPA submission
- Removed C-c single key user keybinding as part of package definition and in the README gave guidance on defining a user keybinding to activate the ollama buddy menu
- Added =ellama= comparison description
- Activating and deactivating the =ollama= monitor process now users responsibility

** <2025-02-11>

Significant improvements and refactoring, particularly around connection handling, streaming responses, and status monitoring.

- Replace curl-based requests with native network processes
- Added customizatble ollama host and port  
- Added connection monitoring with automatic status updates
- Added permanently visible status showing connection state and current model
- Improve error handling for connection failures
- Refined AI assistant presentation

** <2025-02-07>

Increase menu columns to 4, add dictionary lookup and save chat options  

- Change `ollama-buddy-menu-columns` from 3 to 4  
- Rename "Describe code" menu key from `?d` to `?c`  
- Add dictionary lookup feature (`?d`)  
- Add synonym lookup feature (`?n`)  
- Add "Save chat" option (`?s`) to write chat buffer to a file  

** <2025-02-07>

Added query finished message.

** <2025-02-06>

- Initial release
- Basic chat functionality
- Menu-driven interface
- Region-based interactions
- Model switching support
