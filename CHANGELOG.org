#+title: Change Log for ollama-buddy
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+todo: TODO DOING | DONE
#+startup: showall

This document contains the release notes for each tagged commit on the
project's main git repository: [[https://github.com/captainflasmr/ollama-buddy

* Versions

** <2026-02-24 Mon> *2.9*

Configurable embedding endpoint for RAG — point the RAG module at any
OpenAI-compatible embedding service independently of your Ollama instance.

*** New Features

- *Configurable RAG embedding endpoint* -- Three new customization variables give
  full control over where embeddings are generated:
  - =ollama-buddy-rag-embedding-base-url= -- set to e.g.@: ="http://localhost:9000"=
    to use a separate embedding server; =nil= (default) keeps the existing
    behaviour of using the main Ollama host and port.
  - =ollama-buddy-rag-embedding-api-style= -- choose between =ollama= (default,
    POST =/api/embed=) and =openai= (POST =/v1/embeddings=) to match the API
    shape of the target server.
  - =ollama-buddy-rag-embedding-api-key= -- optional Bearer token for
    authenticated embedding services; leave =nil= for local servers.
- *OpenAI-compatible embedding services* -- RAG can now use dedicated embedding
  backends!

** <2026-02-24 Mon> *2.8.1*

Inline diff inspection and polish for in-buffer replace mode.

*** New Features

- *Inline diff view* (=C-c d=) -- After a rewrite streams in, pressing =C-c d= inserts the original text immediately below the new text in the same buffer.  Word-level differences are highlighted using =smerge-refine-regions=: green overlays mark added/changed words in the new text; red strikethrough overlays mark removed words in the original block.  Pressing =C-c d= again hides the view.  Accepting or rejecting the rewrite automatically removes the diff block.
- *Automatic "In-Buffer" tone* -- When in-buffer replace is active the system prompt is automatically prefixed with a clean-output instruction ("Return only the requested content.  No preamble, no introduction…"), minimising extraneous text in rewritten regions.
- *Code fence stripping* -- If the model wraps its response in a markdown code block (e.g.@: =```lisp … ```=), the fences are stripped before the text is placed in the buffer, leaving only the inner content.
- *=C-c W= keybinding* in the chat buffer to toggle in-buffer replace without opening the transient menu.
- *Tighter command prompts* -- All region-operating commands (Refactor, Git Commit, Describe Code, Dictionary Lookup, Synonym) now say "Return ONLY …" to suppress introductory phrases and commentary that look out of place when inserted directly into source code.
- Transient menu Actions section shows a live =In-Buffer Replace [ON]/[OFF]= toggle with =✎= indicator; the custom role menu footer shows the same toggle.

** <2026-02-22 Sun> *2.8.0*

Initial in-buffer replace mode — an early-stage feature that streams LLM output directly back into the source buffer instead of the chat buffer.

*** New Features

- *In-buffer replace mode (initial pass)* -- New =ollama-buddy-in-buffer-replace= toggle allows commands that operate on a selected region (refactor, proofread, etc.) to stream their response back into the source buffer in place, replacing the selection. This is an initial attempt at in-buffer editing; the core chat buffer workflow is completely unchanged when the toggle is off.
  - Toggle via transient menu Actions → =W=, or =M-x ollama-buddy-toggle-in-buffer-replace=
  - The =✎= indicator appears in the header line and prompt line when the mode is active
  - While streaming, a =​[Rewriting...]=​ placeholder (dimmed italic) replaces the selection, then streams in with a green highlight
  - After streaming completes, a minor mode activates with two keys: =C-c C-c= to accept (keep new text, remove highlight) and =C-c C-k= to reject (restore original text)
  - Cancellation mid-stream with =C-c C-k= stops the network process and restores the original selection
  - The mode line shows =​ [Rewrite?]=​ while a rewrite is pending decision
  - Only the output destination changes; prompt construction, model selection, system prompts, and command definitions are unaffected
  - New module =ollama-buddy-rewrite.el= loaded optionally via =(require 'ollama-buddy-rewrite nil t)=

** <2026-02-22 Sun> *2.7.3*

Native org-mode collapsible thinking blocks for reasoning models.

*** New Features

- *Collapsible thinking blocks* -- Reasoning model output (=<think>= tags and DeepSeek =message.thinking= API field) is now rendered as a native org-mode heading (=*** ✦ Think=) that folds with =outline-hide-subtree=. While the model streams its reasoning the content is visible under the heading; when the thinking ends the heading is automatically folded, and a =*** ✦ Response= heading is inserted to mark the start of the actual response. Use =C-c V= to toggle all thinking blocks in the buffer, or =TAB= on any =*** ✦ Think= heading line to toggle that block individually.
- *Thinking block visual demarcation* -- When a thinking block is expanded a blank line precedes the =*** ✦ Response= heading, giving clear visual separation between reasoning and response content.
- *Thinking model indicator* -- The =✦= symbol is shown in the status line header and prompt line when the current model supports thinking/reasoning.

** <2026-02-21 Fri> *2.7.2*

Enhanced model completing-read with rich metadata annotations.

*** New Features

- *Rich model annotations in completing-read* -- The model selection prompt (=C-c m=) now shows metadata alongside each model name:
  - *Local Ollama models*: parameter size (e.g. =8.0B=), quantization level (=Q4_K_M=), disk size (=4.6 GB=), and a =▶= indicator when the model is currently loaded
  - *Remote provider models*: provider label (e.g. =OpenAI=, =Anthropic=, =Google=), context window size (e.g. =128k ctx=, =1M ctx=, =200k ctx=)
  - *Claude models*: human-readable display name (e.g. =Claude 3 Haiku=, =Claude Sonnet 4.6=) fetched from the Anthropic API
  - *Gemini models*: display name (e.g. =Gemini 2.5 Flash=) and context window fetched live from the Google API
  - *All models*: existing capability indicators (=☁= cloud, =▶= running, =⚒= tools, =⊙= vision) retained
- *Static context window table* -- Built-in context window sizes for well-known models across OpenAI, Anthropic, Grok, DeepSeek, and Mistral/Codestral; falls back to live API data for Gemini

** <2026-02-20 Thu> *2.7.1*

System prompt selection directly from org browse buffers.

*** New Features

- *RET to set system prompt in org list buffers* -- Pressing =RET= on a level-2 heading in the =*User System Prompts*=, =*Awesome ChatGPT Prompts List*=, or =*Fabric Patterns List*= buffers immediately sets that entry as the current system prompt, without leaving the browse buffer or using the completion UI. A hint comment at the top of each buffer documents the binding. Category (level-1) headings produce a clear error message.

** <2026-02-19 Wed> *2.7.0*

Airplane mode for complete internet isolation.

*** New Features

- *Airplane Mode* -- New =ollama-buddy-toggle-airplane-mode= command (=C-c != / transient Actions =!=) completely isolates the package from the internet. When active:
  - All external LLM providers (OpenAI, Claude, Gemini, Grok, Copilot, Codestral, DeepSeek, OpenRouter) are blocked
  - Ollama cloud models are blocked and hidden from the Model Management buffer
  - Web search (=C-c / s= and =C-c / a=) is disabled
  - Only local Ollama models are available for model selection
  - Attempting to send a prompt to an internet-requiring model shows a minibuffer message
  - The =✈= indicator is shown in the chat buffer header line when active

** <2026-02-17 Mon> *2.6.0*

Refactor remote provider infrastructure into shared module.

*** Internal

- *=ollama-buddy-remote.el=* -- New shared infrastructure module for all remote LLM providers. Extracts common helper functions (prefix handling, API key verification, context building, message formatting, chat buffer preparation, response finalization, error handling, model registration) and a generic OpenAI-compatible send function.
- *Provider modules refactored* -- OpenAI, Grok, Codestral, Copilot, Claude, and Gemini modules now use shared functions from =ollama-buddy-remote.el=, eliminating ~1400 lines of duplicated code. Each OpenAI-compatible provider (OpenAI, Grok, Codestral) is now a thin configuration wrapper (~100-140 lines). Claude and Gemini retain their unique API formats but share all common infrastructure.
- *Duplicate =ollama-buddy-remote-models= declarations removed* -- Variable is now declared only in =ollama-buddy-core.el=.

*** New Providers

- *=ollama-buddy-deepseek.el=* -- DeepSeek integration (prefix: =d:=). Supports =deepseek-chat= and =deepseek-reasoner= models via the DeepSeek API.
- *=ollama-buddy-openrouter.el=* -- OpenRouter integration (prefix: =r:=). Access 400+ models from all major providers through a single API. Supports dynamic model fetching with optional regex filtering via =ollama-buddy-openrouter-model-filter=.

** <2026-02-16 Mon> *2.5.3*

Autocomplete for inline =@= commands and =/= slash commands in chat prompt.

*** New Features

- *=@= Autocomplete* -- Typing =@= in the prompt area triggers =completing-read= with available inline commands (=search=, =rag=, =file=). Inserts the chosen syntax template (e.g., =@search()=) with cursor positioned inside the parentheses. Commands are filtered based on loaded modules. Outside the prompt area, =@= inserts literally. Press =C-g= to cancel and insert a literal =@=.
- *=ollama-buddy-at-commands=* -- New customizable alist for defining available =@= commands and their templates, making it easy to add new inline commands.
- *Inline =@file(path)= Syntax* -- Use =@file(/path/to/file)= in prompts to attach files to the conversation context inline. Paths are expanded and the file content is attached on send. Skips already-attached and non-existent files with a message. Works well with =dired-copy-filename-as-kill= for pasting paths.
- *=/= Slash Commands* -- Typing =/= in the prompt area triggers =completing-read= with action commands that execute immediately without sending text to the LLM. Available commands: =model=, =system=, =clear=, =save=, =load=, =tools=, =context=, =help=, =copy=, =retry=, =tone=, =fabric=, =awesome=, =streaming=, =reset=. Filtered by loaded modules (=system= requires =ollama-buddy-user-prompts=, =tools= requires =ollama-buddy-tools=, =fabric= requires =ollama-buddy-fabric=, =awesome= requires =ollama-buddy-awesome=). Outside the prompt area, =/= inserts literally. Press =C-g= to cancel and insert a literal =/=.
- *=ollama-buddy-slash-commands=* -- New customizable alist for defining available =/= slash commands and their action functions.
- *=ollama-buddy-copy-last-response=* -- Copy the last assistant response to the kill ring (available as =/copy= slash command).
- *=ollama-buddy-retry-last-prompt=* -- Resend the last user prompt to the current model (available as =/retry= slash command).
- *RAG loaded by default* -- =ollama-buddy-rag= is now required by default (no longer needs manual =(require 'ollama-buddy-rag)=). All =@rag()= inline commands and RAG context features are always available.

** <2026-02-16 Mon> *2.5.2*

PDF support for RAG indexing.

*** New Features

- *PDF Indexing* -- RAG can now index PDF files using =pdftotext= (from =poppler-utils=). PDF text extraction is auto-detected at runtime; if =pdftotext= is not installed, PDF files are silently skipped during indexing. PDFs bypass the file size limit since binary size does not reflect extracted text size.

** <2026-02-16 Mon> *2.5.1*

Usability improvements and inline RAG syntax.

*** New Features

- *Inline =@rag(query)= Syntax* -- Use =@rag(query)= in prompts to automatically search and attach RAG context, mirroring =@search()= for web search. Multiple queries supported per prompt. Works across all providers (Ollama, OpenAI, Claude, Gemini, Grok, Copilot, Codestral).
- *Selection Status in Role Menu* -- Role transient menu header now shows current selection info (character count and line count) so you can confirm text is selected before running commands.
- *Session Name Improvements* -- Session names generated via stop-word filtering: removes ~60 common English words and short words, keeps up to 5 key terms joined by hyphens.
- *Autosave Transcript* -- Chat buffer automatically saved to =~autosave.org= after each completed response. File is deleted when a proper session save is performed. Works with both network-process and curl backends.

*** Keybinding Changes

- =C-c r= now opens RAG transient menu directly (replaces individual =C-c r i/s/a/l/d/0= bindings)
- =C-c C-r= resets system prompt in chat buffer
- =C-r= in main transient menu (=C-c O=) resets system prompt

** <2026-02-15 Sun> *2.5.0*

RAG (Retrieval-Augmented Generation) Support

*** Overview

Ollama Buddy now includes a local RAG system via =ollama-buddy-rag.el=, enabling semantic search over your documents and source code. Index any directory, then search and attach relevant context to conversations --- all without sending entire files to the LLM.

RAG uses Ollama's =/api/embed= endpoint with embedding models (e.g. =nomic-embed-text=) to generate vector representations of document chunks, then performs cosine similarity search at query time to find the most relevant passages.

*** RAG Features

- New =ollama-buddy-rag.el= module with complete RAG pipeline
- Configurable chunking: chunk size (default 400 words), overlap (50 words), batch size
- Embedding via Ollama's =/api/embed= endpoint with batch support
- Cosine similarity vector search with configurable threshold and top-k
- Persistent index storage in =~/.emacs.d/ollama-buddy/rag-indexes/=
- Fully asynchronous embedding --- Emacs stays responsive during indexing
- Background operation progress shown in header line status (=RAG indexing 3/12...=)
- Embedding model validation with clear error message if model not pulled
- Embedding models added to recommended models in Model Management
- RAG context shown in Show Attachments buffer (=⊕N= indicator)
- Org heading escaping (=,*=) in all attachment display buffers
- Inline =@rag(query)= syntax in prompts --- automatic search and attach, mirroring =@search()=

*** RAG Commands & Keybindings

| Key     | Command                                | Description               |
|---------+----------------------------------------+---------------------------|
| C-c r   | =ollama-buddy-transient-rag-menu=      | Open RAG transient menu   |
| C-c C-r | =ollama-buddy-reset-system-prompt=     | Reset system prompt       |

RAG transient menu options (via =C-c r= or =C-c O= → =r=):

| Key | Command                              | Description               |
|-----+--------------------------------------+---------------------------|
| i   | =ollama-buddy-rag-index-directory=   | Index a directory         |
| s   | =ollama-buddy-rag-search=            | Search and display        |
| a   | =ollama-buddy-rag-attach=            | Search and attach context |
| l   | =ollama-buddy-rag-list-indexes=      | List available indexes    |
| d   | =ollama-buddy-rag-delete-index=      | Delete an index           |
| 0   | =ollama-buddy-rag-clear-attached=    | Clear RAG context         |

*** Inline =@rag(query)= Syntax

Use =@rag(query)= directly in prompts to automatically search and attach RAG context:

#+begin_example
Tell me about @rag(giant squid attack) from the book.
Compare @rag(captain nemo) with @rag(ned land) in terms of character development.
#+end_example

*** Transient Menu

RAG is accessible from the main transient menu: =C-c O= → =r= (RAG submenu) with index, search, and management options.

Reset system prompt is accessible via =C-c O= → =C-r= in the System Prompts section.

*** Configuration

| Variable                                |              Default | Description                   |
|-----------------------------------------+----------------------+-------------------------------|
| =ollama-buddy-rag-embedding-model=      | ="nomic-embed-text"= | Ollama model for embeddings   |
| =ollama-buddy-rag-chunk-size=           |                  400 | Words per chunk               |
| =ollama-buddy-rag-chunk-overlap=        |                   50 | Overlap between chunks        |
| =ollama-buddy-rag-top-k=                |                    5 | Results to return             |
| =ollama-buddy-rag-similarity-threshold= |                  0.3 | Minimum similarity score      |
| =ollama-buddy-rag-batch-size=           |                   10 | Chunks per embedding API call |

*** Test Suite

49 new tests covering chunking, vector math, index management, formatting, search, and end-to-end roundtrip. Run with =make test-rag=.

** <2026-02-12 Thu> *2.0.0*

Tool Calling Support and Tool-Capable Model Indicators

*** Overview

Ollama Buddy now supports tool calling (function calling), enabling LLMs to invoke Emacs functions during conversations. Models can read files, list directories, search buffers, perform calculations, execute shell commands and more. A new =ollama-buddy-tools.el= module provides the tool framework with built-in tools, a registration API for custom tools, and safety controls.

A ⚒ indicator is shown alongside tool-capable models across all model display surfaces (header line, model management buffer, and completing-read), following the same pattern as the existing vision (⊙) indicator.

*** Tool Calling Features

- New =ollama-buddy-tools.el= module with extensible tool calling framework
- Built-in safe tools: =read_file=, =list_directory=, =get_buffer_content=, =list_buffers=, =calculate=, =search_buffer=
- Built-in unsafe tools (require safe mode off): =write_file=, =execute_shell=
- =ollama-buddy-tools-register= API for adding custom tools with JSON schema parameters
- =ollama-buddy-tools-safe-mode= (default: t) restricts execution to read-only tools
- =ollama-buddy-tools-auto-execute= (default: t) controls confirmation prompts
- =ollama-buddy-tools-max-iterations= (default: 10) prevents infinite tool-call loops
- =ollama-buddy-tools-toggle= to enable/disable tool calling
- =ollama-buddy-tools-info= to view all registered tools and their status

*** Tool-Capable Model Indicator

- New =ollama-buddy-tools-models= customization listing models known to support tool calling
- New =ollama-buddy--model-supports-tools= predicate function (strips prefixes and cloud suffixes for matching)
- Header line shows ⚒ when the current model supports tools
- =C-c m= completing-read shows ⚒ (and ⊙ for vision) annotations next to capable models
- Model management buffer (=C-c M=) shows ⚒ next to tool-capable models in both Available Models and Cloud Models sections

*** Supported Tool-Capable Models

qwen3, qwen3:32b, qwen3:14b, qwen3:8b, qwen3-coder-next, qwen3-coder:480b, deepseek-v3.1:671b, gpt-oss:120b, gpt-oss:20b, glm-4.7, glm-4.7-flash, llama3.1, llama3.3, mistral, mistral-nemo, command-r+, granite4

** <2026-02-12 Thu> *1.5.0*

Dynamic Transient Role Menu with Grouped Commands

*** Overview

Role-specific commands now display in a full transient popup menu instead of the minibuffer =read-key= menu, giving you persistent display, help, and discoverability. The menu rebuilds dynamically each time it opens, reflecting the current role's =ollama-buddy-command-definitions=. A new =:group= property on command definitions controls column layout in the transient menu.

*** Binding Update

The recommended binding for the role menu is now =ollama-buddy-role-transient-menu=:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-role-transient-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

The original =ollama-buddy-menu= minibuffer menu remains available via =M-x ollama-buddy-menu= as a fallback.

*** Changes

- New =ollama-buddy-role-transient-menu= interactive command generates a transient popup from =ollama-buddy-command-definitions=
- New =:group= property on command definitions controls transient column grouping
- Commands without =:group= fall into a single "Commands" column (full backwards compatibility)
- Commands with =:model= show the model name in the transient description
- Duplicate keys are deduplicated (first definition wins)
- =quit= entries are skipped (transient handles quit natively with =C-g=)
- Main transient menu =b= key now opens the role transient menu
- =C-c b= in chat buffer remapped to =ollama-buddy-role-transient-menu=
- All built-in presets updated with =:group= properties:
  - *default*: General, Custom, System
  - *developer*: General, Code Analysis, Code Generation, Custom, System
  - *bard*: General, Text Transformation, System
  - *buffy*: General, Characters, Supernatural, System
  - *emacs*: General, Emacs Enthusiasm, System
  - *janeway*: General, Starfleet, System
  - *translator*: General, Languages, Tools, System
  - *writer*: General, Writing, Editorial, Creative, Analysis, Research, Utility, System
  - *documenter*: General, Overview, Code Docs, Release, Support, Improve, System
  - *tutor*: General, Info Docs, Explanation, Assessment, Context, System

*** Backwards Compatibility

Existing role presets without =:group= properties continue to work. All commands appear in a single "Commands" column, matching the previous flat menu layout.

** <2026-02-12 Thu> *1.4.0*

Tone Selector for Response Style

*** Overview

Added a tone selector that lets users quickly switch between response styles (Normal, Concise, Learning, Explanatory, Formal) without manually editing system prompts.

*** Changes

- New =ollama-buddy-tone-alist= defcustom mapping tone names to system prompt modifier strings
- New =ollama-buddy-set-tone= interactive command with =completing-read= interface
- Tone text prepended to the effective system prompt (before global and session prompts)
- Header line shows =~X= indicator when a non-Normal tone is active (e.g. =~C= for Concise)
- Keybinding =C-c ~= in chat buffer
- Transient menu entry =~= under Display Toggle section

** <2026-02-12 Thu> *1.3.5*

Cloud Models in Model Management and Categorized Recommendations

*** Overview

Enhanced the Model Management page (=C-c M=) with a dedicated cloud models section and reorganized recommended models into categorized groups to help new users find the right model.

*** Model Management Changes

- Cloud models now displayed in their own =☁ Cloud Models= section with auth status indicator
- Each cloud model has Select and Pull Manifest actions
- Cloud models use =cl:= prefix for clear visual distinction
- Local Ollama models no longer show the =o:= prefix unless external providers (OpenAI, Claude, etc.) are loaded

*** Categorized Recommended Models

- Recommended models reorganized into categories: General Chat, Reasoning, Efficient & Capable, Coding, and General Alternatives
- Each category displayed as a foldable org subheading with a description
- Categories with no uninstalled models are automatically hidden
- =ollama-buddy-available-models= now uses a plist structure with =:name=, =:description= and =:models= keys

** <2026-02-11 Wed> *1.3.4*

Header Line and Interface Refinements

*** Overview

Streamlined the header line, transient menu, and session management for a cleaner, more focused interface.

*** Header Line Changes

- Removed ORG/MD format indicator from header line
- Removed provider indicators (acgkps) from header line
- History indicator now off by default (toggle with =C-c >=)
- Added =<= indicator when global system prompt is disabled
- Compact =V= indicator for hidden reasoning (was "REASONING HIDDEN")
- Status feedback message when toggling global system prompt

*** Transient Menu Changes

- Removed numeric keys from transient menu bindings
- Model management reassigned to =M= (was =W=)
- Multishot reassigned to =U= (was =M=)
- Added ORG/MD toggle (=C-o=) under Diagnostics section
- Added History Display toggle (=>=) under Display Toggle section
- Added Global Prompt toggle (=<=) under Display Toggle section
- Context Type display reassigned to =&= (was =8=)

*** Model Management

- Tidied up model selection interface
- Actions section (Import GGUF, Refresh, Custom Pull) moved to top
- Unload All now refreshes the model list after completion
- "Pull Model from Hub" renamed to "Custom Pull Model" for clarity
- Recommended Models section relabelled "Select or Pull to Install"
- Updated curated =ollama-buddy-available-models= list with categorised groupings

*** Session Management

- Simplified session management: replaced List and Delete commands with a single Directory command (=C-c Z=)
- =ollama-buddy-sessions-directory= opens sessions folder in Dired for direct file management

*** Other Changes

- Web search module (=ollama-buddy-web-search=) now auto-loaded with the main package
- =ollama-buddy-full-welcome-enabled= variable controls expanded welcome screen (default: t)
- Smart =C-a= in chat buffer: first press goes to prompt start, second press goes to column 0
- Improved =ollama-buddy--cloud-model-p= detection
- Removed debug messages from =ollama-buddy--validate-model= and =ollama-buddy--get-valid-model=
- Roles directory now opens in other window
- Better global system prompt visibility with status bar feedback

** <2026-02-10 Tue> *1.3.3*

Auto-Pull Cloud Model Manifests

*** Overview

When selecting an Ollama cloud model that hasn't been used before, its manifest now gets pulled automatically. Previously, users would see a confusing "model not found" error because cloud models require a small manifest pull (~400 bytes) before first use.

*** Details

- Added ~ollama-buddy--ensure-cloud-model-available~ pre-flight check
- Automatically runs ~ollama pull~ for unpulled cloud models before sending requests
- Works with both network-process and curl communication backends
- Shows "Pulling cloud model manifest..." message during the brief download
- Invalidates models cache after successful pull so the model appears immediately

** <2026-02-06 Fri> *1.3.2*

New Role Presets: Tutor and Documenter

*** Tutor Preset

A new educational assistant role for learning and understanding concepts. Features include:

- *Explanation commands*: =explain-simply=, =explain-deeply=, =eli5=, =analogy=, =step-by-step=
- *Assessment commands*: =quiz-me=, =check-understanding=, =practice-problems=
- *Context commands*: =prerequisites=, =related-topics=, =real-world=, =common-mistakes=
- *Emacs Info integration*: Browse and fetch info nodes with completion
  - =I= - Fetch info node with all subnodes
  - =b= - Fetch single info node
  - Completion support for browsing manuals (elisp, emacs, org, etc.)
  - Cached node lists for fast subsequent access

Example workflow for learning Emacs:
1. Press =I=, select "elisp", then browse to "(elisp)Hooks"
2. Content is inserted into your buffer
3. Select text and use =s= (explain simply) or =z= (quiz me)

*** Documenter Preset

A technical documentation assistant role. Features include:

- *README & Overview*: =write-readme=, =quick-start=, =architecture-doc=
- *Code Documentation*: =add-comments=, =write-docstring=, =api-docs=, =usage-examples=
- *Release Notes*: =changelog-entry=, =release-notes=
- *Support Docs*: =troubleshooting=, =faq=
- *Improvement*: =improve-docs=, =simplify-docs=

** <2026-02-06 Fri> *1.3.1*

Role Menu Improvements

*** Per-Role Menu Columns

Each role preset can now define its own =ollama-buddy-menu-columns= setting, allowing different menu layouts per role. The role creator now prompts for column count when creating new roles.

#+begin_src elisp
;; In a role preset file:
(setq ollama-buddy-menu-columns 3)
#+end_src

*** Menu Display Fix

Fixed an issue where the custom menu could be truncated when the number of rows exceeded =max-mini-window-height=. The menu now dynamically adjusts to ensure all items are visible.

*** Role Preset Refinements

Streamlined built-in role presets by removing less commonly used commands:
- Removed: =kill-request=, =create-role=, =open-roles-directory=, =token-stats=, =show-history=, =delete-sessions=
- All presets now include =ollama-buddy-menu-columns= setting (default: 3)

** <2026-02-05 Thu> *1.3*

Added Web Search Integration

This release introduces real-time web search capabilities through Ollama's Web Search API, allowing LLMs to access current information from the web.

*** Web Search Pipeline

The web search feature implements a multi-stage pipeline:

1. *Query to Ollama API*: Search queries are sent to =https://ollama.com/api/web_search= via REST API with Bearer token authentication
2. *URL Extraction*: The API returns search results with URLs (the content snippets from the API are currently ignored in favor of fetching full page content)
3. *eww/shr Processing*: Each URL is fetched and processed through Emacs' built-in eww/shr HTML renderer, which:
   - Parses the HTML using =libxml-parse-html-region=
   - Extracts the main content (looking for =<article>=, =<main>=, or content divs)
   - Renders to clean plain text using =shr-insert-document=
   - Escapes org-mode special characters (=*= and =#= at line starts become =,*= and =,#=)
4. *Context Attachment*: The cleaned text is formatted with org headings and attached to the conversation context
5. *LLM Submission*: The search results are included when sending prompts to any configured LLM provider

*** Features

- Added =ollama-buddy-web-search.el= module
- Inline search syntax: =@search(query)= in prompts automatically triggers search
- Manual commands:
  - =M-x ollama-buddy-web-search= - Search and display results in a buffer
  - =M-x ollama-buddy-web-search-attach= - Search and attach to conversation context
- Keybindings: =C-c / s= (search), =C-c / a= (attach)
- Transient menu: =C-c O /= for web search operations
- Async URL fetching for better performance
- Status line shows ♁N indicator when web searches are attached
- Web searches integrated into attachment system:
  - =C-c C-w= shows both file attachments and web searches
  - =C-c 0= clears all attachments including web searches
- Full content display in attachment viewer with org-mode folding

*** Provider Support

Web search works with all LLM providers:
- Local Ollama models (=o:= prefix)
- OpenAI (=a:= prefix)
- Anthropic Claude (=c:= prefix)
- Google Gemini (=g:= prefix)
- X Grok (=k:= prefix)
- GitHub Copilot (=p:= prefix)
- Mistral Codestral (=s:= prefix)

*** Configuration

#+begin_src elisp
;; Required: Get API key from https://ollama.com/settings/keys
(setq ollama-buddy-web-search-api-key "your-api-key")

;; Optional customizations
(setq ollama-buddy-web-search-max-results 5)        ; Number of URLs to fetch
(setq ollama-buddy-web-search-snippet-length 2000)  ; Max chars per result
(setq ollama-buddy-web-search-include-urls nil)     ; Include URLs in context
(setq ollama-buddy-web-search-content-source 'eww)  ; Content source (see below)

;; Load the module
(require 'ollama-buddy-web-search nil t)
#+end_src

*** Content Source Options

The =ollama-buddy-web-search-content-source= variable controls how content is retrieved:

- =eww= (default, recommended) - Fetches each URL and renders HTML through Emacs' eww/shr. Produces cleaner, more complete content but requires additional HTTP requests.

- =api= (experimental) - Uses content returned directly from the Ollama API without fetching individual URLs. Faster but content quality depends on what the API returns.

*** Example Usage

#+begin_src
;; Inline search in prompt
What are the key points from @search(emacs 30 new features)?

;; Multiple searches
Compare @search(rust async) with @search(go concurrency)
#+end_src

** <2026-02-02 Mon> *1.2.1*

Added attachment count indicator to header-line

- Header-line now displays ≡N (paperclip + count) when files are attached
- Provides constant visual reminder of attachment context before sending
- Indicator appears in bold near the beginning of the status line
- Empty when no attachments are present

** <2026-02-01 Sun> *1.2*

Added GitHub Copilot integration

- Added =ollama-buddy-copilot.el= for GitHub Copilot Chat API support
- Implemented GitHub OAuth device flow authentication (no API key needed)
- =M-x ollama-buddy-copilot-login= opens browser for secure GitHub authentication
- =M-x ollama-buddy-copilot-logout= removes saved authentication
- =M-x ollama-buddy-copilot-status= checks authentication status
- OAuth token persisted to =~/.emacs.d/ollama-buddy-copilot-token.json=
- Uses "p:" prefix to identify Copilot models
- Supports OpenAI (gpt-4.1, gpt-4o, gpt-5), Anthropic (claude-sonnet-4, claude-opus-4.5), Google (gemini-2.5-pro, gemini-3-pro), and xAI (grok-code-fast-1) models
- Supports conversation history and system prompts
- Header line shows "p" indicator when Copilot provider is loaded

To use GitHub Copilot:
1. Ensure you have an active GitHub Copilot subscription
2. Run =M-x ollama-buddy-copilot-login=
3. Enter the code shown in your browser at github.com/login/device
4. Select a Copilot model with =C-c m= (e.g., p:gpt-4o)

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :config
  (require 'ollama-buddy-copilot nil t))
#+end_src

** <2026-01-31 Sat> *1.1.6*

Removed model status buffer

- Removed =ollama-buddy-show-model-status= function and =*Ollama Model Status*= buffer
- Removed =C-c v= keybinding and transient menu entry
- Model information is available in the model management buffer (=C-c W=)

** <2026-01-31 Sat> *1.1.5*

Added global system prompt for consistent response formatting

- Added =ollama-buddy-global-system-prompt= customization with sensible default instructing models to avoid markdown tables and use plain prose
- Added =ollama-buddy-global-system-prompt-enabled= toggle (default: t) to enable/disable the global prompt
- Global prompt is prepended to session-specific system prompts, providing baseline formatting across all models and providers
- Added =ollama-buddy-toggle-global-system-prompt= command to quickly toggle the feature
- Added toggle to transient menu under "Display Toggle" (key: 9)
- Works with all providers: Ollama, OpenAI, Claude, Gemini, Grok, and Codestral

This feature helps ensure consistent response formatting across different models, avoiding issues like unexpected markdown tables in responses.

Consolidated model management

- Moved "Recommended Models (Pull)" section into =C-c W= model management buffer
- Removed =C-c A= (Detailed Info) keybinding and =ollama-buddy-show-interface-info= function
- Updated welcome screen to reference =C-c W= for model management

** <2026-01-31 Sat> *1.1.4*

Header-line and keybinding improvements

- Added =C-c RET= keybinding to send prompts (matches gptel convention)
- Simplified header-line: removed backend indicator (N/C)
- Changed header-line "Markdown" indicator to "MD" for compactness
- Fixed markdown to org heading conversion: MD headings now offset by 2 levels so H1 becomes org level 3, properly nesting under the =** [MODEL] RESPONSE= heading

** <2026-01-31 Sat> *1.1.3*

Improved chat UX and simplified codebase

- Added =ollama-buddy-auto-scroll= customization option (default: nil)
- When nil, cursor stays in place during streaming output, allowing manual scrolling
- When t, buffer auto-scrolls to follow new output (previous behavior)
- Added =ollama-buddy-pulse-response= option (default: t) to flash/pulse the response text when streaming completes, providing visual feedback similar to gptel
- Removed model name coloring feature (hash-based highlighting)
- Removed =ollama-buddy-highlight-models-enabled=, =ollama-buddy-toggle-model-highlighting=
- Simplifies codebase and may improve org-mode buffer performance

** <2026-01-30 Fri> *1.1.2*

Simplified chat interface

- Streamlined welcome screen showing only essential commands
- Full interface info (models, commands) moved to separate =*Ollama Buddy Info*= buffer via =C-c A=
- Model selection now shows counts: =C-c m (10 ollama / 4 online)= and =C-u C-c m (7 cloud)=
- Header line displays bold prefix characters for loaded external providers: =acgks=
- Welcome screen lists enabled online LLM providers (e.g., "Online: a: OpenAI | c: Claude")

** <2026-01-28 Wed> *1.1*

Added Ollama Cloud Models support

- Cloud models (running on ollama.com infrastructure) now work seamlessly
- =ollama-buddy-cloud-signin= to automatically open browser for authentication
- Cloud models are proxied through the local Ollama server which handles authentication
- Use =C-u C-c m= or transient menu "Model > Cloud" to select cloud models
- Status line shows ☁ indicator when using a cloud model
- Available cloud models include: qwen3-coder:480b-cloud, deepseek-v3.1:671b-cloud, gpt-oss:120b-cloud, minimax-m2.1:cloud, and more

To use cloud models:
1. Sign in once with =M-x ollama-buddy-cloud-signin= (opens browser automatically)
2. Select a cloud model with =C-u C-c m=
3. Use normally - the local Ollama server handles cloud authentication

#+begin_src elisp
;; Cloud models work with no additional configuration
;; Just sign in once via the transient menu: Cloud Auth > Sign In
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper))
#+end_src

** <2025-12-11 Thu> *1.0.2*

Added Mistral Codestral integration

- Added ollama-buddy-codestral.el for Mistral Codestral API support
- Implemented model handler registration for seamless switching between local Ollama and cloud-based Codestral models
- Supports async streaming responses with real-time token counting
- Follows the same integration pattern as other remote AI providers (OpenAI, Claude, Gemini, Grok)
- Uses "s:" prefix to identify Codestral models in the model list

** <2025-06-21 Sat> *0.13.1*

Refactored content register processing to be more efficient and added a new Emacs package brainstorming prompt file.

** <2025-06-15 Sun> *0.13.0*

Added curl communication backend with fallback support

- Added ollama-buddy-curl.el as separate backend implementation
- Implemented backend dispatcher system in ollama-buddy-core.el
- Updated all async functions to use backend dispatcher
- Added curl backend validation and testing functions
- Maintained full compatibility with existing network process backend

** <2025-05-31 Sat> *0.12.1*

Optimized Unicode escape function to fix blocking with large file attachments

- Fixed UI blocking when sending large attached files
- Used temp buffer with delete-char/insert instead of repeated concat calls
- Reduced processing time from minutes to milliseconds for large payloads

** <2025-05-22 Thu> *0.12.0*

Full system prompt in the status bar replaced with a more meaningful simple role title

- Added system prompt metadata tracking with title, source, and timestamp registry
- Implemented automatic title extraction and unified completing-read interface
- Enhanced fabric/awesome prompt integration with proper metadata handling
- Improved transient menu organization and org-mode formatting with folding
- Added system prompt history display and better error handling for empty files
- Transient menu has been simplified and reorganised

** <2025-05-21 Wed> *0.11.1*

Quite a bit of refactoring to generally make this project more maintainable and I have added a starter kit of user prompts.

- Color System Reworking
  - Removed all model color-related functions and variables
  - Removed dependency on =color.el=
  - Replaced with =highlight-regexp= and hashing to =^font-lock= faces, so now using a more native built-in solutions for model colouring rather than shoe-horning in overlays.

- UI Improvements
  - Simplified the display system by leveraging Org mode
  - Added org-mode styling for output buffers
  - Added =org-hide-emphasis-markers= and =org-hide-leading-stars= settings
  - Changed formatting to use Org markup instead of text properties
  - Converted plain text headers to proper Org headings
  - Replaced color properties with Org emphasis (bold)

- History Management Updates
  - Streamlined history editing functionality
  - Improved model-specific history editing
  - Refactored history display and navigation

- System Prompts
  - Added library of system prompts in these categories:
    - analysis (3 prompts)
    - coding (5 prompts)
    - creative (3 prompts)
    - documentation (3 prompts)
    - emacs (10 prompts)
    - general (3 prompts)
    - technical (3 prompts)
    - writing (3 prompts)

** <2025-05-19 Mon> *0.11.0*

Added user system prompts management

- You can now save, load and manage system prompts
- Created new transient menu for user system prompts (C-c s)
- Organized prompts by categories with org-mode format storage
- Supported prompt editing, listing, creation and deletion
- Updated key bindings to integrate with existing functionality
- Added prompts directory customization with defaults
  
** <2025-05-14 Wed> *0.10.0*

Added file attachment system for including documents in conversations

- Added file attachment support with configurable file size limits (10MB default) and supported file types
- Implemented session persistence for attachments in save/load functionality  
- Added attachment context inclusion in prompts with proper token counting
- Created comprehensive attachment management commands:
  - Attach files to conversations
  - Show current attachments in dedicated buffer
  - Detach specific files
  - Clear all attachments
- Added Dired integration for bulk file attachment
- Included attachment menu in transient interface (C-c A)
- Updated help text to document new attachment keybindings
- Enhanced context calculation to include attachment token usage

** <2025-05-12 Mon> *0.9.50*

Added context size management and monitoring

- Added configurable context sizes for popular models (llama3.2, mistral, qwen, etc.)
- Implemented real-time context usage display in status bar
- Can display in text or bar display types
- Added context size thresholds with visual warnings
- Added interactive commands for context management:
  - =ollama-buddy-show-context-info=: View all model context sizes
  - =ollama-buddy-set-model-context-size=: Manually configure model context
  - =ollama-buddy-toggle-context-percentage=: Toggle context display
- Implemented context size validation before sending prompts
- Added token estimation and breakdown (history/system/current prompt)
- Added keybindings: C-c $ (set context), C-c % (toggle display), C-c C (show info)
- Updated status bar to show current/max context with fontification

** <2025-05-05 Mon> *0.9.44*

- Sorted model names alphabetically in intro message
- Removed multishot writing to register name letters

** <2025-05-05 Mon> *0.9.43*

Fix model reference error exceeding 26 models #15

Update =ollama-buddy= to handle more than 26 models by using prefixed combinations for model references beyond 'z'. This prevents errors in =create-intro-message= when the local server hosts a large number of models.

** <2025-05-03 Sat> *0.9.42*

Added the following to recommended models:

- qwen3:0.6b
- qwen3:1.7b
- qwen3:4b
- qwen3:8b

and fixed pull model

** <2025-05-02 Fri> *0.9.41*

Refactored model prefixing again so that when using only ollama models no prefix is applied and is only applied when online LLMs are selected (for example claude, chatGPT e.t.c)

I think this makes more sense and is cleaner for I suspect the majority who may use this package are probably more interested in just using ollama models and the prefix will probably be a bit confusing.

This could be a bit of a breaking change once again I'm afraid for those ollama users that have switched and are now familiar with prefixing "o:", sorry!

** <2025-05-02 Fri> *0.9.40*

Added vision support for those ollama models that can support it!

** <2025-04-29 Tue> *0.9.38*

Added model unloading functionality to free system resources

- Add unload capability for individual models via the model management UI
- Create keyboard shortcut (C-c C-u) for quick unloading of all models
- Display running model count and unload buttons in model management buffer
  
** <2025-04-25 Fri> *0.9.37*

- Display modified parameters in token stats

** <2025-04-25 Fri> *0.9.36*

Added Reasoning/Thinking section visibility toggle functionality

** <2025-04-21 Mon> *0.9.35*

Added Grok support

** <2025-04-20 Sun> *0.9.33*

Fixed utf-8 encoding stream response issues from remote LLMs.

** <2025-03-20 Thu> *0.9.32*

Finished the remote LLM decoupling process, meaning that the core =ollama-buddy= logic is now not dependent on any remote LLM, and each remote LLM package is self-contained and functions as a unique extension.

** <2025-03-19 Wed> *0.9.31*

Refactored model prefixing logic and cleaned up

- Standardized model prefixing by introducing distinct prefixes for Ollama (=o:=), OpenAI (=a:=), Claude (=c:=), and Gemini (=g:=) models.
- Centralized functions to get full model names with prefixes across different model types.
- Removed redundant and unused variables related to model management.

Note that there may be some breaking changes here especially regarding session recall as all models will now have a prefix to uniquely identify their type.  For =ollama= recall, just edit the session files to prepend the ollama prefix of "o:"

** <2025-04-17 Thu> *0.9.30*

Added Gemini integration!

As with the Claude and ChatGPT integration, you will need to add something similar to them in your configuration. I currently have the following set up to enable access to the remote LLMs:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (ollama-buddy-claude-api-key
   (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (ollama-buddy-gemini-api-key
   (auth-source-pick-first-password :host "ollama-buddy-gemini" :user "apikey"))
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  (require 'ollama-buddy-gemini nil t))
#+end_src

Also with the previous update all the latest model names will be pulled, so there should be a full comprehensive list for each of the main remote AI LLMs!

** <2025-04-17 Thu> *0.9.23*

Refactored history and model management for remote LLMs

- Now pulling in latest model list for remote LLMs (so now ChatGPT 4.1 is available!)
- Removed redundant history and model management functions from =ollama-buddy-claude.el= and =ollama-buddy-openai.el=. Replaced them with shared implementations to streamline code and reduce duplication

** <2025-04-15 Tue> *0.9.22*

Enhanced session management

- Refactored =ollama-buddy-sessions-save= to autogenerate session names using timestamp and model.
- Improved session saving/loading by integrating org file handling.
- Updated mode line to display current session name dynamically.

** <2025-04-10 Thu> *0.9.21*

Add history edit/view toggle features, so effectively merging the former history display into the history edit functionality.

** <2025-04-04 Fri> *0.9.20*

- Added =ollama-buddy-awesome.el= to integrate Awesome ChatGPT Prompts.
  
** <2025-04-01 Tue> *0.9.17*

- Added link to =ollama-buddy= info manual from the chat buffer and transient menu as MELPA has now picked it up and installed it!

** <2025-03-28 Fri> *0.9.16*

- Added =ollama-buddy-fix-encoding-issues= to handle text encoding problems.
- Refactored and streamline fabric pattern description handling.
- Removed unused fabric pattern categories to enhance maintainability.

** <2025-03-28 Fri> *0.9.15*

- Implement asynchronous operations for model management
  - Introduce non-blocking API requests for fetching, copying, and deleting models
- Add caching mechanisms to improve efficiency
  - Cache model data to reduce redundant API calls
  - Manage cache expiration with timestamps and time-to-live settings
- Update status line to reflect ongoing background operations
- Ensure smooth user interaction by minimizing wait times and enhancing performance

** <2025-03-26 Wed> *0.9.13*

- Added automatic writing of last response to a register
- Added M-r to search through prompt history

** <2025-03-26 Wed> *0.9.12*

- Added experimental Claude AI support!
- removed curl and replaced with url.el for online AI integration

** <2025-03-24 Mon> *0.9.11*

Added the ability to toggle streaming on and off

- Added customization option to enable/disable streaming mode
- Implemented toggle function with keybindings (C-c x) and transient menu option
- Added streaming status indicator in the modeline

** <2025-03-22 Sat> *0.9.10*

Added experimental OpenAI support!

** <2025-03-22 Sat> *0.9.9.5*

Added texinfo documentation for future automatic installation through MELPA and created an Emacs manual.

** <2025-03-20 Thu> *0.9.9*

Intro message with model management options (select, pull, delete) and option for recommended models to pull

- Enhance model management and selection features
- Display models available for download but not yet pulled

** <2025-03-19 Wed> *0.9.8*

Added model management interface to pull and delete models

- Introduced `ollama-buddy-manage-models` to list and manage models.
- Added actions for selecting, pulling, stopping, and deleting models.

** <2025-03-19 Wed> *0.9.7*

- Added GGUF file import and Dired integration

** <2025-03-18 Tue> *0.9.6*

- Added a transient menu containing all commands currently presented in the chat buffer
- Added fabric prompting support, see https://github.com/danielmiessler/fabric
- Moved the presets to the top level so they will be present in the package folder
  
** <2025-03-17 Mon> *0.9.5*

Added conversation history editing

- Added functions to edit conversation history (=ollama-buddy-history-edit=, =ollama-buddy-history-save=, etc.).
- Updated =ollama-buddy-display-history= to support history editing.
- Added keybinding =C-c E= for history editing.

** <2025-03-17 Mon> *0.9.1*

New simple basic interface is available.

** <2025-03-17> *0.9.0*

Added command-specific parameter customization

- Added :parameters property to command definitions for granular control
- Implemented functions to apply and restore parameter settings
- Added example configuration to refactor-code command

** <2025-03-16> *0.8.5*

Added system prompt support for commands

- Introduced `:system` field to command definitions.
- Added `ollama-buddy-show-system-prompt` to view active system prompt.
- Updated UI elements to reflect system prompt status.

** <2025-03-14> *0.8.0*

Added system prompt support

- Added =ollama-buddy--current-system-prompt= variable to track system prompts
- Updated prompt area rendering to distinguish system prompts
- Modified request payload to include system prompt when set 
- Enhanced status bar to display system prompt indicator
- Improved help menu with system prompt keybindings
  
** <2025-03-13> *0.7.4*

Added model info command, update keybindings

- Added `ollama-buddy-show-raw-model-info` to fetch and display raw JSON details 
  of the current model in the chat buffer.
- Updated keybindings:
  - `C-c i` now triggers model info display.
  - `C-c h` mapped to help assistant.
  - Improved shortcut descriptions in quick tips section.
- Removed unused help assistant entry from menu.
- Changed minibuffer-prompt key from `?i` to `?b`.

** <2025-03-12> *0.7.3*

Added function to associate models with menu commands

- Added =ollama-buddy-add-model-to-menu-entry= autoload function
- Enabled dynamic modification of command-model associations

** <2025-03-12> *0.7.2*

Added menu model colours back in and removed some redundant code

** <2025-03-11> *0.7.1*

Added debug mode to display raw JSON messages in a debug buffer

- Created new debug buffer to show raw JSON messages from Ollama API
- Added toggle function to enable/disable debug mode (ollama-buddy-toggle-debug-mode)
- Modified stream filter to log and pretty-print incoming JSON messages
- Added keybinding C-c D to toggle debug mode
- Updated documentation in welcome message

** <2025-03-11> *0.7.0*

Added comprehensive Ollama parameter management

- Added customization for all Ollama option API parameters with defaults
- Only send modified parameters to preserve Ollama defaults
- Display active parameters with visual indicators for modified values
- Add keybindings and help system for parameter management
- Remove redundant temperature controls in favor of unified parameters

** <2025-03-10> *0.6.1*

Refactored prompt handling so each org header line should now always have a prompt for better export

- Added functionality to properly handle prompt text when showing/replacing prompts
- Extracted inline lambdas in menu actions into named functions
- Added fallback for when no default model is set

** <2025-03-08> *0.6.0*

Chat buffer now in org-mode

- Enabled =org-mode= in chat buffer for better text structure
- Implemented =ollama-buddy--md-to-org-convert-region= for Markdown to Org conversion
- Turn org conversion on and off
- Updated keybindings =C-c C-o= to toggle Markdown to Org conversion

** <2025-03-07> *0.5.1*

Added temperature control

- Implemented temperature control parameter
- Added menu commands for setting (T), resetting (0)
- Added keybindings (C-c t/T/0) for quick temperature adjustments
- Updated header line and prompt displays to show current temperature
- Included temperature info in welcome screen with usage guidance

** <2025-03-06> *0.5.0*

Implemented session management, so you can now save your conversations and bring them back with the relevant context and chat history!

- Chat history is now maintained separately for each model
- Added session new/load/save/delete/list functionality
- A switch in context can now be achieved by any of the following methods:
  - Loading a previous session
  - Creating a new session
  - Clearing history on the current session
  - Toggling history on and off

** <2025-03-04> *0.4.1*

Added a sparse version of =ollama-buddy= called =ollama-buddy-mini=, see the github repository for the elisp file and a description in =README-mini.org=

** <2025-03-03> *0.4.0*

Added conversation history support and navigation functions

- Implemented conversation history tracking between prompts and responses
- Added configurable history length limits and visual indicators
- Created navigation functions to move between prompts/responses in buffer

** <2025-03-02> *0.3.1*

Enhanced model colour contrast with themes, allowing =ollama-buddy-enable-model-colors= to be enabled by default.

** <2025-03-01> *0.3.0*

Added real-time token usage tracking and display

- Introduce variables to track token counts, rates, and usage history
- Implement real-time token rate updates with a timer
- Add a function to display token usage statistics in a dedicated buffer
- Allow toggling of token stats display after responses
- Integrate token tracking into response processing and status updates
- Ensure cleanup of timers and tracking variables on completion or cancellation

** <2025-02-28> *0.2.4*

Added model-specific color highlighting (experimental)

- Introduce `ollama-buddy-enable-model-colors` (default: nil) to toggle model-based color highlighting.
- Assign consistent colors to models based on string hashing.
- Apply colors to model names in the menu, status, headers, and responses.
- Add `ollama-buddy-toggle-model-colors` command to toggle this feature.

This feature improves UI clarity, making it easier to visually distinguish models.

** <2025-02-28> *0.2.3*

Added Prompt History Support

- Prompts are now integrated into the Emacs history mechanism which means they persist across sessions.  
- Use =M-p= to navigate prompt history, and =M-p= / =M-n= within the minibuffer to insert previous prompts.  

** <2025-02-27> *0.2.2*

Added support for role-based presets

- Introduced `ollama-buddy-roles-directory` for storing role preset files.
- Implemented interactive functions to manage roles:
  - `ollama-buddy-roles-switch-role`
  - `ollama-buddy-role-creator-create-new-role`
  - `ollama-buddy-roles-open-directory`
- Added ability to create and switch between role-specific commands.
- Updated menu commands to include role management options.

** <2025-02-26> *0.2.1*

added multishot execution with model selection

- Assign letters to models for quick selection
- Implement multishot mode for sequential requests to multiple models
- Store responses per model in registers named after assigned letters
- Display multishot progress in status
- Bind `C-c C-l` to trigger multishot prompt

** <2025-02-19> *0.2.0*

Improved prompt handling in chat buffer and simplified setup

- Chat buffer now more prompt based rather than ad-hoc using C-c C-c to send and C-c C-k to cancel
- Connection monitor now optional, ollama status visibility now maintained by strategic status checks simplifying setup.
- Can now change models from chat buffer using C-c C-m
- Updated intro message with ascii logo
- Suggested default "C-c o" for =ollama-buddy-menu=
- defcustom ollama-buddy-command-definitions now will work in the customization interface.

** <2025-02-13>

Models can be assigned to individual commands

- Set menu :model property to associate a command with a model
- Introduce `ollama-buddy-fallback-model` for automatic fallback if the specified model is unavailable.
- Improve `ollama-buddy--update-status-overlay` to indicate model substitution.
- Expand `ollama-buddy-menu` with structured command definitions using properties for improved flexibility.
- Add `ollama-buddy-show-model-status` to display available and used models.
- Refactor command execution flow to ensure model selection is handled dynamically.

** <2025-02-12>

- =ollama-buddy= updated in preparation for MELPA submission
- Removed C-c single key user keybinding as part of package definition and in the README gave guidance on defining a user keybinding to activate the ollama buddy menu
- Added =ellama= comparison description
- Activating and deactivating the =ollama= monitor process now users responsibility

** <2025-02-11>

Significant improvements and refactoring, particularly around connection handling, streaming responses, and status monitoring.

- Replace curl-based requests with native network processes
- Added customizatble ollama host and port  
- Added connection monitoring with automatic status updates
- Added permanently visible status showing connection state and current model
- Improve error handling for connection failures
- Refined AI assistant presentation

** <2025-02-07>

Increase menu columns to 4, add dictionary lookup and save chat options  

- Change `ollama-buddy-menu-columns` from 3 to 4  
- Rename "Describe code" menu key from `?d` to `?c`  
- Add dictionary lookup feature (`?d`)  
- Add synonym lookup feature (`?n`)  
- Add "Save chat" option (`?s`) to write chat buffer to a file  

** <2025-02-07>

Added query finished message.

** <2025-02-06>

- Initial release
- Basic chat functionality
- Menu-driven interface
- Region-based interactions
- Model switching support
