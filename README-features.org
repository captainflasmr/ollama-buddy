#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+todo: TODO DOING | DONE
#+startup: showall

* Features as they came in

** <2025-03-26> *0.9.13*

- Added automatic writing of last response to a register
- Added M-r to search through prompt history

I was just thinking about a general workflow aspect and that is getting responses out of the =ollama-buddy= chat buffer.  Of course if you are already there then it will be easier, but even then the latest prompt, which is probably the one you are interested in will still have to be copied to the kill ring.

This issue is even more pronounced when you are sending text from other buffers to the chat.

So, the solution I have put in place is to always write the last response to a register of your choice.  I always think registers are an underused part of Emacs, I already have repurposed them for the multishot, so why not always make the last response available.

For example, you want to proofread a sentence, you can mark the text, send to the chat using the custom menu to proofread then the response will be available in maybe register "a".  The chat buffer will be brought up if not already visible so you can validate the output, then pop back to your buffer, delete the paragraph and insert the register "a"?, maybe.  I am going to put this in as I suspect no-one uses registers anyway and if they do, they can push the response writing register away using =ollama-buddy-default-register=, I don't think this will do any harm, and actually it is something I may starting using more often.

As a side note, I also need to think about popping into the chat buffer with a buffer text push to the chat, should I do it?, not sure yet, still getting to grips with the whole workflow aspect, so will need a little more time to see what works.

Also as a side note to this ramble, the general register prefix is annoyingly long =C-x r i <register>= so I have rebound in my config to =M-a=, as I never want to go back a sentence and also if I just write to the default "a" register then it feels ergonomically fast.

** <2025-03-25> *0.9.12*

- Added experimental Claude AI support!
- removed curl and replaced with url.el for online AI integration

A very similar implementation as for ChatGPT.

To activate, set the following:

#+begin_src elisp
(require 'ollama-buddy-claude nil t)
(ollama-buddy-claude-api-key "<extremely long key>")
#+end_src

** <2025-03-24> *0.9.11*

Added the ability to toggle streaming on and off

- Added customization option to enable/disable streaming mode
- Implemented toggle function with keybindings (C-c x) and transient menu option
- Added streaming status indicator in the modeline

The latest update introduces the ability to toggle between two response modes:

- *Streaming mode (default)*: Responses appear token by token in real-time, giving you immediate feedback as the AI generates content.
- *Non-streaming mode*: Responses only appear after they're fully generated, showing a "Loading response..." placeholder in the meantime.

While watching AI responses stream in real-time is often helpful, there are situations where you might prefer to see the complete response at once:

- When working on large displays where the cursor jumping around during streaming is distracting
- When you want to focus on your work without the distraction of incoming tokens until the full response is ready

The streaming toggle can be accessed in several ways:

1. Use the keyboard shortcut =C-c x=
2. Press =x= in the transient menu
3. Set the default behavior through customization:
   #+begin_src elisp
   (setq ollama-buddy-streaming-enabled nil) ;; Disable streaming by default
   #+end_src

The current streaming status is visible in the modeline indicator, where an "X" appears when streaming is disabled.

** <2025-03-22> *0.9.10*

Added experimental OpenAI support!

Yes, that's right, I said I never would do it, and of course, this package is still very much =ollama=-centric, but I thought I would just sneak in some rudimentary ChatGPT support, just for fun!

It is a very simple implementation, I haven't managed to get streaming working, so Emacs will just show "Loading Response..." as it waits for the response to arrive. It is asynchronous, however, so you can go off on your Emacs day while it loads (although being ChatGPT, you would think the response would be quite fast!)

By default, OpenAI/ChatGPT will not be enabled, so anyone wanting to use just a local LLM through =ollama= can continue as before. However, you can now sneak in some experimental ChatGPT support by adding the following to your Emacs config as part of the =ollama-buddy= set up.

#+begin_src elisp
(require 'ollama-buddy-openai nil t)
(setq ollama-buddy-openai-api-key "<big long key>")
#+end_src

and you can set the default model to ChatGPT too!

#+begin_src elisp
(setq ollama-buddy-default-model "GPT gpt-4o")
#+end_src

With this enabled, chat will present a list of ChatGPT models to choose from. The custom menu should also now work with chat, so from anywhere in Emacs, you can push predefined prompts to the =ollama= buddy chat buffer now supporting ChatGPT.

There is more integration required to fully incorporate ChatGPT into the =ollama= buddy system, like token rates and history, etc. But not bad for a first effort, methinks!

Here is my current config, now mixing ChatGPT with =ollama= models:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key "<very long key>")
  (ollama-buddy-default-model "GPT gpt-4o")
  :config
  (require 'ollama-buddy-openai nil t)
  (ollama-buddy-update-menu-entry
   'refactor-code :model "qwen2.5-coder:7b")
  (ollama-buddy-update-menu-entry
   'git-commit :model "qwen2.5-coder:3b")
  (ollama-buddy-update-menu-entry
   'describe-code :model "qwen2.5-coder:3b")
  (ollama-buddy-update-menu-entry
   'dictionary-lookup :model "llama3.2:3b")
  (ollama-buddy-update-menu-entry
   'synonym :model "llama3.2:3b")
  (ollama-buddy-update-menu-entry
   'proofread :model "GPT gpt-4o")
  (ollama-buddy-update-menu-entry
   'custom-prompt :model "deepseek-r1:7b"))
#+end_src

** <2025-03-22> *0.9.9.5*

Added texinfo documentation for future automatic installation through MELPA and created an Emacs manual.

If you want to see what the manual would look like, just download the docs directory from github, cd into it, and run:

#+begin_src bash
make
sudo make install-docs
#+end_src

Then calling up =info= =C-h i= and ollama buddy will be present in the Emacs menu, or just select =m= and search for =Ollama Buddy=

For those interested in the manual, I have converted it into html format, which is accessible here:

[[file:docs/ollama-buddy.org]]

It has been converted using the following command:

#+begin_src bash
makeinfo --html --no-split ollama-buddy.texi -o ollama-buddy.html
pandoc -f html -t org -o ollama-buddy.org ollama-buddy.html
#+end_src

** <2025-03-20> *0.9.9*

Intro message with model management options (select, pull, delete) and option for recommended models to pull

- Enhance model management and selection features
- Display models available for download but not yet pulled

** <2025-03-19> *0.9.8*

Added model management interface to pull and delete models

- Introduced `ollama-buddy-manage-models` to list and manage models.
- Added actions for selecting, pulling, stopping, and deleting models.

You can now manage your Ollama models directly within Emacs with =ollama-buddy=

With this update, you can now:

- *Browse Available Models* – See all installed models at a glance.  
- *Select Models Easily* – Set your active AI model with a single click.  
- *Pull Models from Ollama Hub* – Download new models or update existing ones.  
- *Stop Running Models* – Halt background processes when necessary.  
- *Delete Unused Models* – Clean up your workspace with ease.  

1. *Open the Model Management Interface*  
   Press *=C-c W=* to launch the new *Model Management* buffer or through the transient menu.

2. *Manage Your Models*  
   - Click on a model to *select* it.  
   - Use *"Pull"* to fetch models from the Ollama Hub.  
   - Click *"Stop"* to halt active models.  
   - Use *"Delete"* to remove unwanted models.

3. *Perform Quick Actions*  
   - *=g=* → Refresh the model list.  
   - *=i=* → Import a *GGUF model file*.  
   - *=p=* → Pull a new model from the *Ollama Hub*.  

When you open the management interface, you get a structured list like this:

#+begin_src 
Ollama Models Management
=======================

Current Model: mistral:7b
Default Model: mistral:7b

Available Models:
  [ ] llama3.2:1b  Info  Pull  Delete
  [ ] starcoder2:3b  Info  Pull  Delete
  [ ] codellama:7b  Info  Pull  Delete
  [ ] phi3:3.8b  Info  Pull  Delete
  [x] llama3.2:3b  Info  Pull  Delete Stop

Actions:
[Import GGUF File]  [Refresh List]  [Pull Model from Hub]
#+end_src

Previously, managing Ollama models required manually running shell commands. With this update, you can now *do it all from Emacs*, keeping your workflow smooth and efficient!

** <2025-03-19> *0.9.7*

- Added GGUF file import and Dired integration

Import GGUF Models into Ollama from =dired= with the new =ollama-buddy-import-gguf-file= function. In =dired= just navigate to your file and press =C-c i= or =M-x ollama-buddy-import-gguf-file= to start the import process. This eliminates the need to manually input file paths, making the workflow smoother and faster.

The model will then be immediately available in the =ollama-buddy= chat interface.

** <2025-03-18> *0.9.6*

- Added a transient menu containing all commands currently presented in the chat buffer
- Added fabric prompting support, see https://github.com/danielmiessler/fabric
- Moved the presets to the top level so they will be present in the package folder

Ollama Buddy now includes a transient-based menu system to improve usability and streamline interactions. Yes, I originally stated that I would never do it, but I think it compliments my crafted simple textual menu and the fact that I have now defaulted the main chat interface to a simple menu.

This can give the user more options for configuration, they can use the chat in advanced mode where the keybindings are presented in situ, or a more minimal basic setup where the transient menu can be activated.  For my use-package definition I current have the following set up, with the two styles of menus sitting alongside each other :

  #+begin_src elisp
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu)
  #+end_src

The new menu provides an organized interface for accessing the assistant’s core functions, including chat, model management, roles, and Fabric patterns. This post provides an overview of the features available in the Ollama Buddy transient menus.

Yes that's right also =fabric= patterns!, I have decided to add in auto syncing of the patterns directory in https://github.com/danielmiessler/fabric

Simply I pull the patterns directory which contain prompt guidance for a range of different topics and then push them through a completing read to set the =ollama-buddy= system prompt, so a special set of curated prompts can now be applied right in the =ollama-buddy= chat!

Anyways, here is a description of the transient menu system.

*** What is the Transient Menu?

The transient menu in Ollama Buddy leverages Emacs' =transient.el= package (the same technology behind Magit's popular interface) to create a hierarchical, discoverable menu system. This approach transforms the user experience from memorizing numerous keybindings to navigating through logical groups of commands with clear descriptions.

*** Accessing the Menu

The main transient menu can be accessed with the keybinding =C-c O= when in an Ollama Buddy chat buffer. You can also call it via =M-x ollama-buddy-transient-menu= from anywhere in Emacs.

*** What the Menu Looks Like

When called, the main transient menu appears at the bottom of your Emacs frame, organized into logical sections with descriptive prefixes. Here's what you'll see:

#+begin_src 
|o(Y)o| Ollama Buddy
[Chat]             [Prompts]            [Model]               [Roles & Patterns]
o  Open Chat       l  Send Region       m  Switch Model       R  Switch Roles
O  Commands        s  Set System Prompt v  View Model Status  E  Create New Role
RET Send Prompt    C-s Show System      i  Show Model Info    D  Open Roles Directory
h  Help/Menu       r  Reset System      M  Multishot          f  Fabric Patterns
k  Kill/Cancel     b  Ollama Buddy Menu

[Display Options]          [History]              [Sessions]             [Parameters]
A  Toggle Interface Level  H  Toggle History      N  New Session         P  Edit Parameter
B  Toggle Debug Mode       X  Clear History       L  Load Session        G  Display Parameters
T  Toggle Token Display    V  Display History     S  Save Session        I  Parameter Help
U  Display Token Stats     J  Edit History        Q  List Sessions       K  Reset Parameters
C-o Toggle Markdown->Org                          Z  Delete Session      F  Toggle Params in Header
c  Toggle Model Colors                                                   p  Parameter Profiles
g  Token Usage Graph
#+end_src

This visual layout makes it easy to discover and access the full range of Ollama Buddy's functionality. Let's explore each section in detail.

*** Menu Sections Explained

**** Chat Section

This section contains the core interaction commands:

- *Open Chat (o)*: Opens the Ollama Buddy chat buffer
- *Commands (O)*: Opens a submenu with specialized commands
- *Send Prompt (RET)*: Sends the current prompt to the model
- *Help/Menu (h)*: Displays the help assistant with usage tips
- *Kill/Cancel Request (k)*: Cancels the current ongoing request

**** Prompts Section

These commands help you manage and send prompts:

- *Send Region (l)*: Sends the selected region as a prompt
- *Set System Prompt (s)*: Sets the current prompt as a system prompt
- *Show System Prompt (C-s)*: Displays the current system prompt
- *Reset System Prompt (r)*: Resets the system prompt to default
- *Ollama Buddy Menu (b)*: Opens the classic menu interface

**** Model Section

Commands for model management:

- *Switch Model (m)*: Changes the active LLM
- *View Model Status (v)*: Shows status of all available models
- *Show Model Info (i)*: Displays detailed information about the current model
- *Multishot (M)*: Sends the same prompt to multiple models

**** Roles & Patterns Section

These commands help manage roles and use fabric patterns:

- *Switch Roles (R)*: Switch to a different predefined role
- *Create New Role (E)*: Create a new role interactively
- *Open Roles Directory (D)*: Open the directory containing role definitions
- *Fabric Patterns (f)*: Opens the submenu for Fabric patterns

When you select the Fabric Patterns option, you'll see a submenu like this:

#+begin_src 
Fabric Patterns (42 available, last synced: 2025-03-18 14:30)
[Actions]             [Sync]              [Categories]          [Navigation]
s  Send with Pattern  S  Sync Latest      u  Universal Patterns q  Back to Main Menu
p  Set as System      P  Populate Cache   c  Code Patterns
l  List All Patterns  I  Initial Setup    w  Writing Patterns
v  View Pattern Details                   a  Analysis Patterns
#+end_src

**** Display Options Section

Commands to customize the display:

- *Toggle Interface Level (A)*: Switch between basic and advanced interfaces
- *Toggle Debug Mode (B)*: Enable/disable JSON debug information
- *Toggle Token Display (T)*: Show/hide token usage statistics
- *Display Token Stats (U)*: Show detailed token usage information
- *Toggle Markdown->Org (C-o)*: Enable/disable conversion to Org format
- *Toggle Model Colors (c)*: Enable/disable model-specific colors
- *Token Usage Graph (g)*: Display a visual graph of token usage

**** History Section

Commands for managing conversation history:

- *Toggle History (H)*: Enable/disable conversation history
- *Clear History (X)*: Clear the current history
- *Display History (V)*: Show the conversation history
- *Edit History (J)*: Edit the history in a buffer

**** Sessions Section

Commands for session management:

- *New Session (N)*: Start a new session
- *Load Session (L)*: Load a saved session
- *Save Session (S)*: Save the current session
- *List Sessions (Q)*: List all available sessions
- *Delete Session (Z)*: Delete a saved session

**** Parameters Section

Commands for managing model parameters:

- *Edit Parameter (P)*: Opens a submenu to edit specific parameters
- *Display Parameters (G)*: Show current parameter settings
- *Parameter Help (I)*: Display help information about parameters
- *Reset Parameters (K)*: Reset parameters to defaults
- *Toggle Params in Header (F)*: Show/hide parameters in header
- *Parameter Profiles (p)*: Opens the parameter profiles submenu

When you select the Edit Parameter option, you'll see a comprehensive submenu of all available parameters:

#+begin_src 
Parameters
[Generation]                [More Generation]          [Mirostat]
t  Temperature              f  Frequency Penalty       M  Mirostat Mode
k  Top K                    s  Presence Penalty        T  Mirostat Tau
p  Top P                    n  Repeat Last N           E  Mirostat Eta
m  Min P                    x  Stop Sequences
y  Typical P                l  Penalize Newline
r  Repeat Penalty

[Resource]                  [More Resource]            [Memory]
c  Num Ctx                  P  Num Predict             m  Use MMAP
b  Num Batch                S  Seed                    L  Use MLOCK
g  Num GPU                  N  NUMA                    C  Num Thread
G  Main GPU                 V  Low VRAM
K  Num Keep                 o  Vocab Only

[Profiles]                  [Actions]
d  Default Profile          D  Display All
a  Creative Profile         R  Reset All
e  Precise Profile          H  Help
A  All Profiles             F  Toggle Display in Header
                            q  Back to Main Menu
#+end_src

*** Parameter Profiles

Ollama Buddy includes predefined parameter profiles that can be applied with a single command. When you select "Parameter Profiles" from the main menu, you'll see:

#+begin_src 
Parameter Profiles
Current modified parameters: temperature, top_k, top_p
[Available Profiles]
d  Default
c  Creative
p  Precise

[Actions]
q  Back to Main Menu
#+end_src

*** Commands Submenu

The Commands submenu provides quick access to specialized operations:

#+begin_src 
Ollama Buddy Commands
[Code Operations]       [Language Operations]    [Pattern-based]         [Custom]
r  Refactor Code        l  Dictionary Lookup     f  Fabric Patterns      C  Custom Prompt
d  Describe Code        s  Synonym Lookup        u  Universal Patterns   m  Minibuffer Prompt
g  Git Commit Message   p  Proofread Text        c  Code Patterns

[Actions]
q  Back to Main Menu
#+end_src

*** Direct Keybindings

For experienced users who prefer direct keybindings, all transient menu functions can also be accessed through keybindings with the prefix of your choice (or =C-c O= when in the chat minibuffer) followed by the key shown in the menu. For example:

- =C-c O s= - Set system prompt
- =C-c O m= - Switch model
- =C-c O P= - Open parameter menu

*** Customization

The transient menu can be customized by modifying the =transient-define-prefix= definitions in the package. You can add, remove, or rearrange commands to suit your workflow.


** <2025-03-17> *0.9.5*

Added conversation history editing

- Added functions to edit conversation history (=ollama-buddy-history-edit=, =ollama-buddy-history-save=, etc.).
- Updated =ollama-buddy-display-history= to support history editing.
- Added keybinding =C-c E= for history editing.

Introducing conversation history editing!!

*Key Features*

Now, you can directly modify past interactions, making it easier to refine and manage your =ollama-buddy= chat history.

Previously, conversation history was static, you could view it but not change it. With this update, you can now:

- Edit conversation history directly in a buffer.
- Modify past interactions for accuracy or clarity.
- Save or discard changes with intuitive keybindings (=C-c C-c= to save, =C-c C-k= to cancel).
- Edit the history of all models or a specific one.

Simply use the new command *=C-c E=* to open the conversation history editor. This will display your past interactions in an editable format (alist). Once you’ve made your changes, press =C-c C-c= to save them back into Ollama Buddy’s memory.

and with a universal argument you can leverage =C-c E= to edit an individual model.

** <2025-03-17> *0.9.1*

New simple basic interface is available.

As this package becomes more advanced, I've been adding more to the intro message, making it increasingly cluttered. This could be off-putting for users who just want a simple interface to a local LLM via Ollama.

Therefore I have decided to add a customization option to simplify the menu.

Note: all functionality will still be available through keybindings, so just like Emacs then! :)

Note: some could see this initially as a breaking change as the intro message will look different, but rest assured all the functionality is still there (just to re-emphasize), so if you have been using it before and want the original functionality/intro message, just set :

#+begin_src 
(setq ollama-buddy-interface-level 'advanced)
#+end_src

#+begin_src elisp
(defcustom ollama-buddy-interface-level 'basic
  "Level of interface complexity to display.
'basic shows minimal commands for new users.
'advanced shows all available commands and features."
  :type '(choice (const :tag "Basic (for beginners)" basic)
                (const :tag "Advanced (full features)" advanced))
  :group 'ollama-buddy)
#+end_src

By default the menu will be set to Basic, unless obviously set explictly in an init file.  Here is an example of the basic menu:

#+begin_src 
,*** Welcome to OLLAMA BUDDY

,#+begin_example
 ___ _ _      n _ n      ___       _   _ _ _
|   | | |__._|o(Y)o|__._| . |_ _ _| |_| | | |
| | | | | .  |     | .  | . | | | . | . |__ |
|___|_|_|__/_|_|_|_|__/_|___|___|___|___|___|
,#+end_example

,**** Available Models

  (a) another:latest     (d) jamesio:latest
  (b) funnyname2:latest  (e) tinyllama:latest
  (c) funnyname:latest   (f) llama:latest

,**** Quick Tips

- Ask me anything!                    C-c C-c
- Change model                        C-c m
- Cancel request                      C-c k
- Browse prompt history               M-p/M-n
- Advanced interface (show all tips)  C-c A
#+end_src

and of the more advanced version

#+begin_src 
,*** Welcome to OLLAMA BUDDY

,#+begin_example
 ___ _ _      n _ n      ___       _   _ _ _
|   | | |__._|o(Y)o|__._| . |_ _ _| |_| | | |
| | | | | .  |     | .  | . | | | . | . |__ |
|___|_|_|__/_|_|_|_|__/_|___|___|___|___|___|
,#+end_example

,**** Available Models

  (a) another:latest     (d) jamesio:latest
  (b) funnyname2:latest  (e) tinyllama:latest
  (c) funnyname:latest   (f) llama:latest

,**** Quick Tips

- Ask me anything!                    C-c C-c
- Show Help/Token-usage/System-prompt C-c h/U/C-s
- Model Change/Info/Cancel            C-c m/i/k
- Prompt history                      M-p/M-n
- Session New/Load/Save/List/Delete   C-c N/L/S/Y/W
- History Toggle/Clear/Show           C-c H/X/V
- Prompt to multiple models           C-c l
- Parameter Edit/Show/Help/Reset      C-c P/G/I/K
- System Prompt/Clear   C-u/+C-u +C-u C-c C-c
- Toggle JSON/Token/Params/Format     C-c D/T/Z/C-o
- Basic interface (simpler display)   C-c A
- In another buffer? M-x ollama-buddy-menu
#+end_src



** <2025-03-17> *0.9.0*

Added command-specific parameter customization

- Added :parameters property to command definitions for granular control
- Implemented functions to apply and restore parameter settings
- Added example configuration to refactor-code command

With the latest update, you can now define specific parameter sets for each command in the menu, enabling you to optimize each AI interaction for its particular use case.

Different AI tasks benefit from different parameter settings. When refactoring code, you might want a more deterministic, precise response (lower temperature, higher repetition penalty), but when generating creative content, you might prefer more variation and randomness (higher temperature, lower repetition penalty). Previously, you had to manually adjust these parameters each time you switched between different types of tasks.

The new command-specific parameters feature lets you pre-configure the optimal settings for each use case. Here's how it works:

*** Key Features

- *Per-Command Parameter Sets*: Define custom parameter values for each command in your menu
- *Automatic Application*: Parameters are applied when running a command and restored afterward
- *Non-Destructive*: Your global parameter settings remain untouched
- *Easy Configuration*: Simple interface for adding or updating parameters

*** Example Configuration

#+begin_src elisp
;; Define a command with specific parameters
(refactor-code
 :key ?r
 :description "Refactor code"
 :prompt "refactor the following code:"
 :system "You are an expert software engineer..."
 :parameters ((temperature . 0.2) (top_p . 0.7) (repeat_penalty . 1.3))
 :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))

;; Add parameters to an existing command
(ollama-buddy-add-parameters-to-command 'git-commit
 :temperature 0.4
 :top_p 0.9
 :repeat_penalty 1.1)

;; Update properties and parameters at once
(ollama-buddy-update-command-with-params 'describe-code
 :model "codellama:latest"
 :parameters '((temperature . 0.3) (top_p . 0.8)))
#+end_src

This feature is particularly useful for:

1. *Code-related tasks*: Lower temperature for more deterministic code generation
2. *Creative writing*: Higher temperature for more varied and creative outputs
3. *Technical explanations*: Balanced settings for clear, accurate explanations
4. *Summarization tasks*: Custom parameters to control verbosity and focus

** <2025-03-16> *0.8.5*

Added system prompt support for commands

- Introduced `:system` field to command definitions.
- Added `ollama-buddy-show-system-prompt` to view active system prompt.
- Updated UI elements to reflect system prompt status.

Previously, individual menu commands in =ollama-buddy= only included a user prompt. Now, each command can define a *system prompt*, which provides background context to guide the AI's responses. This makes interactions more precise and tailored.  

*Key Features*

- *System prompts per command*: Specify background instructions for each AI-powered command using the new =:system= field.  
- *View active system prompt*: Use =C-c C-s= to display the current system prompt in a dedicated buffer.  
- *Updated UI elements*: The status line now indicates whether a system prompt is active.  

A helper function has also been added to update the default menu, for example, you might want to tweak a couple of things:

#+begin_src elisp
(use-package ollama-buddy
  :bind ("C-c o" . ollama-buddy-menu)
  :custom
  (ollama-buddy-default-model "llama3.2:3b")
  :config
  (ollama-buddy-update-menu-entry
   'refactor-code
   :model "qwen2.5-coder:7b"
   :system "You are an expert software engineer who improves code and only mainly using the principles exhibited by Ada")
  (ollama-buddy-update-menu-entry
   'git-commit
   :model "qwen2.5-coder:3b"
   :system "You are a version control expert and mainly using subversion"))
#+end_src

** <2025-03-14> *0.8.0*

Added system prompt support

- Added =ollama-buddy--current-system-prompt= variable to track system prompts
- Updated prompt area rendering to distinguish system prompts
- Modified request payload to include system prompt when set
- Enhanced status bar to display system prompt indicator
- Improved help menu with system prompt keybindings

So this is system prompt support in Ollama Buddy!, allowing you to set and manage system-level instructions for your AI interactions. This feature enables you to define a *persistent system prompt* that remains active across user queries, providing better control over conversation context.  

*Key Features*

You can now designate any user prompt as a system prompt, ensuring that the AI considers it as a guiding instruction for future interactions. To set the system prompt, use:  

#+begin_src 
C-u C-c C-c
#+end_src

*Example:*

1. Type:

#+begin_src 
Always respond in a formal tone.
#+end_src

2. Press =C-u C-c C-c= This prompt is now set as the *system prompt* and any further chat ollama responses will adhere to the overarching guidelines defined in the prompt.

If you need to clear the system prompt and revert to normal interactions, use:  

#+begin_src 
C-u C-u C-c C-c
#+end_src

*How It Works*

- The active *system prompt* is stored and sent with each user prompt.  
- A "S" indicator appears in the status bar when a system prompt is active.  
- The request payload now includes the system role, allowing AI to recognize persistent instructions.  

*Demo*

Set the system message to:

You must always respond in a single sentence.

Now ask the following:

Tell me why Emacs is so great!

Tell me about black holes

clear the system message and ask again, the reponses should now be more verbose!!

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_015.gif]]

** <2025-03-13> *0.7.4*

Added model info command, update keybindings

- Added `ollama-buddy-show-raw-model-info` to fetch and display raw JSON details 
  of the current model in the chat buffer.
- Updated keybindings:
  - `C-c i` now triggers model info display.
  - `C-c h` mapped to help assistant.
  - Improved shortcut descriptions in quick tips section.
- Removed unused help assistant entry from menu.
- Changed minibuffer-prompt key from `?i` to `?b`.

** <2025-03-12> *0.7.3*

Added function to associate models with menu commands

- Added =ollama-buddy-add-model-to-menu-entry= autoload function
- Enabled dynamic modification of command-model associations

This is a helper function that allows you to associate specific models with individual menu commands.

Configuration to apply a model to a menu entry is now straightforward, in your Emacs init file, add something like:

#+begin_src elisp
(with-eval-after-load 'ollama-buddy
  (ollama-buddy-add-model-to-menu-entry 'dictionary-lookup "tinyllama:latest")
  (ollama-buddy-add-model-to-menu-entry 'synonym "tinyllama:latest"))
#+end_src

This configures simpler tasks like dictionary lookups and synonym searches to use the more efficient TinyLlama model, while your default model will still be used for more complex operations.

** <2025-03-12> *0.7.2*

Added menu model colours back in and removed some redundant code

** <2025-03-11> *0.7.1*

Added debug mode to display raw JSON messages in a debug buffer

- Created new debug buffer to show raw JSON messages from Ollama API
- Added toggle function to enable/disable debug mode (ollama-buddy-toggle-debug-mode)
- Modified stream filter to log and pretty-print incoming JSON messages
- Added keybinding C-c D to toggle debug mode
- Updated documentation in welcome message

** <2025-03-11> *0.7.0*

Added comprehensive Ollama parameter management

- Added customization for all Ollama option API parameters with defaults
- Only send modified parameters to preserve Ollama defaults
- Display active parameters with visual indicators for modified values
- Add keybindings and help system for parameter management
- Remove redundant temperature controls in favor of unified parameters

Introduced parameter management capabilities that give you complete control over your Ollama model's behavior through the options in the ollamas API.

Ollama's API supports a rich set of parameters for fine-tuning text generation, from controlling creativity with =temperature= to managing token selection with =top_p= and =top_k=. Until now, Ollama Buddy only exposed the =temperature= parameter, but this update unlocks the full potential of Ollama's parameter system!

*** Key Features:

- *All Parameters* - set all custom options for the ollama LLM at runtime
- *Smart Parameter Management*: Only modified parameters are sent to Ollama, preserving the model's built-in defaults for optimal performance
- *Visual Parameter Interface*: Clear display showing which parameters are active with highlighting for modified values

** Keyboard Shortcuts

Parameter management is accessible through simple keyboard shortcuts from the chat buffer:

- =C-c P= - Edit a parameter
- =C-c G= - Display current parameters
- =C-c I= - Show parameter help
- =C-c K= - Reset parameters to defaults

** <2025-03-10> *0.6.1*

Refactored prompt handling so each org header line should now always have a prompt for better export

- Added functionality to properly handle prompt text when showing/replacing prompts
- Extracted inline lambdas in menu actions into named functions
- Added fallback for when no default model is set

** <2025-03-08> *0.6.0*

Chat buffer now in org-mode

- Enabled =org-mode= in chat buffer for better text structure
- Implemented =ollama-buddy--md-to-org-convert-region= for Markdown to Org conversion
- Turn org conversion on and off
- Updated keybindings =C-c C-o= to toggle Markdown to Org conversion

*Key Features*  

1. The chat buffer is now in =org-mode= which gives the buffer enhanced readability and structure. Now, conversations automatically format user prompts and AI responses with *org-mode headings*, making them easier to navigate.

2. Of course with org-mode you will now get the additional benefits for free, such as:

   - outlining
   - org export
   - heading navigation
   - source code fontification

3. Previously, responses in *Ollama Buddy* were displayed in markdown formatting, which wasn’t always ideal for *org-mode users*. Now, you can automatically convert Markdown elements, such as bold/italic text, code blocks, and lists, into proper org-mode formatting.  This gives you the flexibility to work with markdown or org-mode as needed.  

** <2025-03-07> *0.5.1*

Added temperature control

- Implemented temperature control parameter
- Added menu commands for setting (T), resetting (0)
- Added keybindings (C-c t/T/0) for quick temperature adjustments
- Updated header line and prompt displays to show current temperature
- Included temperature info in welcome screen with usage guidance

This addition gives users fine-grained control over the creativity and randomness of their AI responses through a new temperature variable.

This update adds several convenient ways to control temperature in Ollama-Buddy:

*Key Features*

1. *Direct Temperature Setting*: Use =C-c t= from the chat buffer or the menu command =[T]= to set an exact temperature value between 0.0 and 2.0.

2. *Preset Temperatures*: Quickly switch between common temperature presets with =C-c T= from the chat buffer:
   - Precise (0.1) - For factual responses
   - Focused (0.3) - For deterministic, coherent outputs
   - Balanced (0.7) - Default setting
   - Creative (0.9) - For more varied, creative responses

3. *Reset to Default*: Return to the default temperature (0.7) with =C-c 0= or the menu command =[0]=.

4. *Visual Feedback*: The current temperature is displayed in the header line and before each response, so you always know what setting you're using.

** <2025-03-06> *0.5.0*

Implemented session management, so you can now save your conversations and bring them back with the relevant context and chat history!

- Chat history is now maintained separately for each model
- Added session new/load/save/delete/list functionality
- A switch in context can now be achieved by any of the following methods:
  - Loading a previous session
  - Creating a new session
  - Clearing history on the current session
  - Toggling history on and off

*Key Benefits*

- More relevant responses when switching between models
- Prevents context contamination across different models
- Clearer session management and organization

*Key Features*

1. *Session Management*

With session management, you can now:

- *Save session* with =ollama-buddy-sessions-save= (or through the ollama-buddy-menu) Preserve your current conversation with a custom name
- *Load session* with =ollama-buddy-sessions-load= (or through the ollama-buddy-menu) Return to previous conversations exactly where you left off
- *List all sessions* with =ollama-buddy-sessions-list= (or through the ollama-buddy-menu) View all saved sessions with metadata including timestamps and models used
- *Delete session* with =ollama-buddy-sessions-delete= (or through the ollama-buddy-menu) Clean up sessions you no longer need
- *New session* with =ollama-buddy-sessions-new=  (or through the ollama-buddy-menu) Begin a clean slate without losing your saved sessions

2. *Menu Commands*

The following commands have been added to the =ollama-buddy-menu=:

- =E= New session
- =L= Load session
- =S= Save session
- =Y= List sessions
- =K= Delete session

** <2025-03-04> *0.4.1*

Added a sparse version of =ollama-buddy= called =ollama-buddy-mini=, see the github repository for the elisp file and a description in =README-mini.org=

** <2025-03-03> *0.4.0*

Added conversation history support and navigation functions

- Implemented conversation history tracking between prompts and responses
- Added configurable history length limits and visual indicators
- Created navigation functions to move between prompts/responses in buffer

*Key Features*

1. *Conversation History*

Ollama Buddy now maintains context between your interactions by:

- Tracking conversation history between prompts and responses
- Sending previous messages to Ollama for improved contextual responses
- Displaying a history counter in the status line showing conversation length
- Providing configurable history length limits to control memory usage

You can control this feature with:

#+begin_src elisp
;; Enable/disable conversation history (default: t)
(setq ollama-buddy-history-enabled t)

;; Set maximum conversation pairs to remember (default: 10)
(setq ollama-buddy-max-history-length 10)

;; Show/hide the history counter in the header line (default: t)
(setq ollama-buddy-show-history-indicator t)
#+end_src

2. *Enhanced Navigation*

Moving through longer conversations is now much easier with:

- Navigation functions to jump between prompts using C-c n/p

3. *Menu Commands*

Three new menu commands have been added:

- =H=: Toggle history tracking on/off
- =X=: Clear the current conversation history
- =V=: View the full conversation history in a dedicated buffer

** <2025-03-02> *0.3.1*

Enhanced model colour contrast with themes, allowing =ollama-buddy-enable-model-colors= to be enabled by default.

** <2025-03-01> *0.3.0*

Added real-time token usage tracking and display

- Introduce variables to track token counts, rates, and usage history
- Implement real-time token rate updates with a timer
- Add a function to display token usage statistics in a dedicated buffer
- Allow toggling of token stats display after responses
- Integrate token tracking into response processing and status updates
- Ensure cleanup of timers and tracking variables on completion or cancellation

*Key Features*

1. *Menu Commands*

   The following command has been added to the =ollama-buddy-menu=:

   - =t= Show a summary of token model usage stats

** <2025-02-28> *0.2.4*

Added model-specific color highlighting

- Introduce `ollama-buddy-enable-model-colors` (default: nil) to toggle model-based color highlighting.
- Assign consistent colors to models based on string hashing.
- Apply colors to model names in the menu, status, headers, and responses.
- Add `ollama-buddy-toggle-model-colors` command to toggle this feature.

This enhancement aims to improve user experience by visually distinguishing different AI models within the interface.

Note: I am likely to use both *colour* and *color* interchangeably in the following text! :)

*Key Features*

1. *Model-Specific Colors*
   
   - A new customizable variable, =ollama-buddy-enable-model-colors=, allows users to enable or disable model-specific colors.
   - Colors are generated based on a model's name using a hashing function that produces consistent and visually distinguishable hues.
   - However there could be an improvement regarding ensuring the contrast is sufficient and hence visibility maintained with differing themes.

2. *Interactive Color Toggle*
   - Users can toggle model-specific colors with the command =ollama-buddy-toggle-model-colors=, providing flexibility in interface customization.

4. *Colored Model Listings*
   - Model names are now displayed with their respective colors in various parts of the interface, including:
     - The status line
     - Model selection menus
     - Command definitions
     - Chat history headers

5. *Menu Commands*

The following command hashing been added to the =ollama-buddy-menu=:

- =C= Toggle colors
   
** <2025-02-28> *0.2.3*

Added Prompt History Support

- Prompts are now integrated into the Emacs history mechanism which means they persist across sessions.  
- Use =M-p= to navigate prompt history, and =M-p= / =M-n= within the minibuffer to insert previous prompts.  

*Key Features*

- Persistent prompt history
- A new variable, =ollama-buddy--prompt-history=, now keeps track of past prompts. This means you can quickly recall and reuse previous queries instead of retyping them from scratch.
- =M-p= - recall a previous prompt in the buffer which will bring up the minibuffer for prompt history selection.
- Minibuffer =M-p= / =M-n= - Navigate through past prompts when prompted for input.

** <2025-02-27> *0.2.2*

Added support for role-based presets

- Introduced `ollama-buddy-roles-directory` for storing role preset files.
- Implemented interactive functions to manage roles:
  - `ollama-buddy-roles-switch-role`
  - `ollama-buddy-role-creator-create-new-role`
  - `ollama-buddy-roles-open-directory`
- Added ability to create and switch between role-specific commands.
- Updated menu commands to include role management options.

This enhancement allows you to create, switch, and manage role-specific command configurations, which basically generates differing menu layouts and hence command options based on your context, making your workflow more personalized and efficient.  

*What Are Role-Based Presets?*

Roles in Ollama Buddy are essentially *profiles* tailored to specific tasks. Imagine you're using Ollama Buddy for:  

- *Coding assistance* with one set of prompts
- *Creative writing* with a different tone and response style
- *Generating Buffy Style Quips* - just a fun one!

With this update, you can now create presets for each of these contexts and switch between them seamlessly without manually re-configuring settings every time. On each switch of context and hence role, a new ollama buddy menu will be generated with the associated keybinding attached to the relevant context commands.

*Key Features*

*1. Store Your Custom Roles*

A new directory =ollama-buddy-roles-directory= (defaulting to =~/.emacs.d/ollama-buddy-presets/=) now holds your role presets. Each role is saved as an =.el= file containing predefined *commands*, *shortcuts*, and *model preferences*.  

*2. Easily Switch Between Roles*

With =M-x ollama-buddy-roles-switch-role= you can pick from available role presets and swap effortlessly between them (or use the menu item from =ollama-buddy-menu=)

*3. Create Custom Roles with Unique Commands*

You can now define *custom commands* for each role with =M-x ollama-buddy-role-creator-create-new-role= (or the menu item from =ollama-buddy-menu=)

This interactive function allows you to:  

- Assign menu shortcuts to commands  
- Describe command behaviour  
- Set a default AI model  
- Define a system prompt for guiding responses  

Once saved, your new role is ready to load anytime!  

*4. Open Role Directory in Dired*

Need to tweak a role manually? A simple, run =M-x ollama-buddy-roles-open-directory= or of course also from the =ollama-buddy-menu= which opens the presets folder in *dired*, where you can quickly edit, copy, or delete role configurations.

*5. Preconfigured presets are available if you'd like to use a ready-made setup.*

- ollama-buddy--preset__buffy.el
- ollama-buddy--preset__default.el
- ollama-buddy--preset__developer.el
- ollama-buddy--preset__janeway.el
- ollama-buddy--preset__translator.el
- ollama-buddy--preset__writer.el

If these files are put in the =ollama-buddy-roles-directory= then the role selection menu will pass through completing-read, and present the following:

{buffy | default | developer | janeway | translator | writer}

With the selection regenerating the =ollama-buddy-menu= accordingly, and off you go.

*6. Menu commands*

The following commands have been added to the =ollama-buddy-menu=:

- =R= Switch Role
- =N= Create New Role
- =D= Open Roles Directory

** <2025-02-26> *0.2.1*

Added multishot execution with model selection  (See multishot section for description of new feature!)

- Assign letters to models for quick selection
- Implement multishot mode for sequential requests to multiple models
- Store responses per model in registers named after assigned letters
- Display multishot progress in status
- Bind `C-c C-l` to trigger multishot prompt

With the new *multishot mode*, you can now send a prompt to multiple models in sequence, and compare their responses, the results are also available in named registers.

*Key Features*

*1. Letter-Based Model Shortcuts*

Instead of manually selecting models, each available model is now assigned a *letter* (e.g., =(a) mistral=, =(b) gemini=). This allows for quick model selection when sending prompts or initiating a *multishot sequence*.

*2. Multishot Execution (=C-c C-l=)*

Ever wondered how different models would answer the same question? With *Multishot Mode*, you can:

- Send your prompt to a sequence of models in one shot.  
- Track progress as responses come in.  
- Store each model’s response in a *register*, making it easy to reference later, each assigned model letter corresponds to the named register.

*3. Status Updates*

When running a multishot execution, the status now updates dynamically:

- *"Multi Start"* when the sequence begins.  
- *"Processing..."* during responses.  
- *"Multi Finished"* when all models have responded.  

*4. How It Works*

1. *=C-c C-l=* to start a multishot session in the chat buffer.
2. Type a sequence of model letters (e.g., =abc= to use models =mistral=, =gemini=, and =llama=).  
3. The selected models will process the prompt *one by one*.  
4. The responses will be saved to registers of the same named letter for recalling later.
  
** <2025-02-19> *0.2.0*

Improved prompt handling in chat buffer and simplified setup

- Chat buffer now more prompt based rather than ad-hoc using C-c C-c to send and C-c C-k to cancel
- Connection monitor now optional, ollama status visibility now maintained by strategic status checks simplifying setup.
- Can now change models from chat buffer using C-c C-m
- Updated intro message with ascii logo
- Suggested default "C-c o" for =ollama-buddy-menu=
- defcustom ollama-buddy-command-definitions now will work in the customization interface.

** <2025-02-13>

Models can be assigned to individual commands

- Set menu :model property to associate a command with a model
- Introduce `ollama-buddy-fallback-model` for automatic fallback if the specified model is unavailable.
- Improve `ollama-buddy--update-status-overlay` to indicate model substitution.
- Expand `ollama-buddy-menu` with structured command definitions using properties for improved flexibility.
- Add `ollama-buddy-show-model-status` to display available and used models.
- Refactor command execution flow to ensure model selection is handled dynamically.
