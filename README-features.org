#+title: Ollama Buddy: Local LLM Integration for Emacs
#+author: James Dyer
#+email: captainflasmr@gmail.com
#+language: en
#+options: ':t toc:nil author:nil email:nil num:nil title:nil
#+todo: TODO DOING | DONE
#+startup: showall

* Features as they come in

** <2025-05-19> *0.11.0*

Added user system prompts management

- You can now save, load and manage system prompts
- Created new transient menu for user system prompts (C-c s)
- Organized prompts by categories with org-mode format storage
- Supported prompt editing, listing, creation and deletion
- Updated key bindings to integrate with existing functionality
- Added prompts directory customization with defaults

This feature makes it easier to save, organize, and reuse your favorite system prompts when working with Ollama language models.

System prompts are special instructions that guide the behavior of language models. By setting effective system prompts, you can:

- Define the AI's role (e.g., "You are a helpful programming assistant who explains code clearly")
- Establish response formats
- Set the tone and style of responses
- Provide background knowledge for specific domains

The new =ollama-buddy-user-prompts= module organizes your system prompts in a clean, category-based system:

- *Save your prompts* - Store effective system prompts you've crafted for future use
- *Categorize* - Prompts are organized by domains like "coding," "writing," "technical," etc.
- *Quick access* - Browse and load your prompt library with completion-based selection
- *Edit in org-mode* - All prompts are stored as org files with proper metadata
- *Manage with ease* - Create, edit, list, and delete prompts through a dedicated transient menu

The new functionality is accessible through the updated key binding =C-c s=, which opens a dedicated transient menu with these options:

- *Save current (S)* - Save your active system prompt
- *Load prompt (L)* - Choose a previously saved prompt
- *Create new (N)* - Start fresh with a new prompt
- *List all Prompts (l)* - View your entire prompt library
- *Edit prompt (e)* - Modify an existing prompt
- *Delete prompt (d)* - Remove prompts you no longer need

If you work frequently with Ollama models, you've likely discovered the power of well-crafted system prompts. They can dramatically improve the quality and consistency of responses. With this new management system, you can:

- Build a personal library of effective prompts
- Maintain context continuity across sessions
- Share prompts with teammates
- Refine your prompts over time

** <2025-05-14> *0.10.0*

Added file attachment system for including documents in conversations

- Added file attachment support with configurable file size limits (10MB default) and supported file types
- Implemented session persistence for attachments in save/load functionality  
- Added attachment context inclusion in prompts with proper token counting
- Created comprehensive attachment management commands:
  - Attach files to conversations
  - Show current attachments in dedicated buffer
  - Detach specific files
  - Clear all attachments
- Added Dired integration for bulk file attachment
- Included attachment menu in transient interface (C-c 1)
- Updated help text to document new attachment keybindings
- Enhanced context calculation to include attachment token usage

You can now seamlessly include text files, code, documentation, and more directly in your conversations with local AI models!

Simply use =C-c C-a= from the chat buffer to attach any file to your current conversation.

The attached files become part of your conversation context, allowing the AI to reference, analyze, or work with their contents directly.

The transient menu has also been updated with a new *Attachment Menu*

#+begin_src 
*File Attachments*
  a Attach file
  w Show attachments
  d Detach file
  0 Clear all attachments
#+end_src

Your attachments aren't just dumped into the conversation - they're intelligently integrated:

- *Token counting* now includes attachment content, so you always know how much context you're using
- *Session persistence* means your attachments are saved and restored when you save/load conversations
- *File size limits* (configurable, 10MB default) prevent accidentally overwhelming your context window

Managing attached files is intuitive with dedicated commands:

- =C-c C-w= - View all current attachments in a nicely formatted org mode buffer, folded to each file
- =C-c C-d= - Detach specific files when you no longer need them
- =C-c 0= - Clear all attachments at once
- =C-c 1= - Access the full attachment menu via a transient interface

Working in Dired? No problem! You can attach files directly from your file browser:

- Mark multiple files and attach them all at once
- Attach the file at point with a single command

Use the configuration as follows:

#+begin_src elisp
(eval-after-load 'dired
  '(progn
     (define-key dired-mode-map (kbd "C-c C-a") #'ollama-buddy-dired-attach-marked-files)))
#+end_src

** <2025-05-12> *0.9.50*

Added context size management and monitoring

- Added configurable context sizes for popular models (llama3.2, mistral, qwen, etc.)
- Implemented real-time context usage display in status bar
- Can display in text or bar display types
- Added context size thresholds with visual warnings
- Added interactive commands for context management:
  - =ollama-buddy-show-context-info=: View all model context sizes
  - =ollama-buddy-set-model-context-size=: Manually configure model context
  - =ollama-buddy-toggle-context-percentage=: Toggle context display
- Implemented context size validation before sending prompts
- Added token estimation and breakdown (history/system/current prompt)
- Added keybindings: C-c $ (set context), C-c % (toggle display), C-c C (show info)
- Updated status bar to show current/max context with fontification

I've added context window management and monitoring capabilities to Ollama Buddy!

This update helps you better understand and manage your model's context usage, preventing errors and optimizing your conversations.

Enable it with the following:

#+begin_src elisp
(setq ollama-buddy-show-context-percentage t)
#+end_src

*** Usage

After implementing these changes:

1. *Text mode* (default): Shows =1024/4096= style display
2. *Bar mode*: Shows =███████░░░░ 2048= style display
3. Use =C-c 8= to toggle between modes
4. The *Text mode* will change colors based on your thresholds:
   - Normal: regular colors
   - Amber (85%+): underlined and bold
   - Red (100%+): inverse video and bold
5. The *Bar mode* will just fill up as normal

The progress bar will visually represent how much of the context window you're using, making it easier to see at a glance when you're approaching the limit.

*** Implementation Details

**** Context Size Detection

Determining a model's context size proved more complex than expected. While experimenting with parsing model info JSON, I discovered that context size information can be scattered across different fields. Rather than implementing a complex JSON parser (which may come later), I chose a pragmatic approach:

I created a new =defcustom= variable =ollama-buddy-fallback-context-sizes= that includes hard-coded values for popular Ollama models. The fallback mechanism is deliberately simple: substring matching followed by a sensible default of 4096 tokens.

#+begin_src elisp
(defcustom ollama-buddy-fallback-context-sizes
  '(("llama3.2:1b" . 2048)
    ("llama3:8b" . 4096)
    ("tinyllama" . 2048)
    ("phi3:3.8b" . 4096)
    ("gemma3:1b" . 4096)
    ("gemma3:4b" . 8192)
    ("llama3.2:3b" . 8192)
    ("llama3.2:8b" . 8192)
    ("llama3.2:70b" . 8192)
    ("starcoder2:3b" . 8192)
    ("starcoder2:7b" . 8192)
    ("starcoder2:15b" . 8192)
    ("mistral:7b" . 8192)
    ("mistral:8x7b" . 32768)
    ("codellama:7b" . 8192)
    ("codellama:13b" . 8192)
    ("codellama:34b" . 8192)
    ("qwen2.5-coder:7b" . 8192)
    ("qwen2.5-coder:3b" . 8192)
    ("qwen3:0.6b" . 4096)
    ("qwen3:1.7b" . 8192)
    ("qwen3:4b" . 8192)
    ("qwen3:8b" . 8192)
    ("deepseek-r1:7b" . 8192)
    ("deepseek-r1:1.5b" . 4096))
  "Mapping of model names to their default context sizes.
Used as a fallback when context size can't be determined from the API."
  :type '(alist :key-type string :value-type integer)
  :group 'ollama-buddy)
#+end_src

This approach may not be perfectly accurate for all models, but it's sufficient for getting the core functionality working. More importantly, as a =defcustom=, users can easily customize these values for complete accuracy with their specific models. Users can also set context values within the chat buffer through =C-c C= (Show Context Information) for each individual model if desired.

This design choice allowed me to focus on the essential features without getting stuck on complex context retrieval logic.

One final thing!, if the ~num_ctx: Context window size in tokens~ is set, then that number will also be taken into consideration.  An assumption will be made that the model is honouring the context size requested and will incorporated into the context calculations accordingly.

**** Token Estimation

For token counting, I've implemented a simple heuristic: each word (using string-split) is multiplied by 1.3. This follows commonly recommended approximations and works well enough in practice. While this isn't currently configurable, I may add it as a customization option in the future.

*** How to Use Context Management in Practice

The =C-c C= (Show Context Information) command is central to this feature. Rather than continuously monitoring context size while you type (which would be computationally expensive and potentially distracting), I've designed the system to calculate context on-demand when you choose.

**** Typical Workflows

*Scenario 1: Paste-and-Send Approach*

Let's say you want to paste a large block of text into the chat buffer. You can simply:

1. Paste your content
2. Press the send keybinding
3. If the context limit is exceeded, you'll get a warning dialog asking whether to proceed anyway

*Scenario 2: Preemptive Checking*

For more control, you can check context usage before sending:

1. Paste your content
2. Run =C-c C= to see the current context breakdown
3. If the context looks too high, you have several options:
   - Trim your current prompt
   - Remove or simplify your system prompt
   - Edit conversation history using Ollama Buddy's history modification features
   - Switch to a model with a larger context window

*Scenario 3: Manage the Max History Length*

Want tight control over context size without constantly monitoring the real-time display? Since conversation history is part of the context, you can simply limit =ollama-buddy-max-history-length= to control the total context size.

For example, when working with small context windows, set =ollama-buddy-max-history-length= to 1. This keeps only the last exchange (your prompt + model response), ensuring your context remains small and predictable, perfect for maintaining control without manual monitoring.

*Scenario 4: Parameter num_ctx: Context window size in tokens*

Simply set this parameter and off you go!

*** Current Status: Experimental

Given the potentially limiting nature of context management, I've set this feature to *disabled by default*.

But to enable set the following :

#+begin_src elisp
(setq ollama-buddy-show-context-percentage t)
#+end_src

This means:

- Context checks won't prevent sending prompts
- Context usage won't appear in the status line
- However, calculations still run in the background, so =C-c C= (Show Context Information) remains functional

As the feature matures and proves its value, I may enable it by default. For now, consider it an experimental addition that users can opt into.

*** More Details

The status bar now displays your current context usage in real-time. You'll see a fraction showing used tokens versus the model's maximum context size (e.g., "2048/8192"). The display automatically updates as your conversation grows.

Context usage changes fontification to help you stay within limits:

- *Normal font*: Normal usage (under 85%)
- *Bold and Underlined*: Approaching limit (85-100%)
- *Inversed*: At or exceeding limit (100%+)

Before sending prompts that exceed the context limit, Ollama Buddy now warns you and asks for confirmation. This prevents unexpected errors and helps you manage long conversations more effectively.

There are now three new interactive commands:

=C-c $= - Set Model Context Size. Manually configure context sizes for custom or fine-tuned models.

=C-c %= - Toggle Context Display. Show or hide the context percentage in the status bar.
  
=C-c C= - Show Context Information. View a detailed breakdown of:

- All model context sizes
- Current token usage by category (history, system prompt, current prompt)
- Percentage usage

-----

The system estimates token counts for:

- *Conversation history*: All previous messages
- *System prompts*: Your custom instructions
- *Current input*: The message you're about to send

This gives you a complete picture of your context usage before hitting send.

The context monitoring is not enabled by default.

** <2025-05-05> *0.9.44*

- Sorted model names alphabetically in intro message
- Removed multishot writing to register name letters

For some reason, when I moved the .ollama folder to an external disk, the models returned with api/tags were inconsistent, which meant it broke consistent letter assignment. I'm not sure why this happened, but it is probably sensible to sort the models alphabetically anyway, as this has the benefit of naturally grouping together model families.

I also removed the multishot feature of writing to the associated model letter. Now that I have to accommodate more than 26 models, incorporating them into the single-letter Emacs register system is all but impossible. I suspect this feature was not much used, and if you think about it, it wouldn't have worked anyway with multiple model shots, as the register letter associated with the model would just show the most recent response. Due to these factors, I think I should remove this feature. If someone wants it back, I will probably have to design a bespoke version fully incorporated into the ollama-buddy system, as I can't think of any other Emacs mechanism that could accommodate this.

** <2025-05-05> *0.9.43*

Fix model reference error exceeding 26 models #15

Update =ollama-buddy= to handle more than 26 models by using prefixed combinations for model references beyond 'z'. This prevents errors in =create-intro-message= when the local server hosts a large number of models.

** <2025-05-03> *0.9.42*

Added the following to recommended models:

- qwen3:0.6b
- qwen3:1.7b
- qwen3:4b
- qwen3:8b

and fixed pull model

** <2025-05-02> *0.9.41*

Refactored model prefixing again so that when using only ollama models no prefix is applied and is only applied when online LLMs are selected (for example claude, chatGPT e.t.c)

I think this makes more sense and is cleaner for I suspect the majority who may use this package are probably more interested in just using ollama models and the prefix will probably be a bit confusing.

This could be a bit of a breaking change once again I'm afraid for those ollama users that have switched and are now familiar with prefixing "o:", sorry!

** <2025-05-02> *0.9.40*

Added vision support for those ollama models that can support it!

Image files are now detected within a prompt and then processed if a model can support vision processing. Here's a quick overview of how it works:

1. *Configuration*: Users can configure the application to enable vision support and specify which models and image formats are supported.  Vision support is enabled by default.
   
2. *Image Detection*: When a prompt is submitted, the system automatically detects any image files referenced in the prompt.
   
3. *Vision Processing*: If the model supports vision, the detected images are processed in relation to the defined prompt. Note that the detection of a model being vision capable is defined in =ollama-buddy-vision-models= and can be adjusted as required.

4. In addition, a menu item has been added to the custom ollama buddy menu :

   #+begin_src 
   [I] Analyze an Image
   #+end_src

When selected, it will allow you to describe a chosen image. At some stage, I may allow integration into =dired=, which would be pretty neat. :)

** <2025-04-29> *0.9.38*

Added model unloading functionality to free system resources

- Add unload capability for individual models via the model management UI
- Create keyboard shortcut (C-c C-u) for quick unloading of all models
- Display running model count and unload buttons in model management buffer

Large language models consume significant RAM and GPU memory while loaded. Until now, there wasn't an easy way to reclaim these resources without restarting the Ollama server entirely. This new functionality allows you to:

- Free up GPU memory when you're done with your LLM sessions
- Switch between resource-intensive tasks more fluidly
- Manage multiple models more efficiently on machines with limited resources
- Avoid having to restart the Ollama server just to clear memory

There are several ways to unload models with the new functionality:

1. *Unload All Models*: Press =C-c C-u= to unload all running models at once (with confirmation)

2. *Model Management Interface*: Access the model management interface with =C-c W= where you'll find:
   - A counter showing how many models are currently running
   - An "Unload All" button to free all models at once
   - Individual "Unload" buttons next to each running model

3. *Quick Access in Management Buffer*: When in the model management buffer, simply press =u= to unload all models

The unloading happens asynchronously in the background, with clear status indicators so you can see when the operation completes.

** <2025-04-25> *0.9.37*

- Display modified parameters in token stats

Enhanced the token statistics section to include any modified parameters, providing a clearer insight into the active configurations. This update helps in debugging and understanding the runtime environment.

** <2025-04-25> *0.9.36*

Added Reasoning/Thinking section visibility toggle functionality

- Introduced the ability to hide reasoning/thinking sections during AI responses, making the chat output cleaner and more focused on final results
- Added a new customizable variable =ollama-buddy-hide-reasoning= (default: nil) which controls visibility of reasoning sections
- Added =ollama-buddy-reasoning-markers= to configure marker pairs that encapsulate reasoning sections (supports multiple formats like <think></think> or ----)
- Added =ollama-buddy-toggle-reasoning-visibility= interactive command to switch visibility on/off
- Added keybinding =C-c V= for toggling reasoning visibility in chat buffer 
- Added transient menu option "V" for toggling reasoning visibility
- When reasoning is hidden, a status message shows which section is being processed (e.g., "Think..." or custom marker names)
- Reasoning sections are automatically detected during streaming responses
- Header line now indicates when reasoning is hidden with "REASONING HIDDEN" text
- All changes preserve streaming response functionality while providing cleaner output

This feature is particularly useful when working with AI models that output their "chain of thought" or reasoning process before providing the final answer, allowing users to focus on the end results while still having the option to see the full reasoning when needed.

** <2025-04-21> *0.9.35*

Added Grok support

Integration is very similar to other remote AIs:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-grok-api-key
   (auth-source-pick-first-password :host "ollama-buddy-grok" :user "apikey"))
  :config
  (require 'ollama-buddy-grok nil t))
#+end_src

** <2025-04-20> *0.9.33*

Fixed utf-8 encoding stream response issues from remote LLMs.

** <2025-04-19> *0.9.32*

Finished the remote LLM decoupling process, meaning that the core =ollama-buddy= logic is now not dependent on any remote LLM, and each remote LLM package is self-contained and functions as a unique extension.

** <2025-04-18> *0.9.31*

Refactored model prefixing logic and cleaned up

- Standardized model prefixing by introducing distinct prefixes for Ollama (=o:=), OpenAI (=a:=), Claude (=c:=), and Gemini (=g:=) models.
- Centralized functions to get full model names with prefixes across different model types.
- Removed redundant and unused variables related to model management.

Note that there may be some breaking changes here especially regarding session recall as all models will now have a prefix to uniquely identify their type.  For =ollama= recall, just edit the session files to prepend the ollama prefix of "o:"

** <2025-04-17> *0.9.30*

Added Gemini integration!

As with the Claude and ChatGPT integration, you will need to add something similar to them in your configuration. I currently have the following set up to enable access to the remote LLMs:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (ollama-buddy-claude-api-key
   (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (ollama-buddy-gemini-api-key
   (auth-source-pick-first-password :host "ollama-buddy-gemini" :user "apikey"))
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  (require 'ollama-buddy-gemini nil t))
#+end_src

Also with the previous update all the latest model names will be pulled, so there should be a full comprehensive list for each of the main remote AI LLMs!

** <2025-04-17> *0.9.23*

Refactored history and model management for remote LLMs

- Now pulling in latest model list for remote LLMs (so now ChatGPT 4.1 is available!)
- Removed redundant history and model management functions from =ollama-buddy-claude.el= and =ollama-buddy-openai.el=. Replaced them with shared implementations to streamline code and reduce duplication

** <2025-04-17> *0.9.23*

Refactored history and model management for remote LLMs

Removed redundant history and model management functions from =ollama-buddy-claude.el= and =ollama-buddy-openai.el=. Replaced them with shared implementations to streamline code and reduce duplication

** <2025-04-15> *0.9.22*

Enhanced session management

- Refactored =ollama-buddy-sessions-save= to autogenerate session names using timestamp and model.
- Improved session saving/loading by integrating org file handling.
- Updated mode line to display current session name dynamically.

Several improvements to session management, making it more intuitive and efficient for users. Here's a breakdown of the new functionality:

When saving a session, Ollama Buddy now creates a default name using the current timestamp and model name, users can still provide a custom name if desired.

An org file is now saved alongside the original elisp session file. This allows for better session recall as all interactions will be pulled back with the underlying session parameters still restored as before. There is an additional benefit in not only recalling precisely the session and any additional org interactions but also quickly saving to an org file for potential later inspection. Along with the improved autogenerated session name, this means it is much faster and more intuitive to save a snapshot of the current chat interaction.

The modeline now displays the current session name!

** <2025-04-11> *0.9.21*

Add history edit/view toggle features, so effectively merging the former history display into the history edit functionality.

** <2025-04-04> *0.9.20*

- Added =ollama-buddy-awesome.el= to integrate Awesome ChatGPT Prompts.

=ollama-buddy-awesome= is an =ollama-buddy= extension that integrates the popular [[https://github.com/f/awesome-chatgpt-prompts][Awesome ChatGPT Prompts]] repository, allowing you to leverage hundreds of curated prompts for various tasks and roles right within your Emacs environment, I thought that since I have integrated the =fabric= set of curated prompts then why not these!

*** Key Features

1. *Seamless Sync*: Automatically fetch the latest prompts from the GitHub repository, ensuring you always have access to the most up-to-date collection.

2. *Smart Categorization*: Prompts are intelligently categorized based on their content, making it easy to find the perfect prompt for your task.

3. *Interactive Selection*: Choose prompts through Emacs' familiar completion interface, with category and title information for quick identification.

4. *Effortless Application*: Apply selected prompts as system prompts in ollama-buddy with a single command, streamlining your AI-assisted workflow.

5. *Prompt Management*: List available prompts, preview their content, and display full prompt details on demand.

*** Getting Started

To access the Awesome ChatGPT prompts, just select the transient menu as normal and select "[a] Awesome ChatGPT Prompts", this will fetch the prompts and prepare everything for your first use and give you a transient menu as follows:

#+begin_example
Actions
[s] Send with Prompt
[p] Set as System Prompt
[l] List All Prompts
[c] Category Browser
[S] Sync Latest Prompts
[q] Back to Main Menu
#+end_example

Now available are a vast array of role-based and task-specific prompts, enhancing your =ollama-buddy= interactions in Emacs.

** <2025-04-01> *0.9.17*

- Added link to =ollama-buddy= info manual from the chat buffer and transient menu as MELPA has now picked it up and installed it!

** <2025-03-28> *0.9.16*

- Added =ollama-buddy-fix-encoding-issues= to handle text encoding problems.
- Refactored and streamline fabric pattern description handling.
- Removed unused fabric pattern categories to enhance maintainability.

** <2025-03-28> *0.9.15*

- Implement asynchronous operations for model management
  - Introduce non-blocking API requests for fetching, copying, and deleting models
- Add caching mechanisms to improve efficiency
  - Cache model data to reduce redundant API calls
  - Manage cache expiration with timestamps and time-to-live settings
- Update status line to reflect ongoing background operations
- Ensure smooth user interaction by minimizing wait times and enhancing performance

** <2025-03-26> *0.9.13*

- Added automatic writing of last response to a register
- Added M-r to search through prompt history

I was just thinking about a general workflow aspect and that is getting responses out of the =ollama-buddy= chat buffer.  Of course if you are already there then it will be easier, but even then the latest prompt, which is probably the one you are interested in will still have to be copied to the kill ring.

This issue is even more pronounced when you are sending text from other buffers to the chat.

So, the solution I have put in place is to always write the last response to a register of your choice.  I always think registers are an underused part of Emacs, I already have repurposed them for the multishot, so why not always make the last response available.

For example, you want to proofread a sentence, you can mark the text, send to the chat using the custom menu to proofread then the response will be available in maybe register "a".  The chat buffer will be brought up if not already visible so you can validate the output, then pop back to your buffer, delete the paragraph and insert the register "a"?, maybe.  I am going to put this in as I suspect no-one uses registers anyway and if they do, they can push the response writing register away using =ollama-buddy-default-register=, I don't think this will do any harm, and actually it is something I may starting using more often.

As a side note, I also need to think about popping into the chat buffer with a buffer text push to the chat, should I do it?, not sure yet, still getting to grips with the whole workflow aspect, so will need a little more time to see what works.

Also as a side note to this ramble, the general register prefix is annoyingly long =C-x r i <register>= so I have rebound in my config to =M-a=, as I never want to go back a sentence and also if I just write to the default "a" register then it feels ergonomically fast.

** <2025-03-25> *0.9.12*

- Added experimental Claude AI support!
- removed curl and replaced with url.el for online AI integration

A very similar implementation as for ChatGPT.

To activate, set the following:

#+begin_src elisp
(require 'ollama-buddy-claude nil t)
(ollama-buddy-claude-api-key "<extremely long key>")
#+end_src

** <2025-03-24> *0.9.11*

Added the ability to toggle streaming on and off

- Added customization option to enable/disable streaming mode
- Implemented toggle function with keybindings (C-c x) and transient menu option
- Added streaming status indicator in the modeline

The latest update introduces the ability to toggle between two response modes:

- *Streaming mode (default)*: Responses appear token by token in real-time, giving you immediate feedback as the AI generates content.
- *Non-streaming mode*: Responses only appear after they're fully generated, showing a "Loading response..." placeholder in the meantime.

While watching AI responses stream in real-time is often helpful, there are situations where you might prefer to see the complete response at once:

- When working on large displays where the cursor jumping around during streaming is distracting
- When you want to focus on your work without the distraction of incoming tokens until the full response is ready

The streaming toggle can be accessed in several ways:

1. Use the keyboard shortcut =C-c x=
2. Press =x= in the transient menu
3. Set the default behavior through customization:
   #+begin_src elisp
   (setq ollama-buddy-streaming-enabled nil) ;; Disable streaming by default
   #+end_src

The current streaming status is visible in the modeline indicator, where an "X" appears when streaming is disabled.

** <2025-03-22> *0.9.10*

Added experimental OpenAI support!

Yes, that's right, I said I never would do it, and of course, this package is still very much =ollama=-centric, but I thought I would just sneak in some rudimentary ChatGPT support, just for fun!

It is a very simple implementation, I haven't managed to get streaming working, so Emacs will just show "Loading Response..." as it waits for the response to arrive. It is asynchronous, however, so you can go off on your Emacs day while it loads (although being ChatGPT, you would think the response would be quite fast!)

By default, OpenAI/ChatGPT will not be enabled, so anyone wanting to use just a local LLM through =ollama= can continue as before. However, you can now sneak in some experimental ChatGPT support by adding the following to your Emacs config as part of the =ollama-buddy= set up.

#+begin_src elisp
(require 'ollama-buddy-openai nil t)
(setq ollama-buddy-openai-api-key "<big long key>")
#+end_src

and you can set the default model to ChatGPT too!

#+begin_src elisp
(setq ollama-buddy-default-model "GPT gpt-4o")
#+end_src

Note that to store the key you would probably want to choose either of the following methods so a sensitive key doesn't get stored in your Emacs init file:

Using =auth-source= (which includes authinfo) or =password-store= are both good options for securely storing and retrieving sensitive information. Here's how you can modify your configuration to use these methods:

1. Using auth-source (authinfo):

First, add your API keys to your =~/.authinfo= or =~/.authinfo.gpg= file (the latter is encrypted):

#+begin_src 
machine ollama-buddy-openai login apikey password <your-openai-api-key>
machine ollama-buddy-claude login apikey password <your-claude-api-key>
#+end_src

Then, modify your Emacs configuration:

#+begin_src elisp
(use-package ollama-buddy
  :load-path "~/source/repos/ollama-buddy"
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (auth-source-pick-first-password :host "ollama-buddy-openai" :user "apikey"))
  (ollama-buddy-default-model "GPT gpt-4o")
  (ollama-buddy-claude-api-key
   (auth-source-pick-first-password :host "ollama-buddy-claude" :user "apikey"))
  (ollama-buddy-claude-default-model "claude-3-sonnet-20240229")
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  ;; ... rest of your configuration
  )
#+end_src

2. Using password-store:

First, ensure you have =password-store= set up and add your API keys:

#+begin_src 
pass insert ollama-buddy/openai-api-key
pass insert ollama-buddy/claude-api-key
#+end_src

Then, modify your Emacs configuration:

#+begin_src elisp
(use-package password-store)

(use-package ollama-buddy
  :load-path "~/source/repos/ollama-buddy"
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key
   (password-store-get "ollama-buddy/openai-api-key"))
  (ollama-buddy-default-model "GPT gpt-4o")
  (ollama-buddy-claude-api-key
   (password-store-get "ollama-buddy/claude-api-key"))
  (ollama-buddy-claude-default-model "claude-3-sonnet-20240229")
  :config
  (require 'ollama-buddy-openai nil t)
  (require 'ollama-buddy-claude nil t)
  ;; ... rest of your configuration
  )
#+end_src

Both of these methods will securely store your API keys and retrieve them when needed, keeping them out of your Emacs configuration file. The =lambda= functions ensure that the keys are only retrieved when they're actually needed.

With this enabled, chat will present a list of ChatGPT models to choose from. The custom menu should also now work with chat, so from anywhere in Emacs, you can push predefined prompts to the =ollama= buddy chat buffer now supporting ChatGPT.

There is more integration required to fully incorporate ChatGPT into the =ollama= buddy system, like token rates and history, etc. But not bad for a first effort, methinks!

Here is my current config, now mixing ChatGPT with =ollama= models:

#+begin_src elisp
(use-package ollama-buddy
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu-wrapper)
  :custom
  (ollama-buddy-openai-api-key "<very long key>")
  (ollama-buddy-default-model "GPT gpt-4o")
  :config
  (require 'ollama-buddy-openai nil t)
  (ollama-buddy-update-menu-entry
   'refactor-code :model "qwen2.5-coder:7b")
  (ollama-buddy-update-menu-entry
   'git-commit :model "qwen2.5-coder:3b")
  (ollama-buddy-update-menu-entry
   'describe-code :model "qwen2.5-coder:3b")
  (ollama-buddy-update-menu-entry
   'dictionary-lookup :model "llama3.2:3b")
  (ollama-buddy-update-menu-entry
   'synonym :model "llama3.2:3b")
  (ollama-buddy-update-menu-entry
   'proofread :model "GPT gpt-4o")
  (ollama-buddy-update-menu-entry
   'custom-prompt :model "deepseek-r1:7b"))
#+end_src

** <2025-03-22> *0.9.9.5*

Added texinfo documentation for future automatic installation through MELPA and created an Emacs manual.

If you want to see what the manual would look like, just download the docs directory from github, cd into it, and run:

#+begin_src bash
make
sudo make install-docs
#+end_src

Then calling up =info= =C-h i= and ollama buddy will be present in the Emacs menu, or just select =m= and search for =Ollama Buddy=

For those interested in the manual, I have converted it into html format, which is accessible here:

[[file:docs/ollama-buddy.org]]

It has been converted using the following command:

#+begin_src bash
makeinfo --html --no-split ollama-buddy.texi -o ollama-buddy.html
pandoc -f html -t org -o ollama-buddy.org ollama-buddy.html
#+end_src

** <2025-03-20> *0.9.9*

Intro message with model management options (select, pull, delete) and option for recommended models to pull

- Enhance model management and selection features
- Display models available for download but not yet pulled

** <2025-03-19> *0.9.8*

Added model management interface to pull and delete models

- Introduced `ollama-buddy-manage-models` to list and manage models.
- Added actions for selecting, pulling, stopping, and deleting models.

You can now manage your Ollama models directly within Emacs with =ollama-buddy=

With this update, you can now:

- *Browse Available Models* – See all installed models at a glance.  
- *Select Models Easily* – Set your active AI model with a single click.  
- *Pull Models from Ollama Hub* – Download new models or update existing ones.  
- *Stop Running Models* – Halt background processes when necessary.  
- *Delete Unused Models* – Clean up your workspace with ease.  

1. *Open the Model Management Interface*  
   Press *=C-c W=* to launch the new *Model Management* buffer or through the transient menu.

2. *Manage Your Models*  
   - Click on a model to *select* it.  
   - Use *"Pull"* to fetch models from the Ollama Hub.  
   - Click *"Stop"* to halt active models.  
   - Use *"Delete"* to remove unwanted models.

3. *Perform Quick Actions*  
   - *=g=* → Refresh the model list.  
   - *=i=* → Import a *GGUF model file*.  
   - *=p=* → Pull a new model from the *Ollama Hub*.  

When you open the management interface, you get a structured list like this:

#+begin_src 
Ollama Models Management
=======================

Current Model: mistral:7b
Default Model: mistral:7b

Available Models:
  [ ] llama3.2:1b  Info  Pull  Delete
  [ ] starcoder2:3b  Info  Pull  Delete
  [ ] codellama:7b  Info  Pull  Delete
  [ ] phi3:3.8b  Info  Pull  Delete
  [x] llama3.2:3b  Info  Pull  Delete Stop

Actions:
[Import GGUF File]  [Refresh List]  [Pull Model from Hub]
#+end_src

Previously, managing Ollama models required manually running shell commands. With this update, you can now *do it all from Emacs*, keeping your workflow smooth and efficient!

** <2025-03-19> *0.9.7*

- Added GGUF file import and Dired integration

Import GGUF Models into Ollama from =dired= with the new =ollama-buddy-import-gguf-file= function. In =dired= just navigate to your file and press =C-c i= or =M-x ollama-buddy-import-gguf-file= to start the import process. This eliminates the need to manually input file paths, making the workflow smoother and faster.

The model will then be immediately available in the =ollama-buddy= chat interface.

** <2025-03-18> *0.9.6*

- Added a transient menu containing all commands currently presented in the chat buffer
- Added fabric prompting support, see https://github.com/danielmiessler/fabric
- Moved the presets to the top level so they will be present in the package folder

Ollama Buddy now includes a transient-based menu system to improve usability and streamline interactions. Yes, I originally stated that I would never do it, but I think it compliments my crafted simple textual menu and the fact that I have now defaulted the main chat interface to a simple menu.

This can give the user more options for configuration, they can use the chat in advanced mode where the keybindings are presented in situ, or a more minimal basic setup where the transient menu can be activated.  For my use-package definition I current have the following set up, with the two styles of menus sitting alongside each other :

  #+begin_src elisp
  :bind
  ("C-c o" . ollama-buddy-menu)
  ("C-c O" . ollama-buddy-transient-menu)
  #+end_src

The new menu provides an organized interface for accessing the assistant’s core functions, including chat, model management, roles, and Fabric patterns. This post provides an overview of the features available in the Ollama Buddy transient menus.

Yes that's right also =fabric= patterns!, I have decided to add in auto syncing of the patterns directory in https://github.com/danielmiessler/fabric

Simply I pull the patterns directory which contain prompt guidance for a range of different topics and then push them through a completing read to set the =ollama-buddy= system prompt, so a special set of curated prompts can now be applied right in the =ollama-buddy= chat!

Anyways, here is a description of the transient menu system.

*** What is the Transient Menu?

The transient menu in Ollama Buddy leverages Emacs' =transient.el= package (the same technology behind Magit's popular interface) to create a hierarchical, discoverable menu system. This approach transforms the user experience from memorizing numerous keybindings to navigating through logical groups of commands with clear descriptions.

*** Accessing the Menu

The main transient menu can be accessed with the keybinding =C-c O= when in an Ollama Buddy chat buffer. You can also call it via =M-x ollama-buddy-transient-menu= from anywhere in Emacs.

*** What the Menu Looks Like

When called, the main transient menu appears at the bottom of your Emacs frame, organized into logical sections with descriptive prefixes. Here's what you'll see:

#+begin_src 
|o(Y)o| Ollama Buddy
[Chat]             [Prompts]            [Model]               [Roles & Patterns]
o  Open Chat       l  Send Region       m  Switch Model       R  Switch Roles
O  Commands        s  Set System Prompt v  View Model Status  E  Create New Role
RET Send Prompt    C-s Show System      i  Show Model Info    D  Open Roles Directory
h  Help/Menu       r  Reset System      M  Multishot          f  Fabric Patterns
k  Kill/Cancel     b  Ollama Buddy Menu

[Display Options]          [History]              [Sessions]             [Parameters]
A  Toggle Interface Level  H  Toggle History      N  New Session         P  Edit Parameter
B  Toggle Debug Mode       X  Clear History       L  Load Session        G  Display Parameters
T  Toggle Token Display    V  Display History     S  Save Session        I  Parameter Help
U  Display Token Stats     J  Edit History        Q  List Sessions       K  Reset Parameters
C-o Toggle Markdown->Org                          Z  Delete Session      F  Toggle Params in Header
c  Toggle Model Colors                                                   p  Parameter Profiles
g  Token Usage Graph
#+end_src

This visual layout makes it easy to discover and access the full range of Ollama Buddy's functionality. Let's explore each section in detail.

*** Menu Sections Explained

**** Chat Section

This section contains the core interaction commands:

- *Open Chat (o)*: Opens the Ollama Buddy chat buffer
- *Commands (O)*: Opens a submenu with specialized commands
- *Send Prompt (RET)*: Sends the current prompt to the model
- *Help/Menu (h)*: Displays the help assistant with usage tips
- *Kill/Cancel Request (k)*: Cancels the current ongoing request

**** Prompts Section

These commands help you manage and send prompts:

- *Send Region (l)*: Sends the selected region as a prompt
- *Set System Prompt (s)*: Sets the current prompt as a system prompt
- *Show System Prompt (C-s)*: Displays the current system prompt
- *Reset System Prompt (r)*: Resets the system prompt to default
- *Ollama Buddy Menu (b)*: Opens the classic menu interface

**** Model Section

Commands for model management:

- *Switch Model (m)*: Changes the active LLM
- *View Model Status (v)*: Shows status of all available models
- *Show Model Info (i)*: Displays detailed information about the current model
- *Multishot (M)*: Sends the same prompt to multiple models

**** Roles & Patterns Section

These commands help manage roles and use fabric patterns:

- *Switch Roles (R)*: Switch to a different predefined role
- *Create New Role (E)*: Create a new role interactively
- *Open Roles Directory (D)*: Open the directory containing role definitions
- *Fabric Patterns (f)*: Opens the submenu for Fabric patterns

When you select the Fabric Patterns option, you'll see a submenu like this:

#+begin_src 
Fabric Patterns (42 available, last synced: 2025-03-18 14:30)
[Actions]             [Sync]              [Categories]          [Navigation]
s  Send with Pattern  S  Sync Latest      u  Universal Patterns q  Back to Main Menu
p  Set as System      P  Populate Cache   c  Code Patterns
l  List All Patterns  I  Initial Setup    w  Writing Patterns
v  View Pattern Details                   a  Analysis Patterns
#+end_src

**** Display Options Section

Commands to customize the display:

- *Toggle Interface Level (A)*: Switch between basic and advanced interfaces
- *Toggle Debug Mode (B)*: Enable/disable JSON debug information
- *Toggle Token Display (T)*: Show/hide token usage statistics
- *Display Token Stats (U)*: Show detailed token usage information
- *Toggle Markdown->Org (C-o)*: Enable/disable conversion to Org format
- *Toggle Model Colors (c)*: Enable/disable model-specific colors
- *Token Usage Graph (g)*: Display a visual graph of token usage

**** History Section

Commands for managing conversation history:

- *Toggle History (H)*: Enable/disable conversation history
- *Clear History (X)*: Clear the current history
- *Display History (V)*: Show the conversation history
- *Edit History (J)*: Edit the history in a buffer

**** Sessions Section

Commands for session management:

- *New Session (N)*: Start a new session
- *Load Session (L)*: Load a saved session
- *Save Session (S)*: Save the current session
- *List Sessions (Q)*: List all available sessions
- *Delete Session (Z)*: Delete a saved session

**** Parameters Section

Commands for managing model parameters:

- *Edit Parameter (P)*: Opens a submenu to edit specific parameters
- *Display Parameters (G)*: Show current parameter settings
- *Parameter Help (I)*: Display help information about parameters
- *Reset Parameters (K)*: Reset parameters to defaults
- *Toggle Params in Header (F)*: Show/hide parameters in header
- *Parameter Profiles (p)*: Opens the parameter profiles submenu

When you select the Edit Parameter option, you'll see a comprehensive submenu of all available parameters:

#+begin_src 
Parameters
[Generation]                [More Generation]          [Mirostat]
t  Temperature              f  Frequency Penalty       M  Mirostat Mode
k  Top K                    s  Presence Penalty        T  Mirostat Tau
p  Top P                    n  Repeat Last N           E  Mirostat Eta
m  Min P                    x  Stop Sequences
y  Typical P                l  Penalize Newline
r  Repeat Penalty

[Resource]                  [More Resource]            [Memory]
c  Num Ctx                  P  Num Predict             m  Use MMAP
b  Num Batch                S  Seed                    L  Use MLOCK
g  Num GPU                  N  NUMA                    C  Num Thread
G  Main GPU                 V  Low VRAM
K  Num Keep                 o  Vocab Only

[Profiles]                  [Actions]
d  Default Profile          D  Display All
a  Creative Profile         R  Reset All
e  Precise Profile          H  Help
A  All Profiles             F  Toggle Display in Header
                            q  Back to Main Menu
#+end_src

*** Parameter Profiles

Ollama Buddy includes predefined parameter profiles that can be applied with a single command. When you select "Parameter Profiles" from the main menu, you'll see:

#+begin_src 
Parameter Profiles
Current modified parameters: temperature, top_k, top_p
[Available Profiles]
d  Default
c  Creative
p  Precise

[Actions]
q  Back to Main Menu
#+end_src

*** Commands Submenu

The Commands submenu provides quick access to specialized operations:

#+begin_src 
Ollama Buddy Commands
[Code Operations]       [Language Operations]    [Pattern-based]         [Custom]
r  Refactor Code        l  Dictionary Lookup     f  Fabric Patterns      C  Custom Prompt
d  Describe Code        s  Synonym Lookup        u  Universal Patterns   m  Minibuffer Prompt
g  Git Commit Message   p  Proofread Text        c  Code Patterns

[Actions]
q  Back to Main Menu
#+end_src

*** Direct Keybindings

For experienced users who prefer direct keybindings, all transient menu functions can also be accessed through keybindings with the prefix of your choice (or =C-c O= when in the chat minibuffer) followed by the key shown in the menu. For example:

- =C-c O s= - Set system prompt
- =C-c O m= - Switch model
- =C-c O P= - Open parameter menu

*** Customization

The transient menu can be customized by modifying the =transient-define-prefix= definitions in the package. You can add, remove, or rearrange commands to suit your workflow.


** <2025-03-17> *0.9.5*

Added conversation history editing

- Added functions to edit conversation history (=ollama-buddy-history-edit=, =ollama-buddy-history-save=, etc.).
- Updated =ollama-buddy-display-history= to support history editing.
- Added keybinding =C-c E= for history editing.

Introducing conversation history editing!!

*Key Features*

Now, you can directly modify past interactions, making it easier to refine and manage your =ollama-buddy= chat history.

Previously, conversation history was static, you could view it but not change it. With this update, you can now:

- Edit conversation history directly in a buffer.
- Modify past interactions for accuracy or clarity.
- Save or discard changes with intuitive keybindings (=C-c C-c= to save, =C-c C-k= to cancel).
- Edit the history of all models or a specific one.

Simply use the new command *=C-c E=* to open the conversation history editor. This will display your past interactions in an editable format (alist). Once you’ve made your changes, press =C-c C-c= to save them back into Ollama Buddy’s memory.

and with a universal argument you can leverage =C-c E= to edit an individual model.

** <2025-03-17> *0.9.1*

New simple basic interface is available.

As this package becomes more advanced, I've been adding more to the intro message, making it increasingly cluttered. This could be off-putting for users who just want a simple interface to a local LLM via Ollama.

Therefore I have decided to add a customization option to simplify the menu.

Note: all functionality will still be available through keybindings, so just like Emacs then! :)

Note: some could see this initially as a breaking change as the intro message will look different, but rest assured all the functionality is still there (just to re-emphasize), so if you have been using it before and want the original functionality/intro message, just set :

#+begin_src 
(setq ollama-buddy-interface-level 'advanced)
#+end_src

#+begin_src elisp
(defcustom ollama-buddy-interface-level 'basic
  "Level of interface complexity to display.
'basic shows minimal commands for new users.
'advanced shows all available commands and features."
  :type '(choice (const :tag "Basic (for beginners)" basic)
                (const :tag "Advanced (full features)" advanced))
  :group 'ollama-buddy)
#+end_src

By default the menu will be set to Basic, unless obviously set explictly in an init file.  Here is an example of the basic menu:

#+begin_src 
,*** Welcome to OLLAMA BUDDY

,#+begin_example
 ___ _ _      n _ n      ___       _   _ _ _
|   | | |__._|o(Y)o|__._| . |_ _ _| |_| | | |
| | | | | .  |     | .  | . | | | . | . |__ |
|___|_|_|__/_|_|_|_|__/_|___|___|___|___|___|
,#+end_example

,**** Available Models

  (a) another:latest     (d) jamesio:latest
  (b) funnyname2:latest  (e) tinyllama:latest
  (c) funnyname:latest   (f) llama:latest

,**** Quick Tips

- Ask me anything!                    C-c C-c
- Change model                        C-c m
- Cancel request                      C-c k
- Browse prompt history               M-p/M-n
- Advanced interface (show all tips)  C-c A
#+end_src

and of the more advanced version

#+begin_src 
,*** Welcome to OLLAMA BUDDY

,#+begin_example
 ___ _ _      n _ n      ___       _   _ _ _
|   | | |__._|o(Y)o|__._| . |_ _ _| |_| | | |
| | | | | .  |     | .  | . | | | . | . |__ |
|___|_|_|__/_|_|_|_|__/_|___|___|___|___|___|
,#+end_example

,**** Available Models

  (a) another:latest     (d) jamesio:latest
  (b) funnyname2:latest  (e) tinyllama:latest
  (c) funnyname:latest   (f) llama:latest

,**** Quick Tips

- Ask me anything!                    C-c C-c
- Show Help/Token-usage/System-prompt C-c h/U/C-s
- Model Change/Info/Cancel            C-c m/i/k
- Prompt history                      M-p/M-n
- Session New/Load/Save/List/Delete   C-c N/L/S/Y/W
- History Toggle/Clear/Show           C-c H/X/V
- Prompt to multiple models           C-c l
- Parameter Edit/Show/Help/Reset      C-c P/G/I/K
- System Prompt/Clear   C-u/+C-u +C-u C-c C-c
- Toggle JSON/Token/Params/Format     C-c D/T/Z/C-o
- Basic interface (simpler display)   C-c A
- In another buffer? M-x ollama-buddy-menu
#+end_src



** <2025-03-17> *0.9.0*

Added command-specific parameter customization

- Added :parameters property to command definitions for granular control
- Implemented functions to apply and restore parameter settings
- Added example configuration to refactor-code command

With the latest update, you can now define specific parameter sets for each command in the menu, enabling you to optimize each AI interaction for its particular use case.

Different AI tasks benefit from different parameter settings. When refactoring code, you might want a more deterministic, precise response (lower temperature, higher repetition penalty), but when generating creative content, you might prefer more variation and randomness (higher temperature, lower repetition penalty). Previously, you had to manually adjust these parameters each time you switched between different types of tasks.

The new command-specific parameters feature lets you pre-configure the optimal settings for each use case. Here's how it works:

*** Key Features

- *Per-Command Parameter Sets*: Define custom parameter values for each command in your menu
- *Automatic Application*: Parameters are applied when running a command and restored afterward
- *Non-Destructive*: Your global parameter settings remain untouched
- *Easy Configuration*: Simple interface for adding or updating parameters

*** Example Configuration

#+begin_src elisp
;; Define a command with specific parameters
(refactor-code
 :key ?r
 :description "Refactor code"
 :prompt "refactor the following code:"
 :system "You are an expert software engineer..."
 :parameters ((temperature . 0.2) (top_p . 0.7) (repeat_penalty . 1.3))
 :action (lambda () (ollama-buddy--send-with-command 'refactor-code)))

;; Add parameters to an existing command
(ollama-buddy-add-parameters-to-command 'git-commit
 :temperature 0.4
 :top_p 0.9
 :repeat_penalty 1.1)

;; Update properties and parameters at once
(ollama-buddy-update-command-with-params 'describe-code
 :model "codellama:latest"
 :parameters '((temperature . 0.3) (top_p . 0.8)))
#+end_src

This feature is particularly useful for:

1. *Code-related tasks*: Lower temperature for more deterministic code generation
2. *Creative writing*: Higher temperature for more varied and creative outputs
3. *Technical explanations*: Balanced settings for clear, accurate explanations
4. *Summarization tasks*: Custom parameters to control verbosity and focus

** <2025-03-16> *0.8.5*

Added system prompt support for commands

- Introduced `:system` field to command definitions.
- Added `ollama-buddy-show-system-prompt` to view active system prompt.
- Updated UI elements to reflect system prompt status.

Previously, individual menu commands in =ollama-buddy= only included a user prompt. Now, each command can define a *system prompt*, which provides background context to guide the AI's responses. This makes interactions more precise and tailored.  

*Key Features*

- *System prompts per command*: Specify background instructions for each AI-powered command using the new =:system= field.  
- *View active system prompt*: Use =C-c C-s= to display the current system prompt in a dedicated buffer.  
- *Updated UI elements*: The status line now indicates whether a system prompt is active.  

A helper function has also been added to update the default menu, for example, you might want to tweak a couple of things:

#+begin_src elisp
(use-package ollama-buddy
  :bind ("C-c o" . ollama-buddy-menu)
  :custom
  (ollama-buddy-default-model "llama3.2:3b")
  :config
  (ollama-buddy-update-menu-entry
   'refactor-code
   :model "qwen2.5-coder:7b"
   :system "You are an expert software engineer who improves code and only mainly using the principles exhibited by Ada")
  (ollama-buddy-update-menu-entry
   'git-commit
   :model "qwen2.5-coder:3b"
   :system "You are a version control expert and mainly using subversion"))
#+end_src

** <2025-03-14> *0.8.0*

Added system prompt support

- Added =ollama-buddy--current-system-prompt= variable to track system prompts
- Updated prompt area rendering to distinguish system prompts
- Modified request payload to include system prompt when set
- Enhanced status bar to display system prompt indicator
- Improved help menu with system prompt keybindings

So this is system prompt support in Ollama Buddy!, allowing you to set and manage system-level instructions for your AI interactions. This feature enables you to define a *persistent system prompt* that remains active across user queries, providing better control over conversation context.  

*Key Features*

You can now designate any user prompt as a system prompt, ensuring that the AI considers it as a guiding instruction for future interactions. To set the system prompt, use:  

#+begin_src 
C-u C-c C-c
#+end_src

*Example:*

1. Type:

#+begin_src 
Always respond in a formal tone.
#+end_src

2. Press =C-u C-c C-c= This prompt is now set as the *system prompt* and any further chat ollama responses will adhere to the overarching guidelines defined in the prompt.

If you need to clear the system prompt and revert to normal interactions, use:  

#+begin_src 
C-u C-u C-c C-c
#+end_src

*How It Works*

- The active *system prompt* is stored and sent with each user prompt.  
- A "S" indicator appears in the status bar when a system prompt is active.  
- The request payload now includes the system role, allowing AI to recognize persistent instructions.  

*Demo*

Set the system message to:

You must always respond in a single sentence.

Now ask the following:

Tell me why Emacs is so great!

Tell me about black holes

clear the system message and ask again, the reponses should now be more verbose!!

#+attr_org: :width 300px
#+attr_html: :width 100%
[[file:img/ollama-buddy-screen-recording_015.gif]]

** <2025-03-13> *0.7.4*

Added model info command, update keybindings

- Added `ollama-buddy-show-raw-model-info` to fetch and display raw JSON details 
  of the current model in the chat buffer.
- Updated keybindings:
  - `C-c i` now triggers model info display.
  - `C-c h` mapped to help assistant.
  - Improved shortcut descriptions in quick tips section.
- Removed unused help assistant entry from menu.
- Changed minibuffer-prompt key from `?i` to `?b`.

** <2025-03-12> *0.7.3*

Added function to associate models with menu commands

- Added =ollama-buddy-add-model-to-menu-entry= autoload function
- Enabled dynamic modification of command-model associations

This is a helper function that allows you to associate specific models with individual menu commands.

Configuration to apply a model to a menu entry is now straightforward, in your Emacs init file, add something like:

#+begin_src elisp
(with-eval-after-load 'ollama-buddy
  (ollama-buddy-add-model-to-menu-entry 'dictionary-lookup "tinyllama:latest")
  (ollama-buddy-add-model-to-menu-entry 'synonym "tinyllama:latest"))
#+end_src

This configures simpler tasks like dictionary lookups and synonym searches to use the more efficient TinyLlama model, while your default model will still be used for more complex operations.

** <2025-03-12> *0.7.2*

Added menu model colours back in and removed some redundant code

** <2025-03-11> *0.7.1*

Added debug mode to display raw JSON messages in a debug buffer

- Created new debug buffer to show raw JSON messages from Ollama API
- Added toggle function to enable/disable debug mode (ollama-buddy-toggle-debug-mode)
- Modified stream filter to log and pretty-print incoming JSON messages
- Added keybinding C-c D to toggle debug mode
- Updated documentation in welcome message

** <2025-03-11> *0.7.0*

Added comprehensive Ollama parameter management

- Added customization for all Ollama option API parameters with defaults
- Only send modified parameters to preserve Ollama defaults
- Display active parameters with visual indicators for modified values
- Add keybindings and help system for parameter management
- Remove redundant temperature controls in favor of unified parameters

Introduced parameter management capabilities that give you complete control over your Ollama model's behavior through the options in the ollamas API.

Ollama's API supports a rich set of parameters for fine-tuning text generation, from controlling creativity with =temperature= to managing token selection with =top_p= and =top_k=. Until now, Ollama Buddy only exposed the =temperature= parameter, but this update unlocks the full potential of Ollama's parameter system!

*** Key Features:

- *All Parameters* - set all custom options for the ollama LLM at runtime
- *Smart Parameter Management*: Only modified parameters are sent to Ollama, preserving the model's built-in defaults for optimal performance
- *Visual Parameter Interface*: Clear display showing which parameters are active with highlighting for modified values

** Keyboard Shortcuts

Parameter management is accessible through simple keyboard shortcuts from the chat buffer:

- =C-c P= - Edit a parameter
- =C-c G= - Display current parameters
- =C-c I= - Show parameter help
- =C-c K= - Reset parameters to defaults

** <2025-03-10> *0.6.1*

Refactored prompt handling so each org header line should now always have a prompt for better export

- Added functionality to properly handle prompt text when showing/replacing prompts
- Extracted inline lambdas in menu actions into named functions
- Added fallback for when no default model is set

** <2025-03-08> *0.6.0*

Chat buffer now in org-mode

- Enabled =org-mode= in chat buffer for better text structure
- Implemented =ollama-buddy--md-to-org-convert-region= for Markdown to Org conversion
- Turn org conversion on and off
- Updated keybindings =C-c C-o= to toggle Markdown to Org conversion

*Key Features*  

1. The chat buffer is now in =org-mode= which gives the buffer enhanced readability and structure. Now, conversations automatically format user prompts and AI responses with *org-mode headings*, making them easier to navigate.

2. Of course with org-mode you will now get the additional benefits for free, such as:

   - outlining
   - org export
   - heading navigation
   - source code fontification

3. Previously, responses in *Ollama Buddy* were displayed in markdown formatting, which wasn’t always ideal for *org-mode users*. Now, you can automatically convert Markdown elements, such as bold/italic text, code blocks, and lists, into proper org-mode formatting.  This gives you the flexibility to work with markdown or org-mode as needed.  

** <2025-03-07> *0.5.1*

Added temperature control

- Implemented temperature control parameter
- Added menu commands for setting (T), resetting (0)
- Added keybindings (C-c t/T/0) for quick temperature adjustments
- Updated header line and prompt displays to show current temperature
- Included temperature info in welcome screen with usage guidance

This addition gives users fine-grained control over the creativity and randomness of their AI responses through a new temperature variable.

This update adds several convenient ways to control temperature in Ollama-Buddy:

*Key Features*

1. *Direct Temperature Setting*: Use =C-c t= from the chat buffer or the menu command =[T]= to set an exact temperature value between 0.0 and 2.0.

2. *Preset Temperatures*: Quickly switch between common temperature presets with =C-c T= from the chat buffer:
   - Precise (0.1) - For factual responses
   - Focused (0.3) - For deterministic, coherent outputs
   - Balanced (0.7) - Default setting
   - Creative (0.9) - For more varied, creative responses

3. *Reset to Default*: Return to the default temperature (0.7) with =C-c 0= or the menu command =[0]=.

4. *Visual Feedback*: The current temperature is displayed in the header line and before each response, so you always know what setting you're using.

** <2025-03-06> *0.5.0*

Implemented session management, so you can now save your conversations and bring them back with the relevant context and chat history!

- Chat history is now maintained separately for each model
- Added session new/load/save/delete/list functionality
- A switch in context can now be achieved by any of the following methods:
  - Loading a previous session
  - Creating a new session
  - Clearing history on the current session
  - Toggling history on and off

*Key Benefits*

- More relevant responses when switching between models
- Prevents context contamination across different models
- Clearer session management and organization

*Key Features*

1. *Session Management*

With session management, you can now:

- *Save session* with =ollama-buddy-sessions-save= (or through the ollama-buddy-menu) Preserve your current conversation with a custom name
- *Load session* with =ollama-buddy-sessions-load= (or through the ollama-buddy-menu) Return to previous conversations exactly where you left off
- *List all sessions* with =ollama-buddy-sessions-list= (or through the ollama-buddy-menu) View all saved sessions with metadata including timestamps and models used
- *Delete session* with =ollama-buddy-sessions-delete= (or through the ollama-buddy-menu) Clean up sessions you no longer need
- *New session* with =ollama-buddy-sessions-new=  (or through the ollama-buddy-menu) Begin a clean slate without losing your saved sessions

2. *Menu Commands*

The following commands have been added to the =ollama-buddy-menu=:

- =E= New session
- =L= Load session
- =S= Save session
- =Y= List sessions
- =K= Delete session

** <2025-03-04> *0.4.1*

Added a sparse version of =ollama-buddy= called =ollama-buddy-mini=, see the github repository for the elisp file and a description in =README-mini.org=

** <2025-03-03> *0.4.0*

Added conversation history support and navigation functions

- Implemented conversation history tracking between prompts and responses
- Added configurable history length limits and visual indicators
- Created navigation functions to move between prompts/responses in buffer

*Key Features*

1. *Conversation History*

Ollama Buddy now maintains context between your interactions by:

- Tracking conversation history between prompts and responses
- Sending previous messages to Ollama for improved contextual responses
- Displaying a history counter in the status line showing conversation length
- Providing configurable history length limits to control memory usage

You can control this feature with:

#+begin_src elisp
;; Enable/disable conversation history (default: t)
(setq ollama-buddy-history-enabled t)

;; Set maximum conversation pairs to remember (default: 10)
(setq ollama-buddy-max-history-length 10)

;; Show/hide the history counter in the header line (default: t)
(setq ollama-buddy-show-history-indicator t)
#+end_src

2. *Enhanced Navigation*

Moving through longer conversations is now much easier with:

- Navigation functions to jump between prompts using C-c n/p

3. *Menu Commands*

Three new menu commands have been added:

- =H=: Toggle history tracking on/off
- =X=: Clear the current conversation history
- =V=: View the full conversation history in a dedicated buffer

** <2025-03-02> *0.3.1*

Enhanced model colour contrast with themes, allowing =ollama-buddy-enable-model-colors= to be enabled by default.

** <2025-03-01> *0.3.0*

Added real-time token usage tracking and display

- Introduce variables to track token counts, rates, and usage history
- Implement real-time token rate updates with a timer
- Add a function to display token usage statistics in a dedicated buffer
- Allow toggling of token stats display after responses
- Integrate token tracking into response processing and status updates
- Ensure cleanup of timers and tracking variables on completion or cancellation

*Key Features*

1. *Menu Commands*

   The following command has been added to the =ollama-buddy-menu=:

   - =t= Show a summary of token model usage stats

** <2025-02-28> *0.2.4*

Added model-specific color highlighting

- Introduce `ollama-buddy-enable-model-colors` (default: nil) to toggle model-based color highlighting.
- Assign consistent colors to models based on string hashing.
- Apply colors to model names in the menu, status, headers, and responses.
- Add `ollama-buddy-toggle-model-colors` command to toggle this feature.

This enhancement aims to improve user experience by visually distinguishing different AI models within the interface.

Note: I am likely to use both *colour* and *color* interchangeably in the following text! :)

*Key Features*

1. *Model-Specific Colors*
   
   - A new customizable variable, =ollama-buddy-enable-model-colors=, allows users to enable or disable model-specific colors.
   - Colors are generated based on a model's name using a hashing function that produces consistent and visually distinguishable hues.
   - However there could be an improvement regarding ensuring the contrast is sufficient and hence visibility maintained with differing themes.

2. *Interactive Color Toggle*
   - Users can toggle model-specific colors with the command =ollama-buddy-toggle-model-colors=, providing flexibility in interface customization.

4. *Colored Model Listings*
   - Model names are now displayed with their respective colors in various parts of the interface, including:
     - The status line
     - Model selection menus
     - Command definitions
     - Chat history headers

5. *Menu Commands*

The following command hashing been added to the =ollama-buddy-menu=:

- =C= Toggle colors
   
** <2025-02-28> *0.2.3*

Added Prompt History Support

- Prompts are now integrated into the Emacs history mechanism which means they persist across sessions.  
- Use =M-p= to navigate prompt history, and =M-p= / =M-n= within the minibuffer to insert previous prompts.  

*Key Features*

- Persistent prompt history
- A new variable, =ollama-buddy--prompt-history=, now keeps track of past prompts. This means you can quickly recall and reuse previous queries instead of retyping them from scratch.
- =M-p= - recall a previous prompt in the buffer which will bring up the minibuffer for prompt history selection.
- Minibuffer =M-p= / =M-n= - Navigate through past prompts when prompted for input.

** <2025-02-27> *0.2.2*

Added support for role-based presets

- Introduced `ollama-buddy-roles-directory` for storing role preset files.
- Implemented interactive functions to manage roles:
  - `ollama-buddy-roles-switch-role`
  - `ollama-buddy-role-creator-create-new-role`
  - `ollama-buddy-roles-open-directory`
- Added ability to create and switch between role-specific commands.
- Updated menu commands to include role management options.

This enhancement allows you to create, switch, and manage role-specific command configurations, which basically generates differing menu layouts and hence command options based on your context, making your workflow more personalized and efficient.  

*What Are Role-Based Presets?*

Roles in Ollama Buddy are essentially *profiles* tailored to specific tasks. Imagine you're using Ollama Buddy for:  

- *Coding assistance* with one set of prompts
- *Creative writing* with a different tone and response style
- *Generating Buffy Style Quips* - just a fun one!

With this update, you can now create presets for each of these contexts and switch between them seamlessly without manually re-configuring settings every time. On each switch of context and hence role, a new ollama buddy menu will be generated with the associated keybinding attached to the relevant context commands.

*Key Features*

*1. Store Your Custom Roles*

A new directory =ollama-buddy-roles-directory= (defaulting to =~/.emacs.d/ollama-buddy-presets/=) now holds your role presets. Each role is saved as an =.el= file containing predefined *commands*, *shortcuts*, and *model preferences*.  

*2. Easily Switch Between Roles*

With =M-x ollama-buddy-roles-switch-role= you can pick from available role presets and swap effortlessly between them (or use the menu item from =ollama-buddy-menu=)

*3. Create Custom Roles with Unique Commands*

You can now define *custom commands* for each role with =M-x ollama-buddy-role-creator-create-new-role= (or the menu item from =ollama-buddy-menu=)

This interactive function allows you to:  

- Assign menu shortcuts to commands  
- Describe command behaviour  
- Set a default AI model  
- Define a system prompt for guiding responses  

Once saved, your new role is ready to load anytime!  

*4. Open Role Directory in Dired*

Need to tweak a role manually? A simple, run =M-x ollama-buddy-roles-open-directory= or of course also from the =ollama-buddy-menu= which opens the presets folder in *dired*, where you can quickly edit, copy, or delete role configurations.

*5. Preconfigured presets are available if you'd like to use a ready-made setup.*

- ollama-buddy--preset__buffy.el
- ollama-buddy--preset__default.el
- ollama-buddy--preset__developer.el
- ollama-buddy--preset__janeway.el
- ollama-buddy--preset__translator.el
- ollama-buddy--preset__writer.el

If these files are put in the =ollama-buddy-roles-directory= then the role selection menu will pass through completing-read, and present the following:

{buffy | default | developer | janeway | translator | writer}

With the selection regenerating the =ollama-buddy-menu= accordingly, and off you go.

*6. Menu commands*

The following commands have been added to the =ollama-buddy-menu=:

- =R= Switch Role
- =N= Create New Role
- =D= Open Roles Directory

** <2025-02-26> *0.2.1*

Added multishot execution with model selection  (See multishot section for description of new feature!)

- Assign letters to models for quick selection
- Implement multishot mode for sequential requests to multiple models
- Display multishot progress in status
- Bind `C-c M` to trigger multishot prompt

With the new *multishot mode*, you can now send a prompt to multiple models in sequence, and compare their responses.

*Key Features*

*1. Letter-Based Model Shortcuts*

Instead of manually selecting models, each available model is now assigned a *letter* (e.g., =(a) mistral=, =(b) gemini=). This allows for quick model selection when sending prompts or initiating a *multishot sequence*.

*2. Multishot Execution (=C-c C-l=)*

Ever wondered how different models would answer the same question? With *Multishot Mode*, you can:

- Send your prompt to a sequence of models in one shot.  
- Track progress as responses come in.  

*3. Status Updates*

When running a multishot execution, the status now updates dynamically:

- *"Multi Start"* when the sequence begins.  
- *"Processing..."* during responses.  
- *"Multi Finished"* when all models have responded.  

*4. How It Works*

1. *=C-c C-l=* to start a multishot session in the chat buffer.
2. Type a sequence of model letters (e.g., =abc= to use models =mistral=, =gemini=, and =llama=).  
3. The selected models will process the prompt *one by one*.  
  
** <2025-02-19> *0.2.0*

Improved prompt handling in chat buffer and simplified setup

- Chat buffer now more prompt based rather than ad-hoc using C-c C-c to send and C-c C-k to cancel
- Connection monitor now optional, ollama status visibility now maintained by strategic status checks simplifying setup.
- Can now change models from chat buffer using C-c C-m
- Updated intro message with ascii logo
- Suggested default "C-c o" for =ollama-buddy-menu=
- defcustom ollama-buddy-command-definitions now will work in the customization interface.

** <2025-02-13>

Models can be assigned to individual commands

- Set menu :model property to associate a command with a model
- Introduce `ollama-buddy-fallback-model` for automatic fallback if the specified model is unavailable.
- Improve `ollama-buddy--update-status-overlay` to indicate model substitution.
- Expand `ollama-buddy-menu` with structured command definitions using properties for improved flexibility.
- Add `ollama-buddy-show-model-status` to display available and used models.
- Refactor command execution flow to ensure model selection is handled dynamically.
